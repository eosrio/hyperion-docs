{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#hyperion-history-api","title":"Hyperion History API","text":"<p>Scalable Full History &amp; State Solution for Antelope (former EOSIO) based blockchains.  Made with \u2665 by Rio Blocks / EOS Rio</p>"},{"location":"#official-documentation","title":"Official documentation","text":""},{"location":"#for-providers","title":"For Providers","text":"<p>Getting Started Configuring Updating Repairing</p>"},{"location":"#for-developers","title":"For Developers","text":"<p>Getting Started Stream Client Endpoints List</p> <p>Testing/Development environment:</p> <p>Running Docker</p>"},{"location":"#api-reference","title":"API Reference","text":"<p>V2 V1 Compatible</p>"},{"location":"#ecosystem-resources","title":"Ecosystem Resources:","text":"<ul> <li>Hyperion Lightweight Explorer</li> <li>Hyperion Delphi Oracle</li> </ul>"},{"location":"#1-overview","title":"1. Overview","text":"<p>Hyperion is a high-performance, scalable solution designed to index, store, and retrieve the full history and current state of Antelope-based blockchains (formerly EOSIO). Antelope chains can generate vast amounts of data, demanding robust indexing, optimized storage, and efficient querying capabilities. Hyperion addresses these challenges by providing open-source software tailored for block producers, infrastructure providers, and dApp developers.</p> <p>Key Features:</p> <ul> <li>Scalable Indexing: Designed to handle high-throughput Antelope chains.</li> <li>Full History: Captures and stores every action and state change.</li> <li>Optimized Data Structure: Actions are stored flattened, with inline actions linked via transaction IDs, reducing redundancy (e.g., notifications identical to parent actions are omitted). Full blocks/transactions are reconstructed on demand, saving storage space.</li> <li>Current State Indexing: Optionally stores the latest state of specific contracts/tables in MongoDB for fast lookups.</li> <li>Modern API (v2): Offers comprehensive endpoints for history, state, and statistics. Legacy v1 API support is maintained for compatibility.</li> <li>Live Streaming: Provides real-time action and state delta streams via WebSockets.</li> <li>Extensible: Features a plugin system managed by the <code>hpm</code> tool.</li> </ul>"},{"location":"#2-core-concepts","title":"2. Core Concepts","text":"<p>Hyperion operates by separating the concerns of historical event streams and current on-chain state:</p> <ol> <li>Data Ingestion: The Indexer connects to an Antelope node's State History Plugin (SHIP) WebSocket endpoint.</li> <li>Processing &amp; Queuing: The Indexer deserializes action traces and state deltas, applies filtering (whitelists/blacklists), enriches data, and pushes processed data onto RabbitMQ queues.</li> <li>History Storage: Indexer worker processes consume data from RabbitMQ and index historical action traces and state deltas into Elasticsearch. This forms the backbone for historical queries.</li> <li>State Storage: If configured, Indexer workers (or dedicated sync tools) process deltas or perform full scans to maintain the current state of specified accounts, proposals, voters, or contract tables within MongoDB.</li> <li>Data Serving: The API Server handles client requests. It queries:</li> <li>Elasticsearch for historical data (<code>/v2/history/*</code>, <code>/v1/*</code>).</li> <li>MongoDB for current state data (<code>/v2/state/*</code>).</li> <li>Redis for cached responses and transaction lookups.</li> <li>The Antelope Node directly for real-time chain info or as a fallback.</li> </ol>"},{"location":"#3-architecture","title":"3. Architecture","text":"<p>A typical Hyperion deployment involves the following components. While they can run on a single machine for smaller chains or development, production environments benefit from distributing them across multiple servers connected via a high-speed network.</p>"},{"location":"#31-antelope-node-ship-enabled","title":"3.1 Antelope Node (SHIP Enabled)","text":"<p>The source of blockchain data. A node (e.g., built from the AntelopeIO/leap repository) running the <code>state_history_plugin</code> provides action traces and state deltas via a WebSocket connection to the Hyperion Indexer.</p>"},{"location":"#32-rabbitmq","title":"3.2 RabbitMQ","text":"<p>A robust message queuing system. Used as a buffer and transport layer between the different stages of the Hyperion Indexer (Reader -&gt; Deserializer -&gt; Indexer Workers) and for routing real-time data streams to connected API clients.</p>"},{"location":"#33-redis","title":"3.3 Redis","text":"<p>An in-memory data store used for:</p> <ul> <li>API Response Caching: Temporarily storing results of frequent API queries.</li> <li>Preemptive Transaction Caching: Storing recent transaction details for fast lookups via <code>v2/history/get_transaction</code> and <code>check_transaction</code>.</li> <li>API Usage Statistics: Tracking API endpoint usage rates.</li> <li>Inter-process Communication: Facilitating coordination, e.g., for rate limiting across clustered API instances (via <code>@fastify/rate-limit</code>).</li> <li>Live Streaming Coordination: Used by the Socket.IO Redis adapter for managing stream subscriptions across clustered API instances.</li> </ul>"},{"location":"#34-elasticsearch-cluster","title":"3.4 Elasticsearch Cluster","text":"<p>The primary datastore for indexed historical data. It stores processed action traces, state deltas, and block headers.</p> <ul> <li>Role: Enables powerful search and aggregation capabilities for historical queries (e.g., <code>get_actions</code>, <code>get_deltas</code>).</li> <li>Requirement: Essential for all Hyperion history functionalities.</li> <li>Recommendation: Requires significant RAM (32GB+ per node recommended), CPU, and fast storage (SSD/NVMe recommended for ingest nodes, HDDs can be used for cold storage nodes). Multi-node clusters are highly recommended for production.</li> </ul>"},{"location":"#35-mongodb","title":"3.5 MongoDB","text":"<p>This MongoDB integration complements Elasticsearch by focusing on current state data rather than historical actions, enabling efficient state queries without scanning history.</p> <ul> <li>Recommendation: Requires adequate RAM, CPU, and Disk I/O, particularly if indexing large amounts of contract state.</li> </ul> <p>System Contract State Storage:</p> <ul> <li>Stores searchable state data for Antelope system contracts like token balances, proposals, and voter information</li> <li>Maintains three primary collections by default:<ul> <li><code>accounts</code>: Stores token balances with indexes for code, scope, and symbol</li> <li><code>proposals</code>: Tracks governance proposals with detailed approval status</li> <li><code>voters</code>: Manages staking and voting records with optimized query paths</li> </ul> </li> </ul> <p>Custom Contract State Tracking:</p> <ul> <li>Supports operator-defined custom contracts and tables</li> <li>Uses a flexible configuration system to define which contract tables to synchronize</li> <li>Automatically creates appropriate indexes based on contract schemas</li> <li>Stores tables in collections named <code>{contract}-{table}</code></li> </ul> <p>State Synchronization:</p> <ul> <li>Enables state synchronization even when starting from snapshots, providing a complete view of the blockchain state</li> <li>Managed through the <code>hyp-control</code> CLI tool, allowing for targeted synchronization of specific contracts</li> <li>Maintains block references to track state changes over time</li> </ul> <p>Query Optimization:</p> <ul> <li>Creates specialized indexes based on common query patterns</li> <li>Supports advanced query capabilities including MongoDB operators like <code>$gt</code>, <code>$lt</code>, <code>$in</code> for filters</li> <li>Automatically handles date fields for time-based queries</li> </ul> <p>API Integration:</p> <ul> <li>Provides dedicated API endpoints for querying state data</li> <li>Supports endpoints like <code>/v2/state/*</code> API endpoints</li> <li>Offers flexible filtering options with pagination</li> </ul> <p>Dynamic Contract Schema Support:</p> <ul> <li>Automatic ABI-based index creation </li> <li>Allows for manual index configuration for custom query patterns</li> <li>Optional text search indexes for specific fields</li> </ul>"},{"location":"#36-hyperion-indexer","title":"3.6 Hyperion Indexer","text":"<p>A Node.js application responsible for fetching data from SHIP, deserializing it, processing actions and deltas according to configured filters/handlers, and publishing data to RabbitMQ queues for indexing and state updates. Managed by the PM2 process manager.</p>"},{"location":"#37-hyperion-api-server","title":"3.7 Hyperion API Server","text":"<p>A Node.js (Fastify framework) application that serves the HTTP API endpoints (v1 and v2). It queries Elasticsearch, MongoDB, Redis, and the Antelope node as needed. It also manages the Swagger documentation UI and handles WebSocket connections for live streaming. Typically run in cluster mode using PM2 for scalability and resilience.</p>"},{"location":"#4-ecosystem","title":"4. Ecosystem","text":""},{"location":"#41-hyperion-stream-client","title":"4.1 Hyperion Stream Client","text":"<p>A client library (for Web and Node.js) simplifying connection to the real-time streaming endpoints offered by enabled Hyperion providers.</p> <p>See Stream Client Documentation.</p>"},{"location":"#42-hyperion-explorer","title":"4.2 Hyperion Explorer","text":"<p>Standalone version of the Hyperion Lightweight Explorer, replaces the deprecated plugin.</p> <p>Deployment and instructions</p>"},{"location":"#43-hyperion-plugins","title":"4.3 Hyperion Plugins","text":"<p>Hyperion features an extensible plugin architecture. Plugins can add custom data handlers, API routes, or other functionalities. Managed via the <code>hpm</code> (hyperion plugin manager) command-line tool.</p> <ul> <li>Example: Hyperion Delphi Oracle</li> </ul> <p></p>"},{"location":"api/v1/","title":"API Reference: v1","text":""},{"location":"api/v1/#v1historyget_actions","title":"/v1/history/get_actions","text":""},{"location":"api/v1/#post","title":"POST","text":""},{"location":"api/v1/#summary","title":"Summary","text":"<p>get actions</p>"},{"location":"api/v1/#description","title":"Description","text":"<p>legacy get actions query</p>"},{"location":"api/v1/#request-body","title":"Request Body","text":"<pre><code>{\n  \"account_name\": \"string\",\n  \"pos\": 0,\n  \"offset\": 0,\n  \"filter\": \"string\",\n  \"sort\": \"desc\",\n  \"after\": \"2020-01-17T19:51:03.618Z\",\n  \"before\": \"2020-01-17T19:51:03.618Z\",\n  \"parent\": 0\n}\n</code></pre>"},{"location":"api/v1/#schema","title":"Schema","text":"variable type description account_name string  minLength: 1 maxLength: 12 notified account pos integer action position (pagination) offset integer limit of [n] actions per page filter string minLength: 3 code:name filter sort string sort direction Enum: [ desc, asc, 1, -1 ] after string($date-time) filter after specified date (ISO8601) before string($date-time) filter before specified date (ISO8601) parent integer minimum: 0 filter by parent global sequence"},{"location":"api/v1/#responses","title":"Responses","text":"Code Description 200"},{"location":"api/v1/#example","title":"Example","text":"<pre><code>curl -X POST \"https://eos.hyperion.eosrio.io/v1/history/get_actions\"\n</code></pre>"},{"location":"api/v1/#v1historyget_controlled_accounts","title":"/v1/history/get_controlled_accounts","text":""},{"location":"api/v1/#post_1","title":"POST","text":""},{"location":"api/v1/#summary_1","title":"Summary","text":"<p>get controlled accounts by controlling accounts</p>"},{"location":"api/v1/#description_1","title":"Description","text":"<p>get controlled accounts by controlling accounts</p>"},{"location":"api/v1/#request-body-required","title":"Request Body Required","text":"<pre><code>{\n  \"controlling_account\": \"string\"\n}\n</code></pre>"},{"location":"api/v1/#schema_1","title":"Schema","text":"variable type description controlling_account string controlling account"},{"location":"api/v1/#responses_1","title":"Responses","text":"Code Description 200"},{"location":"api/v1/#example_1","title":"Example","text":"<pre><code>curl -X POST \"https://eos.hyperion.eosrio.io/v1/history/get_controlled_accounts\" -d '{\"controlling_account\":\"eosio\"}'\n</code></pre>"},{"location":"api/v1/#v1historyget_key_accounts","title":"/v1/history/get_key_accounts","text":""},{"location":"api/v1/#post_2","title":"POST","text":""},{"location":"api/v1/#summary_2","title":"Summary","text":"<p>get accounts by public key</p>"},{"location":"api/v1/#description_2","title":"Description","text":"<p>get accounts by public key</p>"},{"location":"api/v1/#request-body-required_1","title":"Request Body Required","text":"<pre><code>{\n  \"public_key\": \"string\"\n}\n</code></pre>"},{"location":"api/v1/#schema_2","title":"Schema","text":"variable type description public_key public key public key"},{"location":"api/v1/#responses_2","title":"Responses","text":"Code Description 200 Default Response"},{"location":"api/v1/#example_2","title":"Example","text":"<pre><code>curl -X POST \"https://eos.hyperion.eosrio.io/v1/history/get_key_accounts\" -d '{\"public_key\":\"EOS8fDDZm7ommT5XBf9MPYkRioXX6GeCUeSNkTpimdwKon5bNAVm7\"}'\n</code></pre> Code Description 200"},{"location":"api/v1/#v1historyget_transaction","title":"/v1/history/get_transaction","text":""},{"location":"api/v1/#post_3","title":"POST","text":""},{"location":"api/v1/#summary_3","title":"Summary","text":"<p>get transaction by id</p>"},{"location":"api/v1/#description_3","title":"Description","text":"<p>get all actions belonging to the same transaction</p>"},{"location":"api/v1/#request-body-required_2","title":"Request Body Required","text":"<pre><code>{\n  \"id\": \"string\"\n}\n</code></pre>"},{"location":"api/v1/#schema_3","title":"Schema","text":"variable type description id string transaction id"},{"location":"api/v1/#responses_3","title":"Responses","text":"Code Description 200 Default Response"},{"location":"api/v1/#example_3","title":"Example","text":"<pre><code>curl -X POST \"https://eos.hyperion.eosrio.io/v1/history/get_transaction\"\n</code></pre>"},{"location":"api/v1/#v1chainget_block","title":"/v1/chain/get_block","text":""},{"location":"api/v1/#post_4","title":"POST","text":""},{"location":"api/v1/#summary_4","title":"Summary","text":"<p>Returns an object containing various details about a specific block on the blockchain.</p>"},{"location":"api/v1/#description_4","title":"Description","text":"<p>Returns an object containing various details about a specific block on the blockchain.</p>"},{"location":"api/v1/#request-body_1","title":"Request Body","text":"<pre><code>{\n  \"block_num_or_id\": \"string\"\n}\n</code></pre>"},{"location":"api/v1/#schema_4","title":"Schema","text":"<p>block_num_or_id* -&gt; string -&gt; Provide a block number or a block id</p>"},{"location":"api/v1/#responses_4","title":"Responses","text":"Code Description 200 Default Response"},{"location":"api/v1/#example_4","title":"Example","text":"<pre><code>curl -X POST \"https://eos.hyperion.eosrio.io/v1/chain/get_block\" -d '{\"block_num_or_id\": \"1000\"}'\n</code></pre>"},{"location":"api/v1/#v1trace_apiget_block","title":"/v1/trace_api/get_block","text":""},{"location":"api/v1/#post_5","title":"POST","text":""},{"location":"api/v1/#summary_5","title":"Summary","text":"<p>Get block traces.</p>"},{"location":"api/v1/#description_5","title":"Description","text":"<p>Get block traces.</p>"},{"location":"api/v1/#request-body_2","title":"Request Body","text":"<pre><code>{\n  \"block_num\": 0,\n  \"block_id\": \"string\"\n}\n</code></pre>"},{"location":"api/v1/#schema_5","title":"Schema","text":"<p>block_num -&gt; integer -&gt; block number block_id -&gt; string -&gt; block id</p>"},{"location":"api/v1/#responses_5","title":"Responses","text":"Code Description 200 Default Response"},{"location":"api/v1/#examples","title":"Examples","text":"<p>Get block 10000: <pre><code>curl -X POST --url https://wax.hyperion.eosrio.io/v1/trace_api/get_block --data '{\"block_num\": 10000}' --header 'content-type: application/json'\n</code></pre></p> <p>Get latest block: <pre><code>curl -X POST --url https://wax.hyperion.eosrio.io/v1/trace_api/get_block --data '{\"block_num\": -1}' --header 'content-type: application/json'\n</code></pre></p> <p>By block_id: <pre><code>curl -X POST --url https://wax.hyperion.eosrio.io/v1/trace_api/get_block --data '{\"block_id\": \"0307e42d3b2328805606929438411aa78dffb1756b2a0bcb2a9f7ce8a7035990\"}' --header 'content-type: application/json'\n</code></pre></p>"},{"location":"api/v2/","title":"API Reference: v2","text":"<p>Tip</p> <p>For more details and live testing, check out our WAX swagger</p>"},{"location":"api/v2/#v2historyget_abi_snapshot","title":"/v2/history/get_abi_snapshot","text":""},{"location":"api/v2/#get","title":"GET","text":""},{"location":"api/v2/#summary","title":"Summary","text":"<p>Fetch contract ABI</p>"},{"location":"api/v2/#description","title":"Description","text":"<p>Fetch contract ABI at a specific block if specified.</p>"},{"location":"api/v2/#parameters","title":"Parameters","text":"Name Located in Description Required Schema contract query Contract account Yes string block query Target block No integer fetch query Fetch the ABI No boolean"},{"location":"api/v2/#responses","title":"Responses","text":"Code Description 200 Default Response"},{"location":"api/v2/#examples","title":"Examples","text":"<pre><code>/v2/history/get_abi_snapshot?contract=eosio.token&amp;fetch=true\n</code></pre>"},{"location":"api/v2/#v2historyget_actions","title":"/v2/history/get_actions","text":""},{"location":"api/v2/#get_1","title":"GET","text":""},{"location":"api/v2/#summary_1","title":"Summary","text":"<p>Get actions</p>"},{"location":"api/v2/#description_1","title":"Description","text":"<p>Get past actions based on notified account.</p>"},{"location":"api/v2/#parameters_1","title":"Parameters","text":"Name Located in Description Required Schema account query Notified account No string filter query Filter by code:name No string track query Total results to track (count) No string skip query Skip [n] actions (pagination) No integer limit query Limit of [n] actions per page No integer sort query Sort direction ('desc', 'asc', '1' or '-1') No string block_num query Filter actions by block_num range [from]-[to] No string global_sequence query Filter actions by global_sequence range [from]-[to] No string after query Filter actions after specified date (ISO8601) No string before query Filter actions before specified date (ISO8601) No string simple query Simplified output mode No boolean noBinary query exclude large binary data No boolean checkLib query perform reversibility check No boolean <p>Note</p> <p>Besides the parameters listed above, this endpoint also accepts generic parameters based on indexed fields (e.g. <code>act.authorization.actor=eosio</code> or <code>act.name=delegatebw</code>). </p> <p>If included they will be combined with an AND operator.</p> <p>Tip</p> <p>You can check action mappings on index-templates.ts file for the list of all possible parameters.</p>"},{"location":"api/v2/#responses_1","title":"Responses","text":"Code Description 200 Default Response"},{"location":"api/v2/#examples_1","title":"Examples","text":"<pre><code>/v2/history/get_actions?account=eosio&amp;act.name=delegatebw\n</code></pre> <p><code>noBinary</code> - Will suppress large hex payloads to reduce the transmission latency</p> <pre><code>/v2/history/get_actions?noBinary=true\n</code></pre> <p><code>checkLib</code> - Performs lib test. The default is false, meaning the query will be faster since it won't wait for the get_info to happen. If you wish to see the lib value on the results you must request explicitly.</p> <pre><code>/v2/history/get_actions?checkLib=true\n</code></pre> <p>The simple mode will add the irreversible field (<code>true/false</code>) on each action</p> <pre><code>/v2/history/get_actions?checkLib=true&amp;simple=true\n</code></pre> <p></p>"},{"location":"api/v2/#v2historyget_created_accounts","title":"/v2/history/get_created_accounts","text":""},{"location":"api/v2/#get_2","title":"GET","text":""},{"location":"api/v2/#summary_2","title":"Summary","text":"<p>Get created accounts</p>"},{"location":"api/v2/#description_2","title":"Description","text":"<p>Get all accounts created by one creator.</p>"},{"location":"api/v2/#parameters_2","title":"Parameters","text":"Name Located in Description Required Schema account query Account creator Yes string"},{"location":"api/v2/#responses_2","title":"Responses","text":"Code Description 200 Default Response"},{"location":"api/v2/#examples_2","title":"Examples","text":"<pre><code>/v2/history/get_created_accounts?account=eosio\n</code></pre>"},{"location":"api/v2/#v2historyget_creator","title":"/v2/history/get_creator","text":""},{"location":"api/v2/#get_3","title":"GET","text":""},{"location":"api/v2/#summary_3","title":"Summary","text":"<p>Get account creator</p>"},{"location":"api/v2/#description_3","title":"Description","text":"<p>Get the creator of a specific account.</p>"},{"location":"api/v2/#parameters_3","title":"Parameters","text":"Name Located in Description Required Schema account query Created account Yes string"},{"location":"api/v2/#responses_3","title":"Responses","text":"Code Description 200 Default Response"},{"location":"api/v2/#examples_3","title":"Examples","text":"<pre><code>/v2/history/get_creator?account=eosriobrazil\n</code></pre>"},{"location":"api/v2/#v2historyget_deltas","title":"/v2/history/get_deltas","text":""},{"location":"api/v2/#get_4","title":"GET","text":""},{"location":"api/v2/#summary_4","title":"Summary","text":"<p>Get state deltas</p>"},{"location":"api/v2/#description_4","title":"Description","text":"<p>Get state deltas of any table.</p>"},{"location":"api/v2/#parameters_4","title":"Parameters","text":"Name Located in Description Required Schema code query Contract account No string scope query Table scope No string table query Table name No string payer query Payer account No string <p>Note</p> <p>Besides the parameters listed above, this endpoint also accepts generic parameters based on indexed fields (e.g. <code>block_num=10000000</code>). </p> <p>If included they will be combined with an AND operator.</p> <p>Tip</p> <p>You can check delta mappings on index-templates.ts file for the list of all possible parameters.</p>"},{"location":"api/v2/#responses_4","title":"Responses","text":"Code Description 200 Default Response"},{"location":"api/v2/#examples_4","title":"Examples","text":"<p>Get all deltas from <code>eosio.token</code> contract on a specific block <pre><code>/v2/history/get_deltas?code=eosio.token&amp;block_num=100000\n</code></pre></p> <p>Get all deltas from the table <code>accounts</code> of the <code>eosio.token</code> contract  <pre><code>/v2/history/get_deltas?code=eosio.token&amp;table=accounts\n</code></pre></p> <p></p>"},{"location":"api/v2/#v2historyget_transacted_accounts","title":"/v2/history/get_transacted_accounts","text":""},{"location":"api/v2/#get_5","title":"GET","text":""},{"location":"api/v2/#summary_5","title":"Summary","text":"<p>Get account interactions based on transfers</p>"},{"location":"api/v2/#description_5","title":"Description","text":"<p>Get all accounts that interacted with the source account provided. Interactions can be from or to (direction) source account.</p>"},{"location":"api/v2/#parameters_5","title":"Parameters","text":"Name Located in Description Required Schema account query Source account Yes string direction query Interaction direction ('in', 'out' or 'both') Yes string symbol query Token symbol No string contract query Token contract No string min query Transfer minimum value No number max query Transfer maximum value No number limit query Limit of [n] interactions No number after query Filter actions after specified date (ISO8601) or block number No string before query Filter actions before specified date (ISO8601) or block number No string"},{"location":"api/v2/#responses_5","title":"Responses","text":"Code Description 200 Default Response"},{"location":"api/v2/#examples_5","title":"Examples","text":"<pre><code>/v2/history/get_transacted_accounts?account=eosriobrazil&amp;direction=out\n</code></pre>"},{"location":"api/v2/#v2historyget_transaction","title":"/v2/history/get_transaction","text":""},{"location":"api/v2/#get_6","title":"GET","text":""},{"location":"api/v2/#summary_6","title":"Summary","text":"<p>Get transaction by id</p>"},{"location":"api/v2/#description_6","title":"Description","text":"<p>Get a transaction with all its actions.</p>"},{"location":"api/v2/#parameters_6","title":"Parameters","text":"Name Located in Description Required Schema id query Transaction ID Yes string"},{"location":"api/v2/#responses_6","title":"Responses","text":"Code Description 200 Default Response"},{"location":"api/v2/#examples_6","title":"Examples","text":"<pre><code>v2/history/get_transaction?id=eec44c2ab2c2330e88fdacd3cd4c63838adb60da679ff12f299a4341fd036658\n</code></pre>"},{"location":"api/v2/#v2historyget_transfers","title":"/v2/history/get_transfers","text":""},{"location":"api/v2/#get_7","title":"GET","text":""},{"location":"api/v2/#summary_7","title":"Summary","text":"<p>Get token transfers</p>"},{"location":"api/v2/#description_7","title":"Description","text":"<p>Get token transfers from contracts using the eosio.token standard.</p>"},{"location":"api/v2/#parameters_7","title":"Parameters","text":"Name Located in Description Required Schema from query Source account No string to query Destination account No string symbol query Token symbol No string contract query Token contract No string skip query Skip [n] actions (pagination) No integer limit query Limit of [n] actions per page No integer after query Filter actions after specified date (ISO8601) No string before query Filter actions before specified date (ISO8601) No string"},{"location":"api/v2/#responses_7","title":"Responses","text":"Code Description 200 Default Response"},{"location":"api/v2/#examples_7","title":"Examples","text":"<p>Get all transfers from <code>eosriobrazil</code> account <pre><code>/v2/history/get_transfers?from=eosriobrazil\n</code></pre></p> <p>Get all transfers from <code>eosriobrazil</code> account to <code>eosio.ramfee</code> account <pre><code>/v2/history/get_transfers?from=eosriobrazil&amp;to=eosio.ramfee\"\n</code></pre></p> <p>Get all transfers from <code>eosriobrazil</code> account to <code>eosio.ramfee</code> account after November 2019 <pre><code>/v2/history/get_transfers?from=eosriobrazil&amp;to=eosio.ramfee&amp;after=2019-11-01T00:00:00.000Z\n</code></pre></p> <p></p>"},{"location":"api/v2/#v2stateget_account","title":"/v2/state/get_account","text":""},{"location":"api/v2/#get_8","title":"GET","text":""},{"location":"api/v2/#summary_8","title":"Summary","text":"<p>Get account summary</p>"},{"location":"api/v2/#description_8","title":"Description","text":"<p>Get all data related to an account.</p>"},{"location":"api/v2/#parameters_8","title":"Parameters","text":"Name Located in Description Required Schema account query Account name No string"},{"location":"api/v2/#responses_8","title":"Responses","text":"Code Description 200 Default Response"},{"location":"api/v2/#examples_8","title":"Examples","text":"<pre><code>/v2/state/get_account?account=eosio\n</code></pre>"},{"location":"api/v2/#v2stateget_key_accounts","title":"/v2/state/get_key_accounts","text":""},{"location":"api/v2/#get_9","title":"GET","text":""},{"location":"api/v2/#summary_9","title":"Summary","text":"<p>Get accounts by public key</p>"},{"location":"api/v2/#description_9","title":"Description","text":"<p>Get all accounts with specified public key.</p>"},{"location":"api/v2/#parameters_9","title":"Parameters","text":"Name Located in Description Required Schema public_key query Public key Yes string"},{"location":"api/v2/#responses_9","title":"Responses","text":"Code Description 200 Default Response"},{"location":"api/v2/#examples_9","title":"Examples","text":"<pre><code>/v2/state/get_key_accounts?public_key=EOS8fDDZm7ommT5XBf9MPYkRioXX6GeCUeSNkTpimdwKon5bNAVm7\"\n</code></pre>"},{"location":"api/v2/#v2stateget_key_accounts_1","title":"/v2/state/get_key_accounts","text":""},{"location":"api/v2/#post","title":"POST","text":""},{"location":"api/v2/#summary_10","title":"Summary","text":"<p>Get accounts by public key</p>"},{"location":"api/v2/#description_10","title":"Description","text":"<p>Get all accounts with specified public key.</p>"},{"location":"api/v2/#parameters_10","title":"Parameters","text":"Name Located in Description Required Schema public_key body Public key Yes string"},{"location":"api/v2/#responses_10","title":"Responses","text":"Code Description 200 Default Response"},{"location":"api/v2/#examples_10","title":"Examples","text":"<pre><code>/v2/state/get_key_accounts - body: {\"public_key\":\"EOS8fDDZm7ommT5XBf9MPYkRioXX6GeCUeSNkTpimdwKon5bNAVm7\"}\n</code></pre>"},{"location":"api/v2/#v2stateget_links","title":"/v2/state/get_links","text":""},{"location":"api/v2/#get_10","title":"GET","text":""},{"location":"api/v2/#summary_11","title":"Summary","text":"<p>Get permission links</p>"},{"location":"api/v2/#description_11","title":"Description","text":"<p>Get permission links from an account.</p>"},{"location":"api/v2/#parameters_11","title":"Parameters","text":"Name Located in Description Required Schema account name (query) Account name Yes string code (query) Contract name No string action (query) Method name No string permission (query) Permission name No string"},{"location":"api/v2/#responses_11","title":"Responses","text":"Code Description 200 Default Response"},{"location":"api/v2/#examples_11","title":"Examples","text":"<pre><code>/v2/state/get_links?account=eosriobrazil\n</code></pre>"},{"location":"api/v2/#v2stateget_proposals","title":"/v2/state/get_proposals","text":""},{"location":"api/v2/#get_11","title":"GET","text":""},{"location":"api/v2/#summary_12","title":"Summary","text":"<p>Get proposals</p>"},{"location":"api/v2/#description_12","title":"Description","text":"<p>Get all available proposals.</p>"},{"location":"api/v2/#parameters_12","title":"Parameters","text":"Name Located in Description Required Schema proposer query Filter by proposer No string proposal query Filter by proposal name No string account query Filter by either requested or provided account No string requested query Filter by requested account No string provided query Filter by provided account No string executed query Filter by execution status No boolean track query Total results to track (count) No string skip query Skip [n] actions (pagination) No integer limit query Limit of [n] actions per page No integer"},{"location":"api/v2/#responses_12","title":"Responses","text":"Code Description 200 Default Response"},{"location":"api/v2/#examples_12","title":"Examples","text":"<pre><code>/v2/state/get_proposals?proposer=eosriobrazil\n</code></pre>"},{"location":"api/v2/#v2stateget_tokens","title":"/v2/state/get_tokens","text":""},{"location":"api/v2/#get_12","title":"GET","text":""},{"location":"api/v2/#summary_13","title":"Summary","text":"<p>Get tokens from account</p>"},{"location":"api/v2/#description_13","title":"Description","text":"<p>Retrieve all tokens holded by an account.</p>"},{"location":"api/v2/#parameters_13","title":"Parameters","text":"Name Located in Description Required Schema account query Account name Yes string"},{"location":"api/v2/#responses_13","title":"Responses","text":"Code Description 200 Default Response"},{"location":"api/v2/#examples_13","title":"Examples","text":"<pre><code>/v2/state/get_tokens?account=eosriobrazil\n</code></pre>"},{"location":"api/v2/#v2stateget_voters","title":"/v2/state/get_voters","text":""},{"location":"api/v2/#get_13","title":"GET","text":""},{"location":"api/v2/#summary_14","title":"Summary","text":"<p>Get voters</p>"},{"location":"api/v2/#description_14","title":"Description","text":"<p>Get all voting accounts.</p>"},{"location":"api/v2/#parameters_14","title":"Parameters","text":"Name Located in Description Required Schema producer query Filter by voted producer (comma separated) No string skip query Skip [n] actions (pagination) No integer limit query Limit of [n] actions per page No integer"},{"location":"api/v2/#responses_14","title":"Responses","text":"Code Description 200 Default Response"},{"location":"api/v2/#examples_14","title":"Examples","text":"<p>Get all <code>eosriobrazil</code> voters <pre><code>/v2/state/get_voters?producer=eosriobrazil\n</code></pre></p> <p>Get only the first 3 responses <pre><code>/v2/state/get_voters?producer=eosriobrazil&amp;limit=3\n</code></pre></p> <p></p>"},{"location":"api/v2/#v2health","title":"/v2/health","text":""},{"location":"api/v2/#get_14","title":"GET","text":""},{"location":"api/v2/#summary_15","title":"Summary:","text":"<p>API Service Health Report</p>"},{"location":"api/v2/#responses_15","title":"Responses","text":"Code Description 200 Default Response"},{"location":"dev/endpoint/","title":"Hyperion Open History Endpoint List","text":"<p>Tip</p> <p>Check the endpoint status at https://bloks.io/hyperion</p>"},{"location":"dev/endpoint/#1-mainnets","title":"1. Mainnets:","text":""},{"location":"dev/endpoint/#eos","title":"EOS","text":"url docs explorer https://eos.hyperion.eosrio.io/v2 docs https://eos.eosusa.io/v2 docs https://hyperion.paycash.online/v2 docs"},{"location":"dev/endpoint/#wax","title":"WAX","text":"url docs explorer https://wax.eosrio.io/v2 docs explorer https://api.waxsweden.org/v2 docs https://wax.eosusa.io/v2 docs https://wax.eosphere.io/v2 docs https://wax.pink.gg/v2 docs https://wax.blokcrafters.io/v2 docs https://hyperion.wax.eosdetroit.io/v2 docs https://wax.cryptolions.io/v2 docs explorer"},{"location":"dev/endpoint/#telos","title":"TELOS","text":"url docs explorer https://mainnet.telos.net/v2 docs explorer https://telos.eosrio.io/v2 docs explorer https://telos.eosusa.io/v2 docs https://telos.eosphere.io/v2 docs https://telos.caleos.io/v2 docs explorer https://hyperion.telos.eosdetroit.io/v2 docs"},{"location":"dev/endpoint/#uos","title":"UOS","text":"url docs explorer https://ultra.eosusa.io/v2 docs"},{"location":"dev/endpoint/#instar","title":"INSTAR","text":"url docs explorer https://instar.eosusa.io/v2 docs"},{"location":"dev/endpoint/#proton","title":"Proton","text":"url docs explorer https://proton.eosusa.io/v2 docs https://proton.cryptolions.io/v2 docs explorer"},{"location":"dev/endpoint/#fio","title":"FIO","text":"url docs explorer https://fio.cryptolions.io/v2 docs explorer https://fio.eosusa.io/v2 docs"},{"location":"dev/endpoint/#libre","title":"LIBRE","text":"url docs explorer https://libre.eosusa.io/v2 docs"},{"location":"dev/endpoint/#aikonore","title":"Aikon/ORE","text":"url docs explorer https://ore.eosusa.io/v2 docs"},{"location":"dev/endpoint/#coffe","title":"Coffe","text":"url docs explorer https://hyperion.coffe.io/v2 docs"},{"location":"dev/endpoint/#ux","title":"UX","text":"url docs explorer https://ux.eosusa.io/v2 docs"},{"location":"dev/endpoint/#2-testnets","title":"2. Testnets:","text":""},{"location":"dev/endpoint/#kylin-eos-testnet","title":"Kylin EOS Testnet","text":"url docs explorer https://kylin.eossweden.org/v2 docs https://kylin.eosusa.io/v2 docs"},{"location":"dev/endpoint/#jungle-4-eos-testnet","title":"Jungle 4 EOS Testnet","text":"url docs explorer https://jungle.eosusa.io/v2 docs"},{"location":"dev/endpoint/#wax-testnet","title":"WAX Testnet","text":"url docs explorer https://testnet.wax.pink.gg/v2 docs https://wax-test.blokcrafters.io/v2 docs https://test.wax.eosusa.io/v2 docs"},{"location":"dev/endpoint/#telos-testnet","title":"Telos Testnet","text":"url docs explorer https://testnet.telos.net/v2 docs explorer https://test.telos.eosusa.io/v2 docs https://testnet.telos.caleos.io/v2 docs explorer"},{"location":"dev/endpoint/#uos-testnet","title":"UOS Testnet","text":"url docs explorer https://test.ultra.eosusa.io/v2 docs"},{"location":"dev/endpoint/#proton-testnet","title":"Proton Testnet","text":"url docs explorer https://test.proton.eosusa.io/v2 docs https://testnet.protonchain.com/v2 docs explorer"},{"location":"dev/endpoint/#fio-testnet","title":"FIO Testnet","text":"url docs explorer https://test.fio.eosusa.io/v2 docs"},{"location":"dev/endpoint/#libre-testnet","title":"LIBRE Testnet","text":"url docs explorer https://test.libre.eosusa.io/v2 docs"},{"location":"dev/endpoint/#ux-testnet","title":"UX Testnet","text":"url docs explorer https://test.ux.eosusa.io/v2 docs"},{"location":"dev/howtouse/","title":"How to use","text":""},{"location":"dev/howtouse/#1-javascript-client-using-https","title":"1. JavaScript client using https","text":"<p>Fetching data from Hyperion using JavaScript is quite simple. </p> <p>To do that we're going to use https library to make the requests.</p> <p>So, the first step is to include the https lib: <pre><code>const https = require('https');\n</code></pre></p> <p>In this example, we will fetch the WAX chain for the transaction id:  14b36232e919307090c7e9a7a2b17915f40bf0ce72734647c9f8c145c110dda0.</p> <p>This is the query: <pre><code>\"https://wax.hyperion.eosrio.io/v2/history/get_transaction?id=14b36232e919307090c7e9a7a2b17915f40bf0ce72734647c9f8c145c110dda0\"\n</code></pre></p> <p>Now, let's create a Promise function that will receive the tx id as parameter and save it into a variable called <code>getTransaction</code>: <pre><code>let getTransaction = function (args) {\n    let url = \"https://wax.hyperion.eosrio.io/v2/history/get_transaction?id=\" + args;\n    return new Promise(function (resolve) {\n        https.get(url, (resp) =&gt; {\n            let data = '';\n\n            // A chunk of data has been recieved.\n            resp.on('data', (chunk) =&gt; {\n                data += chunk;\n            });\n\n            // The whole response has been received. Print out the result.\n            resp.on('end', () =&gt; {\n                resolve(JSON.parse(data));\n            });\n        });\n\n    })\n};\n</code></pre></p> <p>And finally, let's call the function passing the tx id as parameter: <pre><code>(async() =&gt;{\n  let id = \"14b36232e919307090c7e9a7a2b17915f40bf0ce72734647c9f8c145c110dda0\"\n   getTransaction(id).then(data =&gt; {\n      console.log(data);\n  });\n})();\n</code></pre></p> <p>Request response:</p> <pre><code>{\n  trx_id: '14b36232e919307090c7e9a7a2b17915f40bf0ce72734647c9f8c145c110dda0',\n  lib: 49925737,\n  actions: [\n    {\n      action_ordinal: 1,\n      creator_action_ordinal: 0,\n      act: [Object],\n      context_free: false,\n      elapsed: '0',\n      account_ram_deltas: [Array],\n      '@timestamp': '2020-04-08T23:02:02.500',\n      block_num: 49924003,\n      producer: 'zenblockswax',\n      trx_id: '14b36232e919307090c7e9a7a2b17915f40bf0ce72734647c9f8c145c110dda0',\n      global_sequence: 260804226,\n      cpu_usage_us: 1173,\n      net_usage_words: 36,\n      inline_count: 3,\n      inline_filtered: false,\n      receipts: [Array],\n      code_sequence: 7,\n      abi_sequence: 7,\n      notified: [Array],\n      timestamp: '2020-04-08T23:02:02.500'\n    },\n    {\n      action_ordinal: 2,\n      creator_action_ordinal: 1,\n      act: [Object],\n      context_free: false,\n      elapsed: '0',\n      account_ram_deltas: [Array],\n      '@timestamp': '2020-04-08T23:02:02.500',\n      block_num: 49924003,\n      producer: 'zenblockswax',\n      trx_id: '14b36232e919307090c7e9a7a2b17915f40bf0ce72734647c9f8c145c110dda0',\n      global_sequence: 260804227,\n      receipts: [Array],\n      code_sequence: 6,\n      abi_sequence: 6,\n      notified: [Array],\n      timestamp: '2020-04-08T23:02:02.500'\n    }\n  ],\n  query_time_ms: 45.26\n}\n</code></pre> <p></p>"},{"location":"dev/howtouse/#2-third-party-library","title":"2. Third party library","text":"<p>There is a third party Javascript library made by EOS Cafe.  Refer to their github for further information: https://github.com/eoscafe/hyperion-api</p> <p>Note</p> <p>This is a third party library and is not maintained by EOS Rio</p>"},{"location":"dev/stream_client/","title":"Hyperion Stream Client (v3.6+)","text":"<p>The Hyperion Stream Client is a powerful TypeScript/JavaScript library for connecting to and streaming real-time or historical data from Hyperion History API servers (version 3.6 and newer).</p> <p>It simplifies handling WebSocket connections, message parsing, and stream management.</p> <p>Compatibility Note: Hyperion Stream Client v3.6 is only compatible with Hyperion servers from v3.6 onwards.</p>"},{"location":"dev/stream_client/#key-features","title":"Key Features","text":"<ul> <li>Real-time Data Feeds: Subscribe to live streams of blockchain actions and table state changes (deltas) as they occur.</li> <li>Historical Data Replay: Efficiently fetch and process historical sequences of actions or deltas, with precise control over block ranges.</li> <li>Simplified Development: Avoids the need to implement custom WebSocket handling and data deserialization logic.</li> <li>Flexible Data Consumption: Choose between an intuitive event-driven API (<code>stream.on('message', ...)</code>) or the modern AsyncIterator pattern (<code>for await...of stream</code>) for processing data.</li> <li>Robust Filtering: Utilize Hyperion's server-side filtering to receive only the data relevant to your application, saving bandwidth and client-side processing.</li> <li>Cross-Environment Support: Seamlessly integrate into Node.js (v18+) backends or modern browser-based applications.</li> <li>Automatic Reconnection: Leverages <code>socket.io-client</code>'s robust reconnection capabilities. </li> <li>Debug Mode: Optional detailed logging for development.</li> </ul>"},{"location":"dev/stream_client/#prerequisites","title":"Prerequisites","text":"<ul> <li>Hyperion API Endpoint: Access to a running Hyperion History API (v3.6+) instance that has streaming enabled. You can find a list of public endpoints here.</li> <li>Node.js: Version 18 or higher is required for server-side usage or for building/bundling for the browser. For direct In-Browser usage see here</li> </ul>"},{"location":"dev/stream_client/#installation","title":"Installation","text":"<p>Install the client using npm or yarn:</p> <p><pre><code>npm install @eosrio/hyperion-stream-client --save\n</code></pre> or</p> <pre><code>yarn add @eosrio/hyperion-stream-client\n</code></pre>"},{"location":"dev/stream_client/#getting-started","title":"Getting Started","text":"<p>Ready to dive in?</p> <ul> <li>For a quick hands-on example, see:</li> </ul> <p>Getting Started </p> <ul> <li>To understand how to configure the client, visit:</li> </ul> <p>Client Configuration </p>"},{"location":"dev/stream_client/#further-reading","title":"Further Reading","text":"<p>Explore the following sections to learn more about specific aspects of the Hyperion Stream Client:</p> <ul> <li>Streaming Actions: Detailed guide on requesting and filtering action streams.</li> <li>Streaming Table Deltas: Comprehensive information on streaming table state changes.</li> <li>Handling Stream Data: Learn about the event-driven API, the AsyncIterator pattern, and the structure of received data.</li> <li>Block Range Parameters: Understand how to define <code>start_from</code> and <code>read_until</code>.</li> <li>Browser Usage: Specific instructions for integrating the client in web browsers.</li> <li>Error Handling: Best practices for managing errors and disconnections.</li> </ul>"},{"location":"dev/stream_client/block-ranges/","title":"Block Range Parameters (<code>start_from</code> &amp; <code>read_until</code>)","text":"<p>When requesting action or delta streams using the Hyperion Stream Client, the <code>start_from</code> and <code>read_until</code> parameters are crucial for defining the range of blockchain history you're interested in, or for controlling live data flow. Both parameters accept the same types of values.</p>"},{"location":"dev/stream_client/block-ranges/#overview","title":"Overview","text":"<ul> <li><code>start_from</code>: Defines the point in the blockchain's history (or live feed) from which the stream should begin sending data.</li> <li><code>read_until</code>: Defines the point at which the stream should stop sending data. If <code>ignore_live: false</code> (the default), and <code>read_until</code> is set to a point in the past or <code>0</code>, the stream will continue with live data after historical data is exhausted.</li> </ul> <p>If both <code>start_from</code> and <code>read_until</code> specify a range that has already passed, and <code>ignore_live: true</code>, the stream will send the historical data within that range and then end.</p>"},{"location":"dev/stream_client/block-ranges/#accepted-value-types","title":"Accepted Value Types","text":"<p>Both <code>start_from</code> and <code>read_until</code> accept the following three types of values:</p>"},{"location":"dev/stream_client/block-ranges/#1-positive-number-absolute-block-number","title":"1. Positive Number (Absolute Block Number)","text":"<ul> <li>Description: Represents a specific, absolute block number on the blockchain. The stream will start or stop at this exact block.</li> <li>Type: <code>number</code></li> <li>Example Usage:<ul> <li><code>start_from: 150000000</code> (Start streaming data from block 150,000,000 onwards)</li> <li><code>read_until: 150000100</code> (Stop streaming after block 150,000,100 has been processed)</li> </ul> </li> </ul> <pre><code>// Example: Stream actions from block 10,000,000 to 10,000,050\nconst stream = await client.streamActions({\n  contract: \"eosio.token\",\n  action: \"transfer\",\n  start_from: 10000000,\n  read_until: 10000050,\n  ignore_live: true // Important if you only want this specific historical range\n});\n</code></pre>"},{"location":"dev/stream_client/block-ranges/#2-zero-or-negative-number-relative-to-head-block","title":"2. Zero or Negative Number (Relative to Head Block)","text":"<ul> <li>Description: Represents a block number relative to the current head block of the blockchain at the time the stream request is processed by the Hyperion server (for <code>start_from</code>) or as the stream progresses (for <code>read_until</code> in live mode).<ul> <li><code>0</code>: Represents the current head block.<ul> <li><code>start_from: 0</code>: Start streaming live data from the current head.</li> <li><code>read_until: 0</code>: If <code>start_from</code> is in the past, read history up to the current head, then continue live indefinitely (if <code>ignore_live: false</code>). If <code>start_from</code> is also <code>0</code>, this means stream live indefinitely. If <code>ignore_live: true</code>, read up to the current head and stop.</li> </ul> </li> <li>Negative Number (e.g., <code>-100</code>): Represents N blocks before the current head block.<ul> <li><code>start_from: -100</code>: Start streaming from 100 blocks prior to the current head block.</li> <li><code>read_until: -50</code>: Stop streaming when the block 50 blocks prior to the current head (dynamically, if live) is processed.</li> </ul> </li> </ul> </li> <li>Type: <code>number</code></li> <li>Example Usage:<ul> <li><code>start_from: 0</code> (Start live streaming from the current moment)</li> <li><code>start_from: -500</code> (Start streaming from 500 blocks ago)</li> <li><code>read_until: -10</code> (If live, stop when the block that is 10 blocks behind the then-current head is processed. If historical, stop 10 blocks before the head at the end of the historical replay).</li> </ul> </li> </ul> <pre><code>// Example: Stream the last 100 blocks of actions and then continue live\nconst stream = await client.streamActions({\n  contract: \"eosio.token\",\n  action: \"transfer\",\n  start_from: -100,\n  read_until: 0, // Continue live\n  // ignore_live: false (default)\n});\n\n// Example: Stream only the last 20 blocks and then stop\nconst historicalStream = await client.streamActions({\n  contract: \"eosio.token\",\n  action: \"transfer\",\n  start_from: -20,\n  read_until: 0,    // Read up to current head\n  ignore_live: true // Then stop\n});\n</code></pre>"},{"location":"dev/stream_client/block-ranges/#3-iso-8601-timestamp-string-specific-point-in-time","title":"3. ISO 8601 Timestamp String (Specific Point in Time)","text":"<ul> <li>Description: Represents a specific date and time. Hyperion will find the block produced at or closest (usually just after) this timestamp to start or stop the stream. The timestamp should be in UTC.</li> <li>Format: <code>YYYY-MM-DDTHH:mm:ss.sssZ</code> (milliseconds <code>.sss</code> are optional; <code>Z</code> denotes UTC).</li> <li>Type: <code>string</code></li> <li>Example Usage:<ul> <li><code>start_from: \"2023-01-01T00:00:00Z\"</code> (Start streaming from the first block on or after Jan 1, 2023, 00:00 UTC)</li> <li><code>read_until: \"2023-01-31T23:59:59.999Z\"</code> (Stop streaming after the last block on or before Jan 31, 2023, 23:59:59.999 UTC)</li> </ul> </li> </ul> <pre><code>// Example: Stream actions from a specific hour on a specific day\nconst stream = await client.streamActions({\n  contract: \"eosio.system\",\n  action: \"voteproducer\",\n  start_from: \"2024-03-15T14:00:00Z\",\n  read_until: \"2024-03-15T15:00:00Z\",\n  ignore_live: true\n});\n</code></pre>"},{"location":"dev/stream_client/block-ranges/#common-scenarios-and-combinations","title":"Common Scenarios and Combinations","text":"<ul> <li> <p>Live Streaming from Now:</p> <ul> <li><code>start_from: 0</code></li> <li><code>read_until: 0</code> (or omitted, with <code>ignore_live: false</code>)</li> </ul> </li> <li> <p>Recent History then Live:</p> <ul> <li><code>start_from: -N</code> (e.g., <code>-1000</code> for last 1000 blocks)</li> <li><code>read_until: 0</code> (or omitted, with <code>ignore_live: false</code>)</li> </ul> </li> <li> <p>Specific Historical Range Only:</p> <ul> <li><code>start_from: &lt;block_num_A | timestamp_A&gt;</code></li> <li><code>read_until: &lt;block_num_B | timestamp_B&gt;</code></li> <li><code>ignore_live: true</code> (crucial to ensure it stops after the range)</li> </ul> </li> <li> <p>All History from a Point then Live:</p> <ul> <li><code>start_from: &lt;block_num | timestamp&gt;</code></li> <li><code>read_until: 0</code> (or omitted, with <code>ignore_live: false</code>)</li> </ul> </li> <li> <p>Stream Only the Last N Blocks and Stop:</p> <ul> <li><code>start_from: -N</code></li> <li><code>read_until: 0</code> (Hyperion reads up to the head when historical fetch completes)</li> <li><code>ignore_live: true</code></li> </ul> </li> </ul>"},{"location":"dev/stream_client/block-ranges/#important-considerations","title":"Important Considerations","text":"<ul> <li><code>ignore_live: true</code>: If you want the stream to definitively stop after processing a historical range (defined by <code>start_from</code> and <code>read_until</code>), you must set <code>ignore_live: true</code>. Otherwise, if <code>read_until</code> is <code>0</code> or a point in the past, the stream will transition to live data.</li> <li>Server-Side Resolution: The exact block numbers corresponding to negative offsets or timestamps are determined by the Hyperion server.</li> <li>Block Production Rate: When using negative numbers or timestamps, be mindful of the blockchain's block production rate to estimate the number of blocks or duration.</li> <li>Stream Termination: When <code>read_until</code> is reached and <code>ignore_live: true</code>, the stream will naturally end. If using the AsyncIterator pattern, it will yield <code>null</code>. If using the event-driven API, no more <code>'message'</code> events will be emitted for that stream. You might also receive a specific \"history_end\" type message from the server internally, which the client handles to terminate the async iterator.</li> </ul> <p>Understanding these parameters allows you to precisely control the data window for your streams, whether you're replaying history or tapping into the live pulse of the blockchain.</p>"},{"location":"dev/stream_client/block-ranges/#next-steps","title":"Next Steps","text":"<ul> <li>Streaming Actions: See how these parameters are used in Streaming Actions.</li> <li>Streaming Table Deltas: See their application in Streaming Table Deltas.</li> <li>Handling Stream Data: Learn how to process the data you receive in Handling Stream Data.</li> </ul>"},{"location":"dev/stream_client/browser-usage/","title":"Browser Usage","text":"<p>The Hyperion Stream Client is designed to work in modern web browsers, allowing you to build front-end applications that can directly stream and react to blockchain data from a Hyperion API.</p> <p>There are several ways to include and use the client in your browser-based projects:</p>"},{"location":"dev/stream_client/browser-usage/#1-es-modules-with-a-bundler-recommended-for-modern-applications","title":"1. ES Modules with a Bundler (Recommended for Modern Applications)","text":"<p>If you're using a modern JavaScript framework like React, Vue, Angular, or a build tool like Webpack, Rollup, or Vite, you can install the client via npm/yarn and import it directly as an ES Module.</p> <p>Installation (if not already done): <pre><code>npm install @eosrio/hyperion-stream-client --save\n# or\nyarn add @eosrio/hyperion-stream-client\n</code></pre></p> <p>Usage in your JavaScript/TypeScript code:</p> <pre><code>import { HyperionStreamClient } from \"@eosrio/hyperion-stream-client\";\n\n// Example: Initialize client in a component or main script\nconst client = new HyperionStreamClient({\n  endpoint: \"https://eos.hyperion.eosrio.io\", // Your Hyperion endpoint\n  // other options...\n});\n\nasync function startStreaming() {\n  try {\n    await client.connect();\n    console.log(\"Connected to Hyperion in the browser!\");\n\n    const stream = await client.streamActions({\n      contract: \"eosio.token\",\n      action: \"transfer\",\n      start_from: 0, // Live data\n      filters: [{ field: \"act.data.to\", value: \"somebrowserapp\" }]\n    });\n\n    stream.on(\"message\", (data) =&gt; {\n      // Update your UI or application state with the new data\n      console.log(\"Browser received action:\", data.content.act.data);\n      const displayElement = document.getElementById(\"stream-output\");\n      if (displayElement) {\n        displayElement.innerHTML += `&lt;p&gt;Transfer: ${JSON.stringify(data.content.act.data)}&lt;/p&gt;`;\n      }\n    });\n\n  } catch (error) {\n    console.error(\"Browser streaming error:\", error);\n  }\n}\n\n// Call startStreaming() when appropriate (e.g., on component mount, button click)\n// Ensure the DOM is ready if you're manipulating it directly as in the example.\n// document.addEventListener('DOMContentLoaded', startStreaming);\nstartStreaming(); // Or trigger as needed\n</code></pre> <p>Your bundler will handle resolving the module and including it in your final application bundle.</p>"},{"location":"dev/stream_client/browser-usage/#2-umd-universal-module-definition-bundle","title":"2. UMD (Universal Module Definition) Bundle","text":"<p>The client is also distributed as a UMD bundle, which can be included directly in an HTML file using a <code>&lt;script&gt;</code> tag. This makes the <code>HyperionStreamClient</code> available as a global variable (e.g., <code>window.HyperionStreamClient</code> or just <code>HyperionStreamClient</code>).</p> <p>This method is suitable for simpler projects, quick prototypes, or when not using a module bundler.</p> <p>You can source the UMD bundle from:</p> <ul> <li>A CDN like unpkg or jsDelivr.</li> <li>Your local <code>node_modules</code> directory after installation (<code>node_modules/@eosrio/hyperion-stream-client/dist/hyperion-stream-client.js</code>).</li> </ul> <p>Example HTML:</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n  &lt;title&gt;Hyperion Stream Client - UMD Example&lt;/title&gt;\n  &lt;!-- Option 1: From CDN --&gt;\n  &lt;script src=\"https://unpkg.com/@eosrio/hyperion-stream-client/dist/hyperion-stream-client.js\"&gt;&lt;/script&gt;\n\n  &lt;!-- Option 2: From local path (if you've copied it or are serving node_modules) --&gt;\n  &lt;!-- &lt;script src=\"/path/to/hyperion-stream-client.js\"&gt;&lt;/script&gt; --&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;h1&gt;Hyperion Stream Output&lt;/h1&gt;\n  &lt;div id=\"stream-output\"&gt;&lt;/div&gt;\n\n  &lt;script&gt;\n    const client = new HyperionStreamClient({ // or window.HyperionStreamClient\n      endpoint: \"https://wax.hyperion.eosrio.io\",\n      debug: true\n    });\n\n    async function initializeStream() {\n      client.on('connect', () =&gt; console.log('UMD Client Connected!'));\n      client.on('error', (err) =&gt; console.error('UMD Client Error:', err));\n\n      try {\n        await client.connect();\n\n        const stream = await client.streamActions({\n          contract: \"eosio.token\",\n          action: \"transfer\",\n          start_from: -5, // Last 5 and live\n          read_until: 0\n        });\n\n        const outputDiv = document.getElementById('stream-output');\n        stream.on(\"message\", (data) =&gt; {\n          console.log(\"UMD received:\", data.content);\n          if (outputDiv) {\n            const p = document.createElement('p');\n            p.textContent = `Block ${data.content.block_num}: ${data.content.act.name} - ${JSON.stringify(data.content.act.data)}`;\n            outputDiv.appendChild(p);\n          }\n        });\n      } catch (e) {\n        console.error(\"Error setting up UMD stream:\", e);\n      }\n    }\n\n    initializeStream();\n  &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"dev/stream_client/browser-usage/#3-import-maps-modern-browsers","title":"3. Import Maps (Modern Browsers)","text":"<p>Import Maps are a newer browser feature that allows you to control how JavaScript modules are resolved, similar to how <code>paths</code> work in <code>tsconfig.json</code> or aliases in Webpack. This lets you use bare module specifiers (like <code>import { HyperionStreamClient } from \"@eosrio/hyperion-stream-client\";</code>) directly in <code>&lt;script type=\"module\"&gt;</code> tags without a build step, provided the browser supports import maps.</p> <p>Browser Compatibility: Import maps are supported in Chrome 89+, Edge 89+, Safari 16.4+, and Firefox 108+. For older browsers, you'll need a polyfill like es-module-shims.</p> <p>There are two main approaches with import maps:</p>"},{"location":"dev/stream_client/browser-usage/#31-mapping-to-the-umd-bundle-simpler","title":"3.1. Mapping to the UMD Bundle (Simpler)","text":"<p>This approach maps the <code>@eosrio/hyperion-stream-client</code> specifier to its UMD bundle. It's simpler because the UMD bundle has its dependencies (like <code>socket.io-client</code>, <code>async</code>) already bundled or handled internally.</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n  &lt;title&gt;Hyperion - Import Map (UMD) Example&lt;/title&gt;\n  &lt;!-- Optional: Polyfill for older browsers --&gt;\n  &lt;!-- &lt;script async src=\"https://ga.jspm.io/npm:es-module-shims@1.8.0/dist/es-module-shims.js\"&gt;&lt;/script&gt; --&gt;\n\n  &lt;script type=\"importmap\"&gt;\n  {\n    \"imports\": {\n      \"@eosrio/hyperion-stream-client\": \"https://unpkg.com/@eosrio/hyperion-stream-client/dist/hyperion-stream-client.js\"\n    }\n  }\n  &lt;/script&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;h1&gt;Import Map (UMD) Output&lt;/h1&gt;\n  &lt;div id=\"output\"&gt;&lt;/div&gt;\n\n  &lt;script type=\"module\"&gt;\n    import { HyperionStreamClient } from \"@eosrio/hyperion-stream-client\";\n\n    const client = new HyperionStreamClient({\n      endpoint: \"https://jungle4.dfuse.eosnation.io\", // Example public testnet endpoint\n      debug: true\n    });\n\n    const outputDiv = document.getElementById('output');\n\n    client.on('connect', () =&gt; outputDiv.innerHTML += '&lt;p&gt;Import Map (UMD) Client Connected!&lt;/p&gt;');\n    client.on('error', (err) =&gt; outputDiv.innerHTML += `&lt;p style=\"color:red;\"&gt;Import Map (UMD) Client Error: ${err}&lt;/p&gt;`);\n\n    async function start() {\n      await client.connect();\n      const stream = await client.streamActions({ contract: \"eosio\", action: \"onblock\", start_from: -1, read_until: 0, ignore_live: true });\n      stream.on('message', data =&gt; {\n        outputDiv.innerHTML += `&lt;p&gt;Block ${data.content.block_num} by ${data.content.producer}&lt;/p&gt;`;\n      });\n    }\n    start();\n  &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"dev/stream_client/browser-usage/#32-mapping-to-the-esm-bundle-more-complex-requires-dependency-mapping","title":"3.2. Mapping to the ESM Bundle (More Complex, Requires Dependency Mapping)","text":"<p>This approach maps directly to the client's ES Module entry point (e.g., <code>lib/esm/index.js</code>). This is cleaner in terms of module purity but requires you to also map all of its direct and transitive dependencies (like <code>socket.io-client</code>, <code>async</code>, <code>cross-fetch</code>) in the import map.</p> <p><pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n  &lt;title&gt;Hyperion - Import Map (ESM) Example&lt;/title&gt;\n  &lt;!-- Optional: Polyfill for older browsers --&gt;\n  &lt;!-- &lt;script async src=\"https://ga.jspm.io/npm:es-module-shims@1.8.0/dist/es-module-shims.js\"&gt;&lt;/script&gt; --&gt;\n\n  &lt;script type=\"importmap\"&gt;\n  {\n    \"imports\": {\n      \"@eosrio/hyperion-stream-client\": \"https://unpkg.com/@eosrio/hyperion-stream-client/lib/esm/index.js\",\n      \"socket.io-client\": \"https://cdn.jsdelivr.net/npm/socket.io-client@4.7.5/+esm\",   \u0627\u0646\u0631\u0698\u06cc Ensure version matches package.json\n      \"async\": \"https://cdn.jsdelivr.net/npm/async@3.2.5/+esm\",                         // Ensure version matches\n      \"cross-fetch\": \"https://cdn.jsdelivr.net/npm/cross-fetch@4.0.0/+esm\"             // Ensure version matches\n      // You might need to map further dependencies of socket.io-client, etc.\n      // if they are not self-contained in their +esm distributions.\n    }\n  }\n  &lt;/script&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;h1&gt;Import Map (ESM) Output&lt;/h1&gt;\n  &lt;div id=\"output-esm\"&gt;&lt;/div&gt;\n\n  &lt;script type=\"module\"&gt;\n    import { HyperionStreamClient } from \"@eosrio/hyperion-stream-client\";\n\n    const client = new HyperionStreamClient({\n      endpoint: \"https://telos.eosusa.io\", // Example public endpoint\n      debug: true\n    });\n    const outputDiv = document.getElementById('output-esm');\n    // ... rest of the client usage logic as in the UMD import map example ...\n    client.on('connect', () =&gt; outputDiv.innerHTML += '&lt;p&gt;Import Map (ESM) Client Connected!&lt;/p&gt;');\n    // ...\n    async function startEsm() {\n      await client.connect();\n      // ...\n    }\n    startEsm();\n  &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> Note on ESM Dependency Mapping: Keeping track of and correctly mapping all transitive dependencies for the direct ESM approach can be challenging and error-prone. For simpler browser use without a bundler, the UMD bundle (either directly or via an import map) is often more straightforward.</p>"},{"location":"dev/stream_client/browser-usage/#considerations-for-browser-environments","title":"Considerations for Browser Environments","text":"<ul> <li>CORS (Cross-Origin Resource Sharing): The Hyperion API endpoint you are connecting to must have CORS headers configured correctly to allow connections from the domain your web application is served from. If not, you'll encounter CORS errors in the browser console.</li> <li>WebSocket Support: All modern browsers support WebSockets, which the client uses under the hood via <code>socket.io-client</code>.</li> <li>Resource Management: Be mindful of the number of streams and the volume of data being processed in the browser, as it can impact performance and memory usage. Stop streams when they are no longer needed.</li> <li>Error Handling: Implement robust error handling for connection issues and stream errors to provide a good user experience.</li> </ul>"},{"location":"dev/stream_client/browser-usage/#next-steps","title":"Next Steps","text":"<ul> <li>Client Configuration: Learn about all the options for <code>new HyperionStreamClient()</code> in Client Configuration.</li> <li>Streaming Actions: Dive into the details of requesting action streams in Streaming Actions.</li> <li>Streaming Table Deltas: Understand how to stream table changes in Streaming Table Deltas.</li> <li>Handling Stream Data: Explore methods for processing data in Handling Stream Data.</li> </ul>"},{"location":"dev/stream_client/configuration/","title":"Client Configuration","text":"<p>When you create an instance of the <code>HyperionStreamClient</code>, you can pass a configuration object to customize its behavior. This guide details each available option.</p>"},{"location":"dev/stream_client/configuration/#basic-instantiation","title":"Basic Instantiation","text":"<p>Here's a typical instantiation with some common options:</p> <pre><code>import { HyperionStreamClient } from \"@eosrio/hyperion-stream-client\";\n\nconst client = new HyperionStreamClient({\n  // Required: Hyperion API endpoint\n  endpoint: \"https://eos.hyperion.eosrio.io\",\n\n  // Optional: Enable debug logging (default: false)\n  debug: false,\n\n  // Optional: Stream irreversible blocks only (default: false)  \n  libStream: false,\n\n  // Optional: Monitor last irreversible block and emit 'libUpdate' events. (default: false)\n  libMonitor: false,\n\n  // Optional: Connection timeout in ms (default: 5000)\n  connectionTimeout: 5000\n});\n</code></pre>"},{"location":"dev/stream_client/configuration/#configuration-options","title":"Configuration Options","text":"<p>The constructor <code>new HyperionStreamClient(options)</code> accepts an <code>options</code> object with the following properties:</p>"},{"location":"dev/stream_client/configuration/#endpoint","title":"<code>endpoint</code>","text":"<ul> <li>Type: <code>string</code></li> <li>Required: Yes</li> <li>Description: The base HTTP(S) URL of the Hyperion API server you want to connect to. The client will automatically manage WebSocket connections to the <code>/stream</code> path on this endpoint.</li> <li>Example: <code>\"https://wax.hyperion.eosrio.io\"</code>, <code>\"http://localhost:7000\"</code></li> </ul> <pre><code>const client = new HyperionStreamClient({\n  endpoint: \"https://telos.hyperion.eosrio.io\"\n});\n</code></pre>"},{"location":"dev/stream_client/configuration/#debug","title":"<code>debug</code>","text":"<ul> <li>Type: <code>boolean</code></li> <li>Optional: Yes</li> <li>Default: <code>false</code></li> <li>Description: If set to <code>true</code>, the client will output verbose debugging messages to the console. This is useful during development to understand connection states, message flows, and potential issues.</li> <li>Example:     <pre><code>const client = new HyperionStreamClient({\n  endpoint: \"https://eos.hyperion.eosrio.io\",\n  debug: true\n});\n</code></pre></li> </ul>"},{"location":"dev/stream_client/configuration/#libstream","title":"<code>libStream</code>","text":"<ul> <li>Type: <code>boolean</code></li> <li>Optional: Yes</li> <li>Default: <code>false</code></li> <li>Description:<ul> <li>If <code>true</code>, data messages from streams will be buffered internally until they are confirmed as part of an irreversible block (LIB - Last Irreversible Block).</li> <li>When a message becomes irreversible:<ul> <li>If you are listening to <code>client.on('libData', ...)</code>: The message will be emitted through this event.</li> <li>If you are listening to <code>stream.on('message', ...)</code> for a specific stream: The <code>irreversible</code> property of the <code>IncomingData</code> object will be set to <code>true</code>.</li> </ul> </li> <li>This option introduces latency as data is held until LIB, but it ensures that your application only processes data that is final and will not be reversed due to a microfork.</li> <li>If <code>false</code>, data is emitted as soon as it's received from Hyperion, regardless of its irreversibility status (though <code>data.mode</code> will still indicate \"live\" or \"history\").</li> </ul> </li> <li>Example:     <pre><code>const client = new HyperionStreamClient({\n  endpoint: \"https://eos.hyperion.eosrio.io\",\n  libStream: true\n});\n\n// Option 1: Listen on the client for all irreversible data\nclient.on('libData', (data) =&gt; {\n  console.log('Irreversible data (client-level):', data.content);\n});\n\n// Option 2: Read individual stream messages (irreversable only, reversible messages will not be emitted)\nconst stream = await client.streamActions({...});\nstream.on('message', (data) =&gt; {\n    console.log('Irreversible data (stream-level):', data.content);\n});\n</code></pre></li> </ul>"},{"location":"dev/stream_client/configuration/#libmonitor","title":"<code>libMonitor</code>","text":"<ul> <li>Type: <code>boolean</code></li> <li>Optional: Yes</li> <li>Default: <code>false</code></li> <li>Description: If set to <code>true</code>, the client will actively monitor and listen for Last Irreversible Block (LIB) updates from the Hyperion server. When a new LIB is reported, the client will emit a <code>libUpdate</code> event with the LIB data (<code>{ chain_id, block_num, block_id }</code>). This is useful for applications that need to track the chain's finality status independently of data streams.</li> <li>Example:     <pre><code>const client = new HyperionStreamClient({\n  endpoint: \"https://eos.hyperion.eosrio.io\",\n  libMonitor: true\n});\n\nclient.on('libUpdate', (libInfo) =&gt; {\n  console.log(`New LIB: Block ${libInfo.block_num}, ID: ${libInfo.block_id}`);\n});\n</code></pre></li> </ul>"},{"location":"dev/stream_client/configuration/#connectiontimeout","title":"<code>connectionTimeout</code>","text":"<ul> <li>Type: <code>number</code></li> <li>Optional: Yes</li> <li>Default: <code>5000</code> (milliseconds)</li> <li>Description: The timeout duration in milliseconds for the initial WebSocket connection attempt to the Hyperion server. If the connection is not established within this time, the <code>client.connect()</code> promise will reject, and a <code>connect_error</code> event may be emitted internally by <code>socket.io-client</code>.</li> <li>Example:     <pre><code>const client = new HyperionStreamClient({\n  endpoint: \"https://eos.hyperion.eosrio.io\",\n  connectionTimeout: 10000 // 10 seconds\n});\n</code></pre></li> </ul>"},{"location":"dev/stream_client/configuration/#chainapi-advancedrarely-needed","title":"<code>chainApi</code> (Advanced/Rarely Needed)","text":"<ul> <li>Type: <code>string</code></li> <li>Optional: Yes</li> <li>Default: <code>undefined</code></li> <li>Description: The HTTP(S) URL of a standard Antelope chain API endpoint (e.g., a <code>nodeos</code> instance). This is not typically required for basic streaming operations, as the Hyperion endpoint itself usually provides necessary chain metadata. It might be used in specific scenarios if the client needs to fetch chain information (like <code>get_info</code>) directly from a node separate from the Hyperion instance, but the current client implementation primarily relies on the Hyperion endpoint for its operational data.</li> <li>Example:     <pre><code>const client = new HyperionStreamClient({\n  endpoint: \"https://eos.hyperion.eosrio.io\",\n  // chainApi: \"https://eos.greymass.com\" // Example, usually not needed\n});\n</code></pre></li> </ul>"},{"location":"dev/stream_client/configuration/#summary-table","title":"Summary Table","text":"Option Type Required Default Description <code>endpoint</code> <code>string</code> Yes Base URL of the Hyperion API server. <code>debug</code> <code>boolean</code> No <code>false</code> Enable/disable verbose console debug logging. <code>libStream</code> <code>boolean</code> No <code>false</code> Stream only irreversible data (introduces latency). <code>libMonitor</code> <code>boolean</code> No <code>false</code> Monitor and emit Last Irreversible Block updates. <code>connectionTimeout</code> <code>number</code> No <code>5000</code> (ms) Timeout for the initial WebSocket connection. <code>chainApi</code> <code>string</code> No <code>undefined</code> (Advanced) URL of a standard Antelope chain API. Usually not needed."},{"location":"dev/stream_client/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Getting Started: If you haven't already, check out the Getting Started Guide for a practical example.</li> <li>Streaming Actions: Learn how to request and filter action streams in Streaming Actions.</li> <li>Streaming Table Deltas: Discover how to stream table state changes in Streaming Table Deltas.</li> </ul>"},{"location":"dev/stream_client/data-handling/","title":"Handling Stream Data","text":"<p>Once you've initiated a stream for actions or table deltas using the Hyperion Stream Client, you need a way to process the incoming data. The client offers two primary mechanisms for this: </p> <ul> <li>the Event-Driven API (<code>stream.on('message', ...)</code>)</li> <li>or the AsyncIterator Pattern (<code>for await...of</code>).</li> </ul> <p>This guide also details the structure of the data messages you'll receive.</p>"},{"location":"dev/stream_client/data-handling/#1-data-consumption-methods","title":"1. Data Consumption Methods","text":"<p>You can choose the method that best fits your application's architecture and coding style.</p>"},{"location":"dev/stream_client/data-handling/#11-event-driven-api-streamonmessage","title":"1.1. Event-Driven API (<code>stream.on('message', ...)</code>)","text":"<p>This is a reactive approach where you register a callback function (handler) that gets executed every time a new data message arrives on the stream.</p> <p>How it works: Each stream object returned by <code>client.streamActions()</code> or <code>client.streamDeltas()</code> is an event emitter.</p> <pre><code>import { HyperionStreamClient, IncomingData, ActionContent, DeltaContent } from \"@eosrio/hyperion-stream-client\";\n// Assuming 'client' is an initialized and connected HyperionStreamClient instance\n\nasync function useEventDrivenAPI() {\n  try {\n    const actionStream = await client.streamActions({\n      contract: \"eosio.token\",\n      action: \"transfer\",\n      start_from: -10, // Last 10 blocks and then live\n      replayOnReconnect: true\n    });\n\n    actionStream.on(\"message\", (data: IncomingData&lt;ActionContent&gt;) =&gt; {\n      console.log(`[EVENT] Action (Block ${data.content.block_num}):`, data.content.act.data);\n      // Process the 'data' object here\n    });\n\n    actionStream.on(\"error\", (error) =&gt; {\n      console.error(\"[EVENT] Stream Error:\", error);\n    });\n\n    actionStream.on(\"start\", (startInfo) =&gt; {\n      console.log(\"[EVENT] Stream Started:\", startInfo);\n    });\n\n    // You can also listen for 'message' on delta streams\n    const deltaStream = await client.streamDeltas({\n      code: \"eosio.token\",\n      table: \"accounts\",\n      scope: \"eosio\",\n      start_from: 0, // Live\n    });\n\n    deltaStream.on(\"message\", (data: IncomingData&lt;DeltaContent&gt;) =&gt; {\n      console.log(`[EVENT] Delta (Block ${data.content.block_num}):`, data.content.data);\n    });\n\n  } catch (error) {\n    console.error(\"Failed to set up streams for event-driven API:\", error);\n  }\n}\n</code></pre> <p>Pros:</p> <ul> <li>Well-suited for applications that need to react immediately to incoming data.</li> <li>Natural fit for UIs or systems that update based on real-time events.</li> <li>Allows multiple listeners for the same stream if needed (though less common).</li> </ul> <p>Cons:</p> <ul> <li>Can lead to \"callback hell\" if complex sequential processing is required.</li> <li>Managing state across multiple asynchronous callbacks can sometimes be more complex.</li> </ul>"},{"location":"dev/stream_client/data-handling/#12-asynciterator-pattern-for-awaitof","title":"1.2. AsyncIterator Pattern (<code>for await...of</code>)","text":"<p>This modern JavaScript feature allows you to consume data from the stream in a way that looks synchronous, making sequential processing much cleaner, especially when combined with <code>async/await</code>.</p> <p>How it works: The stream objects are also Async Iterables.</p> <pre><code>import { HyperionStreamClient, IncomingData, ActionContent } from \"@eosrio/hyperion-stream-client\";\n// Assuming 'client' is an initialized and connected HyperionStreamClient instance\n\nasync function useAsyncIterator() {\n  try {\n    const actionStream = await client.streamActions({\n      contract: \"eosio.token\",\n      action: \"transfer\",\n      start_from: -5,\n      read_until: 0, // Get 5 historical blocks then stop for this example\n      ignore_live: true,\n    });\n\n    console.log(\"Processing actions with AsyncIterator...\");\n    for await (const message of actionStream) {\n      // The stream yields `null` when it ends cleanly (e.g., `read_until` is reached and `ignore_live: true`).\n      if (message === null) {\n        console.log(\"[ITERATOR] Stream ended.\");\n        break;\n      }\n\n      // 'message' is an IncomingData&lt;ActionContent&gt; object\n      console.log(`[ITERATOR] Action (Block ${message.content.block_num}):`, message.content.act.data);\n      // Perform asynchronous operations if needed before processing the next message\n      // await someAsyncProcessing(message.content);\n    }\n    console.log(\"[ITERATOR] Finished processing all messages.\");\n\n  } catch (error) {\n    console.error(\"[ITERATOR] Error during stream iteration:\", error);\n  }\n}\n</code></pre> <p>Pros:</p> <ul> <li>Significantly improves readability for sequential data processing.</li> <li>Works naturally with <code>async/await</code> for performing asynchronous tasks per message without complex callback management.</li> <li>Easier to manage state within the loop.</li> <li>Provides a clear way to detect when a finite stream has ended (by yielding <code>null</code>).</li> </ul> <p>Cons:</p> <ul> <li>If you need to process multiple messages concurrently without waiting for the previous one to finish (and order doesn't strictly matter), the event-driven approach might be more direct, though you can also manage concurrency within an async iterator loop.</li> </ul> <p>Choosing a Method:</p> <ul> <li>For simple, reactive updates or when you need to handle messages as fast as they arrive without strict order for processing logic: Event-Driven API.</li> <li>For processing messages one by one, especially if each involves async operations or complex state management: AsyncIterator Pattern.</li> </ul>"},{"location":"dev/stream_client/data-handling/#2-message-payload-structure","title":"2. Message Payload Structure","text":"<p>Regardless of how you consume the data, each message you receive from a stream will be an object conforming to the <code>IncomingData&lt;T&gt;</code> interface.</p>"},{"location":"dev/stream_client/data-handling/#incomingdatat","title":"<code>IncomingData&lt;T&gt;</code>","text":"<pre><code>interface IncomingData&lt;T extends ActionContent | DeltaContent&gt; {\n  /** Unique identifier for the stream request that generated this message. */\n  uuid: string;\n\n  /** Type of data payload: \"action\" or \"delta\". */\n  type: \"action\" | \"delta\";\n\n  /**\n   * Mode of the data:\n   * - \"history\": Data replayed from historical blocks.\n   * - \"live\": Data from new, live blocks.\n   */\n  mode: \"live\" | \"history\";\n\n  /**\n   * The actual data payload, which will be either ActionContent or DeltaContent.\n   */\n  content: T;\n\n  /**\n   * Indicates if the data is from an irreversible block.\n   * This is primarily relevant if the client was configured with `libStream: true`.\n   * If `libStream: false` (default), this will likely always be `false` for messages\n   * received via `stream.on('message', ...)` or the async iterator, as those are\n   * emitted immediately. Use `client.on('libData', ...)` for an irreversible-only feed\n   * when `libStream: true`.\n   */\n  irreversible: boolean;\n}\n</code></pre>"},{"location":"dev/stream_client/data-handling/#actioncontent-when-type-is-action","title":"<code>ActionContent</code> (when <code>type</code> is \"action\")","text":"<p>This object contains the details of a specific action trace. Key fields include:</p> <p><pre><code>interface ActionContent {\n  /** ISO 8601 timestamp of the block containing the action. */\n  \"@timestamp\": string;\n  /** Global sequence number of this action. */\n  global_sequence: number;\n  /** Block number where the action occurred. */\n  block_num: number;\n  /** Transaction ID the action belongs to. */\n  trx_id: string;\n  /** Producer of the block. */\n  producer: string;\n  /** Array of accounts notified by this action. */\n  notified: string[];\n\n  /** Details of the action itself. */\n  act: {\n    /** Account that executed the action (the contract). */\n    account: string;\n    /** Name of the action. */\n    name: string;\n    /** Authorization array (actor, permission). */\n    authorization: { actor: string; permission: string; }[];\n    /**\n     * Decoded action data. The structure of this object depends on the action's ABI.\n     * For common actions like `eosio.token::transfer`, this will be well-structured.\n     * For custom actions, it might be a hex string if Hyperion doesn't have the ABI\n     * for that block, or a JSON object if it does.\n     */\n    data: any;\n    /** Hex representation of the action data (often present if `data` is an object). */\n    hex_data?: string;\n  };\n\n  /** CPU usage in microseconds for this action. */\n  cpu_usage_us?: number;\n  /** Net usage in words for this action. */\n  net_usage_words?: number;\n\n  /**\n   * Special decoded fields provided by Hyperion (prefixed with '@').\n   * Example for eosio.token::transfer:\n   * \"@transfer.from\"?: string;\n   * \"@transfer.to\"?: string;\n   * \"@transfer.quantity\"?: string;\n   * \"@transfer.memo\"?: string;\n   */\n  [key: string]: any; // Allows for these @-prefixed fields\n}\n</code></pre> Note: The <code>functions.replaceMetaFields</code> utility within the client automatically moves data from fields like <code>@transfer</code> into <code>act.data</code> if <code>act.data</code> is initially missing or incomplete for that specific meta field. This helps normalize the data structure.</p>"},{"location":"dev/stream_client/data-handling/#deltacontent-when-type-is-delta","title":"<code>DeltaContent</code> (when <code>type</code> is \"delta\")","text":"<p>This object contains details about a change to a row in a contract's table. Key fields include:</p> <p><pre><code>interface DeltaContent {\n  /** ISO 8601 timestamp of the block containing the delta. */\n  \"@timestamp\": string;\n  /** Contract account that owns the table. */\n  code: string;\n  /** Scope of the table where the change occurred. */\n  scope: string;\n  /** Name of the table. */\n  table: string;\n  /** Primary key of the row (as a string). */\n  primary_key: string;\n  /** Account that paid for the RAM for this row. */\n  payer: string;\n  /**\n   * Indicates if the row is present after this delta operation.\n   * - `1`: Row exists (was created or updated).\n   * - `0`: Row was deleted.\n   */\n  present: number;\n  /** Block number where the delta occurred. */\n  block_num: number;\n  /** Block ID where the delta occurred. */\n  block_id: string; // Typically the block ID hash\n\n  /**\n   * Decoded row data after the change. The structure depends on the table's ABI.\n   * If `present` is 0 (deleted), this `data` object might represent the state *before* deletion\n   * or be minimal, depending on the Hyperion version and configuration.\n   */\n  data: Record&lt;string, any&gt;;\n\n  /**\n   * Special decoded fields provided by Hyperion (prefixed with '@').\n   * Example for a table named 'mydata':\n   * \"@mydata.field1\"?: any;\n   */\n  [key: string]: any; // Allows for these @-prefixed fields\n}\n</code></pre> Note: Similar to <code>ActionContent</code>, <code>functions.replaceMetaFields</code> also works on <code>DeltaContent</code> to move data from fields like <code>@tablename.data</code> into the main <code>data</code> object if it's initially missing for those meta fields.</p>"},{"location":"dev/stream_client/data-handling/#handling-data-irreversibility","title":"Handling Data Irreversibility","text":"<ul> <li> <p>If you initialize the client with <code>libStream: true</code>:</p> <ul> <li>Data messages processed via <code>stream.on(\"message\", ...)</code> or the <code>for await...of</code> loop will have their <code>irreversible</code> flag set to <code>true</code> only when the data has become irreversible.</li> <li>Alternatively, you can listen to <code>client.on(\"libData\", (data) =&gt; { ... })</code> for a global feed of all irreversible data from all streams associated with that client instance.</li> </ul> </li> <li> <p>If <code>libStream: false</code> (default):</p> <ul> <li>The <code>irreversible</code> flag on messages from <code>stream.on(\"message\", ...)</code> or the async iterator will generally be <code>false</code>. Your application receives data as soon as Hyperion processes it, which might be before it's irreversible.</li> <li>You can still monitor irreversibility using <code>client.on(\"libUpdate\", ...)</code> if <code>libMonitor: true</code>.</li> </ul> </li> </ul> <p>Choose the approach based on your application's tolerance for potentially reversible data.</p>"},{"location":"dev/stream_client/data-handling/#next-steps","title":"Next Steps","text":"<ul> <li>Streaming Actions: Learn the specifics of requesting action streams in Streaming Actions.</li> <li>Streaming Table Deltas: Understand how to stream table state changes in Streaming Table Deltas.</li> <li>Error Handling: Review best practices in Error Handling.</li> </ul>"},{"location":"dev/stream_client/error-handling/","title":"Error Handling","text":"<p>Data streams can be interrupted, server requests can fail, and unexpected issues can arise. This guide outlines common error scenarios and best practices for handling them.</p>"},{"location":"dev/stream_client/error-handling/#types-of-errors","title":"Types of Errors","text":"<p>Errors can occur at different stages of interaction with the Hyperion Stream Client:</p> <ol> <li>Client Instantiation Errors: Rare, but could occur if invalid options are somehow passed (though TypeScript helps prevent this).</li> <li>Initial Connection Errors: When <code>client.connect()</code> is called.</li> <li>Stream Request Errors: When <code>client.streamActions()</code> or <code>client.streamDeltas()</code> is called.</li> <li>Ongoing Connection Errors: Disconnections or errors after a connection was established.</li> <li>In-Stream Errors: Errors specific to an active data stream after it has started.</li> <li>AsyncIterator Errors: Errors encountered while consuming data via the <code>for await...of</code> loop.</li> </ol>"},{"location":"dev/stream_client/error-handling/#1-client-instantiation","title":"1. Client Instantiation","text":"<p>Usually, if you provide a syntactically correct options object (especially with TypeScript), instantiation itself won't throw. The primary concern here is ensuring the <code>endpoint</code> is a valid string.</p> <pre><code>try {\n  const client = new HyperionStreamClient({\n    // endpoint: undefined, // This would likely be caught by TypeScript or cause issues later\n    endpoint: \"https://eos.hyperion.eosrio.io\"\n  });\n} catch (e) {\n  // Highly unlikely to catch here with proper options\n  console.error(\"Client instantiation failed:\", e);\n}\n</code></pre>"},{"location":"dev/stream_client/error-handling/#2-initial-connection-errors-clientconnect","title":"2. Initial Connection Errors (<code>client.connect()</code>)","text":"<p>The <code>client.connect()</code> method returns a <code>Promise</code>. If the initial connection to the Hyperion server fails (e.g., server down, incorrect endpoint, network issue, CORS issue in browsers), the promise will reject.</p> <p>Additionally, the client itself will emit an <code>'error'</code> event.</p> <p><pre><code>// Assuming 'client' is an instance of HyperionStreamClient\n\nclient.on(\"error\", (error) =&gt; {\n  // This handler catches various client-level errors, including initial connection failures\n  // if the connect() promise isn't explicitly caught, or for ongoing issues.\n  console.error(\"Client-level error:\", error);\n});\n\nasync function connectToServer() {\n  try {\n    console.log(\"Attempting to connect...\");\n    await client.connect();\n    console.log(\"Successfully connected!\");\n  } catch (error) {\n    console.error(\"Failed to connect (Promise rejected):\", error.message);\n    // Handle specific error types if needed (e.g., check error.name or error.message)\n    // Example: if (error.message.includes('timeout')) { ... }\n  }\n}\n\nconnectToServer();\n</code></pre> Common Causes:</p> <ul> <li>Invalid Hyperion endpoint URL.</li> <li>Hyperion server is offline or unreachable.</li> <li>Network connectivity problems (client-side or server-side).</li> <li>Browser Specific: CORS (Cross-Origin Resource Sharing) issues. The Hyperion server must allow requests from your web application's origin. Check the browser console for CORS errors.</li> <li>Connection timeout (see <code>connectionTimeout</code> in Client Configuration).</li> </ul>"},{"location":"dev/stream_client/error-handling/#3-stream-request-errors-streamactions-streamdeltas","title":"3. Stream Request Errors (<code>streamActions()</code> &amp; <code>streamDeltas()</code>)","text":"<p>When you call <code>client.streamActions(request)</code> or <code>client.streamDeltas(request)</code>, these methods also return Promises. The promise will reject if the Hyperion server cannot fulfill the stream request.</p> <p><pre><code>async function requestMyStream() {\n  if (!client.online) {\n    console.error(\"Cannot request stream: Client is not connected.\");\n    return;\n  }\n\n  try {\n    const stream = await client.streamActions({\n      contract: \"invalid.contract.name\", // Example of a potentially invalid parameter\n      action: \"someaction\",\n      start_from: 0\n    });\n    // ... set up stream message handlers ...\n  } catch (error) {\n    console.error(\"Failed to initiate stream request:\", error.message);\n    // 'error' might be an object from the server indicating the problem\n    // e.g., { status: \"ERROR\", error: \"Invalid contract name\" }\n    // or a client-side error if the request couldn't even be made.\n    if (error.error) { // Check if it's a structured error from Hyperion\n        console.error(\"Server error details:\", error.error);\n    }\n  }\n}\n</code></pre> Common Causes:</p> <ul> <li>Invalid request parameters (e.g., non-existent contract, malformed filters).</li> <li>Server-side issues preventing stream setup for the given parameters.</li> <li>The client is not connected when the request is made.</li> </ul>"},{"location":"dev/stream_client/error-handling/#4-ongoing-connection-errors-disconnections","title":"4. Ongoing Connection Errors &amp; Disconnections","text":"<p>The Hyperion Stream Client uses <code>socket.io-client</code> under the hood, which handles automatic reconnections by default.</p> <ul> <li> <p><code>client.on('disconnect', (reason) =&gt; { ... })</code>:     This event is fired when the client loses its connection to the server. The <code>reason</code> string often indicates why (e.g., <code>\"io server disconnect\"</code>, <code>\"transport close\"</code>).     <pre><code>client.on(\"disconnect\", (reason) =&gt; {\n  console.warn(\"Client disconnected. Reason:\", reason);\n  // Update UI to show disconnected status.\n  // The client will attempt to reconnect automatically.\n});\n</code></pre></p> </li> <li> <p><code>client.on('connect', () =&gt; { ... })</code>:     This event is fired not only on the initial successful connection but also on successful reconnections.     <pre><code>client.on(\"connect\", () =&gt; {\n  console.log(\"Client connected (or reconnected)!\");\n  // If you have streams with `replayOnReconnect: true`, they will attempt to restart.\n  // You might need to re-request streams that don't have `replayOnReconnect: true`.\n});\n</code></pre></p> </li> <li> <p><code>client.on('error', (error) =&gt; { ... })</code>:     This client-level error handler can also catch errors related to reconnection attempts or other general socket issues.</p> </li> </ul> <p>Reconnection Behavior:</p> <ul> <li>Streams configured with <code>replayOnReconnect: true</code> in their request options will automatically attempt to re-establish and resume from where they left off (approximately) upon client reconnection.</li> <li>For streams with <code>replayOnReconnect: false</code>, you would need to manually re-request the missed data after a <code>connect</code> event, otherwise it will resume from the current live data.</li> </ul>"},{"location":"dev/stream_client/error-handling/#5-in-stream-errors","title":"5. In-Stream Errors","text":"<p>Once a stream is successfully started, it can still encounter errors specific to that stream. Each stream object returned by <code>streamActions</code> or <code>streamDeltas</code> emits its own <code>'error'</code> event.</p> <p><pre><code>// ... inside a function where 'stream' is an active HyperionStream object ...\nstream.on(\"error\", (streamError) =&gt; {\n  console.error(`Error on stream ${stream.reqUUID}:`, streamError);\n  // This error might indicate the server terminated this specific stream.\n  // The stream might no longer be usable.\n  // Consider logging, alerting, or attempting to set up a new stream.\n});\n</code></pre> Common Causes:</p> <ul> <li>The server decides to terminate a specific stream for some reason (e.g., resource limits, internal server error related to the query).</li> <li>Malformed messages from the server (less common).</li> </ul>"},{"location":"dev/stream_client/error-handling/#6-asynciterator-errors","title":"6. AsyncIterator Errors","text":"<p>If you are consuming a stream using the <code>for await...of</code> loop, errors can occur during iteration. These should be caught using a <code>try...catch</code> block around the loop.</p> <p><pre><code>async function processWithIterator() {\n  let stream; // Keep stream reference outside try block for potential cleanup\n  try {\n    stream = await client.streamActions({ /* ... */ });\n    for await (const message of stream) {\n      if (message === null) {\n        console.log(\"Stream ended gracefully.\");\n        break;\n      }\n      // Process message\n      if (message.content.block_num % 1000 === 0) { // Artificial error condition\n        throw new Error(\"Simulated processing error during iteration.\");\n      }\n    }\n  } catch (error) {\n    console.error(\"Error during AsyncIterator processing:\", error.message);\n    // The 'stream' might be in an indeterminate state here.\n    // It's often best to assume the iteration is compromised.\n    // If 'stream' is defined, you might still try stream.stop() if appropriate,\n    // though the connection or stream itself might already be closed.\n  } finally {\n    console.log(\"AsyncIterator loop finished or errored out.\");\n    // Perform any cleanup\n  }\n}\n</code></pre> An error thrown from within the <code>for await...of</code> loop (either by your processing logic or an underlying issue with the stream yielding an error) will be caught by the <code>catch</code> block.</p>"},{"location":"dev/stream_client/error-handling/#best-practices","title":"Best Practices","text":"<ul> <li>Always Attach Error Handlers: Attach <code>'error'</code> handlers to the client instance and to every stream instance you create.</li> <li>Catch Promise Rejections: Use <code>.catch()</code> or <code>try...catch</code> with <code>async/await</code> for <code>client.connect()</code>, <code>client.streamActions()</code>, and <code>client.streamDeltas()</code>.</li> <li>Monitor Connection State: Use <code>client.on('connect', ...)</code> and <code>client.on('disconnect', ...)</code> to understand the current connection status and react accordingly (e.g., update UI, manage stream re-requests).</li> <li><code>replayOnReconnect: true</code>: For streams where historical data matters, set <code>replayOnReconnect: true</code> in the stream request options to allow the client to attempt to catch missed data after a disconnection.</li> <li>Logging: Implement comprehensive logging, especially for errors, to help diagnose issues in development and production.</li> <li>Retry Strategies: For critical applications, consider implementing custom retry strategies for <code>client.connect()</code> or for re-requesting streams if they fail, potentially with exponential backoff.</li> <li>User Feedback: In UI applications, provide clear feedback to the user about connection status and any errors encountered.</li> <li>Test Error Conditions: During development, try to simulate error conditions (e.g., stop your Hyperion server, disconnect network) to test how your application responds.</li> </ul> <p>By thoughtfully handling these various error scenarios, you can build more resilient and reliable applications using the Hyperion Stream Client.</p>"},{"location":"dev/stream_client/error-handling/#next-steps","title":"Next Steps","text":"<ul> <li>Client Configuration: Review client settings in Client Configuration.</li> <li>Browser Usage: Specific considerations for Browser Usage.</li> </ul>"},{"location":"dev/stream_client/getting-started/","title":"Getting Started","text":"<p>This guide will walk you through the basic steps to connect to a Hyperion server and start streaming your first set of blockchain data using the Hyperion Stream Client.</p>"},{"location":"dev/stream_client/getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have:</p> <ol> <li> <p>Installed the Client: If you haven't already, install the client via npm or yarn:     <pre><code>npm install @eosrio/hyperion-stream-client --save\n</code></pre>     or</p> <pre><code>yarn add @eosrio/hyperion-stream-client\n</code></pre> </li> <li> <p>Hyperion Endpoint: Have a Hyperion API endpoint URL (v3.6+) ready. You can find public endpoints here or use your own.</p> </li> <li> <p>Node.js Environment: This guide assumes a Node.js environment (v18+).</p> <p>Browser usage: For other ways to include and use the client in browser-based projects, see the Browser Usage Guide.</p> </li> </ol>"},{"location":"dev/stream_client/getting-started/#steps","title":"Steps","text":"<p>Let's create a simple Node.js script to stream <code>transfer</code> actions from the <code>eosio.token</code> contract.</p>"},{"location":"dev/stream_client/getting-started/#1-import-the-client","title":"1. Import the Client","text":"<p>At the top of your file, import <code>HyperionStreamClient</code>:</p> <ul> <li>For ES Modules  (Recommended)</li> </ul> <pre><code>import { HyperionStreamClient } from \"@eosrio/hyperion-stream-client\";\n</code></pre> <ul> <li>For CommonJS</li> </ul> <pre><code>const { HyperionStreamClient } = require(\"@eosrio/hyperion-stream-client\");\n</code></pre>"},{"location":"dev/stream_client/getting-started/#2-initialize-the-client","title":"2. Initialize the Client","text":"<p>Instantiate the client with your Hyperion endpoint.</p> <pre><code>const client = new HyperionStreamClient({\n  endpoint: \"https://eos.hyperion.eosrio.io\", // Replace with your chosen Hyperion v3.6+ endpoint\n  debug: false, // Set to true to enable debug logging during development\n});\n</code></pre> <p>Tip: For a full list of configuration options, see the Client Configuration guide.</p>"},{"location":"dev/stream_client/getting-started/#3-set-up-event-handlers-optional-but-recommended","title":"3. Set up event handlers (Optional but Recommended)","text":"<p>It's good practice to listen for connection events to know the client's status.</p> <pre><code>client.on(\"connect\", () =&gt; {\n  console.log(\"Successfully connected to Hyperion Stream API!\");\n});\n\nclient.on(\"error\", (error) =&gt; {\n  console.error(\"Connection Error:\", error);\n});\n</code></pre>"},{"location":"dev/stream_client/getting-started/#4-connect-to-the-server","title":"4. Connect to the Server","text":"<p>Use the <code>client.connect()</code> method, which returns a Promise.</p> <pre><code>  try {\n    await client.connect();\n  } catch (error) {\n    console.error(\"Failed to establish initial connection:\", error.message);\n  }\n</code></pre>"},{"location":"dev/stream_client/getting-started/#5-request-a-data-stream","title":"5. Request a Data Stream","text":"<p>Once connected, you can request a stream. Let's stream the 5 most recent <code>transfer</code> actions from the <code>eosio.token</code> contract and then stop.</p> <pre><code>  try {\n    const stream = await client.streamActions({\n      contract: \"eosio.token\", //required field\n      action: \"transfer\", //required field\n      account: \"\", // Track all transfers, not specific to one account\n      start_from: -5, // Start from 5 blocks before the current head block\n      read_until: 0,   // Read until the current head block \n      filters: [],     // No additional data filters for this basic example\n      // For this example, we want it to stop after fetching recent history.\n      // If you wanted live data, you'd set read_until: 0 and ignore_live: false (default)\n      ignore_live: true // Stop after historical data is fetched\n    });\n\n    // Handle incoming messages for this specific stream\n    stream.on(\"message\", (data) =&gt; {\n      console.log(\"\\nReceived Action:\");\n      console.log(`  Data:`, data.content.act.data);\n    });\n\n    // Handle errors specific to this stream\n    stream.on(\"error\", (error) =&gt; {\n      console.error(\"\\nStream Error:\", error);\n    });\n\n  } catch (error) {\n    console.error(\"Error starting action stream:\", error.message);\n  }\n</code></pre>"},{"location":"dev/stream_client/getting-started/#6-handling-stream-data","title":"6. Handling Stream data","text":"<p>Once you've initiated a stream for actions or table deltas, you need a way to process the incoming data. The client offers two primary mechanisms for this:</p> <ul> <li> <p>the Event-Driven API (<code>stream.on('message', ...)</code>)</p> <ul> <li>Listen to events on individual <code>actionStream</code> or <code>deltaStream</code> instances</li> </ul> </li> <li> <p>or the AsyncIterator Pattern (<code>for await...of</code>).</p> <ul> <li>allows for more readable, sequential processing of stream data, especially useful with <code>async/await</code></li> </ul> </li> </ul> <p>Handling Stream Data </p>"},{"location":"dev/stream_client/getting-started/#next-steps","title":"Next Steps","text":"<p>Congratulations! You've successfully streamed data using the Hyperion Stream Client.</p> <p>From here, you can explore:</p> <ul> <li>Streaming Actions: Learn more on how to monitor action traces.</li> <li>Streaming Table Deltas: Learn how to monitor changes in contract tables.</li> <li>Advanced Filtering: Dive deeper into the <code>filters</code> option for Streaming Actions and Streaming Table Deltas.</li> <li>Full Configuration Options: See all available settings.</li> <li>Error Handling: See common error scenarios and best practices. </li> </ul>"},{"location":"dev/stream_client/streaming-actions/","title":"Streaming Actions","text":"<p>The Hyperion Stream Client allows you to subscribe to a real-time or historical stream of action traces from the blockchain. This is useful for tracking specific smart contract interactions, monitoring account activity, or building applications that react to on-chain events.</p> <p>You initiate an action stream using the <code>client.streamActions(request)</code> method.</p>"},{"location":"dev/stream_client/streaming-actions/#clientstreamactionsrequest","title":"<code>client.streamActions(request)</code>","text":"<p>This asynchronous method sends a request to the Hyperion server to start streaming action traces based on the criteria defined in the <code>request</code> object. It returns a <code>Promise</code> that resolves to a <code>HyperionStream</code> instance, which you can then use to listen for messages.</p> <pre><code>import { HyperionStreamClient, StreamActionsRequest, IncomingData, ActionContent } from \"@eosrio/hyperion-stream-client\";\n\n// Assuming 'client' is an initialized and connected HyperionStreamClient instance\n\ntry {\n    const actionStream = await client.streamActions({\n        contract: \"eosio.token\",\n        action: \"transfer\",\n        account: \"\",    // Optional: filter by notified account\n        start_from: 0, // Start from HEAD block\n        read_until: 0, // Stream indefinitely\n        filters: [\n          { field: \"act.data.to\", value: \"eosio.ram\" }\n        ]\n    });\n    console.log(\"Action stream request successful. UUID:\", actionStream.reqUUID);\n\n    actionStream.on(\"message\", (message: IncomingData&lt;ActionContent&gt;) =&gt; {\n      console.log(\"Received action:\", message.content.act.data);\n    });\n\n    actionStream.on(\"error\", (error) =&gt; {\n      console.error(\"Stream error:\", error);\n    });\n\n} catch (error) {\n    console.error(\"Failed to initiate action stream:\", error);\n}\n</code></pre>"},{"location":"dev/stream_client/streaming-actions/#request-parameters-streamactionsrequest","title":"Request Parameters (<code>StreamActionsRequest</code>)","text":"<p>The <code>request</code> object passed to <code>client.streamActions()</code> can contain the following properties:</p>"},{"location":"dev/stream_client/streaming-actions/#contract","title":"<code>contract</code>","text":"<ul> <li>Type: <code>string</code></li> <li>Required: Yes</li> <li>Description: The account name of the smart contract whose actions you want to stream.</li> <li>Example: <code>\"eosio.token\"</code>, <code>\"alien.worlds\"</code></li> </ul>"},{"location":"dev/stream_client/streaming-actions/#action","title":"<code>action</code>","text":"<ul> <li>Type: <code>string</code></li> <li>Required: Yes</li> <li>Description: The name of the action you want to stream.<ul> <li>You can specify a single action name (e.g., <code>\"transfer\"</code>).</li> <li>Use an asterisk (<code>\"*\"</code>) to stream all actions executed by the specified <code>contract</code>.</li> </ul> </li> <li>Example: <code>\"transfer\"</code>, <code>\"newaccount\"</code>, <code>\"*\"</code></li> </ul>"},{"location":"dev/stream_client/streaming-actions/#account","title":"<code>account</code>","text":"<ul> <li>Type: <code>string</code></li> <li>Optional: Yes</li> <li>Default: <code>\"\"</code> (empty string, meaning no account-specific filtering beyond <code>contract</code> and <code>action</code>)</li> <li>Description: If provided, the stream will only include actions where this account is involved. Involvement can   mean:<ul> <li>The account is listed in the <code>act.authorization</code> array (i.e., authorized the action).</li> <li>The account is the recipient of a notification triggered by the action (i.e., listed in the <code>notified</code> array of   the action trace).</li> </ul> </li> <li>Example: <code>\"myuseraccnt1\"</code></li> </ul>"},{"location":"dev/stream_client/streaming-actions/#start_from","title":"<code>start_from</code>","text":"<ul> <li>Type: <code>number | string</code></li> <li>Optional: Yes</li> <li>Default: <code>0</code></li> <li>Description: Defines the starting point of the stream.<ul> <li><code>0</code>: Start from the current head block of the blockchain (live streaming).</li> <li>Positive Number: Start from this specific absolute block number (e.g., <code>150000000</code>).</li> <li>Negative Number: Start from a block relative to the current head block. For example, <code>-100</code> starts streaming   100 blocks before the current head.</li> <li>ISO 8601 Timestamp String: Start from the block closest to the specified date and time (e.g.,   <code>\"2023-01-01T00:00:00.000Z\"</code>).</li> </ul> </li> <li>See Also: Block Range Parameters for a more detailed explanation.</li> </ul>"},{"location":"dev/stream_client/streaming-actions/#read_until","title":"<code>read_until</code>","text":"<ul> <li>Type: <code>number | string</code></li> <li>Optional: Yes</li> <li>Default: <code>0</code></li> <li>Description: Defines the ending point of the stream.<ul> <li><code>0</code>: Stream indefinitely (or until <code>ignore_live: true</code> and historical data is complete).</li> <li>Positive Number: Stop at this specific absolute block number.</li> <li>Negative Number: Stop at a block relative to the current head block.</li> <li>ISO 8601 Timestamp String: Stop at the block closest to this specified date and time.</li> </ul> </li> <li>See Also: Block Range Parameters</li> </ul>"},{"location":"dev/stream_client/streaming-actions/#filters","title":"<code>filters</code>","text":"<ul> <li>Type: <code>RequestFilter[]</code> (Array of <code>RequestFilter</code> objects)</li> <li>Optional: Yes</li> <li>Default: <code>[]</code> (empty array, no additional data filters)</li> <li>Description: An array of filter objects to perform server-side filtering based on the content of the action data.   Each <code>RequestFilter</code> object has:<ul> <li><code>field</code> (string): A dot-notation path to the field within the action's data structure (e.g., <code>\"act.data.to\"</code>,   <code>\"@transfer.from\"</code>, <code>\"act.authorization.actor\"</code>). Hyperion often provides special <code>@</code> prefixed fields for commonly   accessed, decoded data (like <code>@transfer.from</code>, <code>@transfer.to</code>, <code>@transfer.quantity</code>).</li> <li><code>value</code> (string | number | boolean): The value to compare against.</li> <li><code>operator</code> (string, optional): The comparison operator. Defaults to <code>\"eq\"</code> (equals). Supported operators include:<ul> <li><code>\"eq\"</code>: equals</li> <li><code>\"ne\"</code>: not equals</li> <li><code>\"gt\"</code>: greater than</li> <li><code>\"lt\"</code>: less than</li> <li><code>\"gte\"</code>: greater than or equal to</li> <li><code>\"lte\"</code>: less than or equal to</li> <li><code>\"contains\"</code>: (for string fields) field contains the value</li> <li><code>\"starts_with\"</code>: (for string fields) field starts with the value</li> <li><code>\"ends_with\"</code>: (for string fields) field ends with the value</li> </ul> </li> </ul> </li> <li>Example:   <pre><code>filters: [\n  { field: \"act.data.to\", value: \"teamgreymass\" }, // transfer.to === 'teamgreymass'\n  { field: \"act.data.amount\", value: 10000, operator: \"gte\" } // data.amount &gt;= 10000 (assuming amount is numeric)\n  { field: \"@transfer.memo\", value: \"payment\", operator: \"contains\" } // memo contains \"payment\"\n]\n</code></pre></li> <li>Note: Refer to   Hyperion's index template definitions   to understand available indexed fields for actions.</li> </ul>"},{"location":"dev/stream_client/streaming-actions/#filter_op","title":"<code>filter_op</code>","text":"<ul> <li>Type: <code>'and' | 'or'</code></li> <li>Optional: Yes</li> <li>Default: <code>\"and\"</code></li> <li>Description: Specifies the logical operator to use when multiple <code>filters</code> are provided.<ul> <li><code>\"and\"</code>: All filter conditions must be met.</li> <li><code>\"or\"</code>: At least one filter condition must be met.</li> </ul> </li> <li>Example:   <pre><code>// Actions where act.data.to IS 'userA' OR act.data.from IS 'userA'\nfilters: [\n  { field: \"act.data.to\", value: \"userA\" },\n  { field: \"act.data.from\", value: \"userA\" }\n],\nfilter_op: \"or\"\n</code></pre></li> </ul>"},{"location":"dev/stream_client/streaming-actions/#ignore_live","title":"<code>ignore_live</code>","text":"<ul> <li>Type: <code>boolean</code></li> <li>Optional: Yes</li> <li>Default: <code>false</code></li> <li>Description:<ul> <li>If <code>true</code>, the stream will stop after all historical data (up to <code>read_until</code> or the current head block if   <code>read_until</code> is 0) has been sent. It will not send live blocks.</li> <li>If <code>false</code>, the stream will transition to sending live blocks after historical data is complete.</li> </ul> </li> <li>Example: To get only the last 100 blocks of <code>eosio.token::transfer</code> actions and then stop:   <pre><code>{\n  contract: \"eosio.token\",\n  action: \"transfer\",\n  start_from: -100,\n  read_until: 0, // Read up to current head\n  ignore_live: true\n}\n</code></pre></li> </ul>"},{"location":"dev/stream_client/streaming-actions/#replayonreconnect","title":"<code>replayOnReconnect</code>","text":"<ul> <li>Type: <code>boolean</code></li> <li>Optional: Yes</li> <li>Default: <code>false</code></li> <li>Description:<ul> <li>If <code>true</code>, and the client disconnects and then reconnects, it will attempt to resend this stream request,   automatically adjusting <code>start_from</code> to the block number after the last successfully received block for this   stream. This helps prevent data loss during transient network issues.</li> <li>If <code>false</code>, the stream request will not automatically replay data from the offline time on reconnect. You would need to manually   set up a new stream. Current live data will keep streaming after reconnection, but you will miss the data from the offline period.</li> </ul> </li> <li>Recommendation: Set to <code>true</code> if you need the full missed sequence of data even after reconnections.</li> </ul>"},{"location":"dev/stream_client/streaming-actions/#handling-the-action-stream","title":"Handling the Action Stream","text":"<p>Once <code>await client.streamActions(request)</code> resolves, it returns a <code>HyperionStream</code> object. You primarily interact with this object by listening to its events:</p> <ul> <li><code>stream.on(\"message\", (data: IncomingData&lt;ActionContent&gt;) =&gt; { ... })</code>:   This is the event fired for each action trace received from the stream. The <code>data</code> object contains:<ul> <li><code>data.uuid</code>: The unique identifier for this stream request.</li> <li><code>data.type</code>: Will be <code>\"action\"</code>.</li> <li><code>data.mode</code>: <code>\"live\"</code> or <code>\"history\"</code>.</li> <li><code>data.content</code>: An <code>ActionContent</code> object containing the detailed action trace (e.g., <code>act.data</code>, <code>block_num</code>,   <code>trx_id</code>).</li> <li><code>data.irreversible</code>: <code>true</code> if this message is confirmed irreversible (relevant if <code>client.options.libStream</code> is   true).   See Handling Stream Data for more on <code>ActionContent</code>.</li> </ul> </li> </ul> <p>For cleaner sequential processing of the data check the AsyncIterator Pattern</p> <ul> <li> <p><code>stream.on(\"error\", (error: any) =&gt; { ... })</code>:   Fired if an error occurs that is specific to this stream (e.g., the server terminates the stream due to an issue).</p> </li> <li> <p><code>stream.on(\"start\", (response: { status: string, reqUUID: string, startingBlock: number | string }) =&gt; { ... })</code>:   Fired when the Hyperion server acknowledges and successfully starts the stream request. <code>response.reqUUID</code> will match   <code>stream.reqUUID</code>. <code>startingBlock</code> reflects the effective <code>start_from</code> resolved by the server.</p> </li> </ul>"},{"location":"dev/stream_client/streaming-actions/#stopping-an-action-stream","title":"Stopping an Action Stream","text":"<p>To stop receiving data for a specific action stream and notify the server to close it:</p> <pre><code>// Assuming 'actionStream' is the object returned by client.streamActions()\nactionStream.stop();\nconsole.log(\"Requested to stop action stream:\", actionStream.reqUUID);\n</code></pre>"},{"location":"dev/stream_client/streaming-actions/#examples","title":"Examples","text":""},{"location":"dev/stream_client/streaming-actions/#1-stream-live-transfers-to-a-specific-account","title":"1. Stream Live Transfers to a Specific Account","text":"<pre><code>const stream = await client.streamActions({\n  contract: \"eosio.token\",\n  action: \"transfer\",\n  start_from: 0, // Live\n  filters: [\n    { field: \"act.data.to\", value: USER_ACCOUNT_NAME }\n  ],\n  replayOnReconnect: true\n});\n\nstream.on(\"message\", (msg) =&gt; {\n  console.log(`Live transfer to ${USER_ACCOUNT_NAME}:`, msg.content.act.data);\n});\n</code></pre>"},{"location":"dev/stream_client/streaming-actions/#2-get-all-actions-by-a-contract-from-a-specific-block-range","title":"2. Get All Actions by a Contract from a Specific Block Range","text":"<pre><code>const stream = await client.streamActions({\n  contract: CONTRACT_NAME,\n  action: \"*\", // All actions\n  start_from: START_BLOCK_NUM,\n  read_until: END_BLOCK_NUM,\n  ignore_live: true   // Stop after this block\n});\n\n// Using AsyncIterator for this example\nfor await (const message of stream) {\n  if (message === null) break; // Stream ended\n  console.log(`  ${message.content.act.name}:`, message.content.act.data);\n}\n</code></pre>"},{"location":"dev/stream_client/streaming-actions/#next-steps","title":"Next Steps","text":"<ul> <li>Handling Stream Data: Dive deeper into the structure of <code>ActionContent</code> and explore the AsyncIterator pattern.</li> <li>Streaming Table Deltas: Learn about monitoring contract table changes.</li> <li>Client Configuration: Review all Client Configuration options.   </li> </ul>"},{"location":"dev/stream_client/streaming-deltas/","title":"Streaming Table Deltas","text":"<p>The Hyperion Stream Client enables you to subscribe to a stream of table delta traces, providing real-time or historical insight into how data within smart contract tables changes over time. This is invaluable for tracking state changes, such as account balances, NFT ownership, game states, or any data stored in on-chain tables.</p> <p>You initiate a table delta stream using the <code>client.streamDeltas(request)</code> method.</p>"},{"location":"dev/stream_client/streaming-deltas/#clientstreamdeltasrequest","title":"<code>client.streamDeltas(request)</code>","text":"<p>This asynchronous method sends a request to the Hyperion server to start streaming table delta traces based on the criteria defined in the <code>request</code> object. It returns a <code>Promise</code> that resolves to a <code>HyperionStream</code> instance, which you can then use to listen for messages.</p> <pre><code>import { HyperionStreamClient, IncomingData, DeltaContent } from \"@eosrio/hyperion-stream-client\";\n\n// Assuming 'client' is an initialized and connected HyperionStreamClient instance\n\ntry {\n    const deltaStream = await client.streamDeltas({\n        code: \"eosio.token\",\n        table: \"accounts\",\n        scope: \"USER_ACCOUNT_NAME\",  // Optional: filter by table scope\n        start_from: 0,               // Start from HEAD block\n        read_until: 0,               // Stream indefinitely\n    });\n    console.log(\"Delta stream request successful. UUID:\", deltaStream.reqUUID);\n\n    deltaStream.on(\"message\", (message: IncomingData&lt;DeltaContent&gt;) =&gt; {\n      console.log(\"Received delta:\", message.content.data);\n      console.log(`  Present (row exists after delta): ${message.content.present === 1}`);\n    });\n\n    deltaStream.on(\"error\", (error) =&gt; {\n      console.error(\"Stream error:\", error);\n    });\n\n} catch (error) {\n    console.error(\"Failed to initiate delta stream:\", error);\n}\n</code></pre>"},{"location":"dev/stream_client/streaming-deltas/#request-parameters-streamdeltasrequest","title":"Request Parameters (<code>StreamDeltasRequest</code>)","text":"<p>The <code>request</code> object passed to <code>client.streamDeltas()</code> can contain the following properties:</p>"},{"location":"dev/stream_client/streaming-deltas/#code","title":"<code>code</code>","text":"<ul> <li>Type: <code>string</code></li> <li>Required: Yes</li> <li>Description: The account name of the smart contract that owns the table(s) you want to monitor.</li> <li>Example: <code>\"eosio.token\"</code>, <code>\"atomicassets\"</code></li> </ul>"},{"location":"dev/stream_client/streaming-deltas/#table","title":"<code>table</code>","text":"<ul> <li>Type: <code>string</code></li> <li>Required: Yes</li> <li>Description: The name of the table within the <code>code</code> contract for which you want to stream deltas.<ul> <li>You can specify a single table name (e.g., <code>\"accounts\"</code>).</li> <li>Use an asterisk (<code>\"*\"</code>) to stream deltas from all tables within the specified <code>code</code> contract.</li> </ul> </li> <li>Example: <code>\"accounts\"</code>, <code>\"assets\"</code>, <code>\"*\"</code></li> </ul>"},{"location":"dev/stream_client/streaming-deltas/#scope","title":"<code>scope</code>","text":"<ul> <li>Type: <code>string</code></li> <li>Optional: Yes</li> <li>Default: <code>\"\"</code> (empty string, meaning deltas from any scope within the table will be included)</li> <li>Description: The specific scope within the table to filter by. Many contracts use account names as scopes.</li> <li>Example: <code>\"myuseraccnt1\"</code>, <code>\"eosio\"</code></li> </ul>"},{"location":"dev/stream_client/streaming-deltas/#payer","title":"<code>payer</code>","text":"<ul> <li>Type: <code>string</code></li> <li>Optional: Yes</li> <li>Default: <code>\"\"</code> (empty string, meaning deltas paid for by any account will be included)</li> <li>Description: Filter deltas by the account name that paid for the RAM for the table row.</li> <li>Example: <code>\"rampayeracct\"</code></li> </ul>"},{"location":"dev/stream_client/streaming-deltas/#start_from","title":"<code>start_from</code>","text":"<ul> <li>Type: <code>number | string</code></li> <li>Optional: Yes</li> <li>Default: <code>0</code></li> <li>Description: Defines the starting point of the stream.<ul> <li><code>0</code>: Start from the current head block of the blockchain (live streaming).</li> <li>Positive Number: Start from this specific absolute block number (e.g., <code>150000000</code>).</li> <li>Negative Number: Start from a block relative to the current head block. For example, <code>-100</code> starts streaming   100 blocks before the current head.</li> <li>ISO 8601 Timestamp String: Start from the block closest to the specified date and time (e.g.,   <code>\"2023-01-01T00:00:00.000Z\"</code>).</li> </ul> </li> <li>See Also: Block Range Parameters for a more detailed explanation.</li> </ul>"},{"location":"dev/stream_client/streaming-deltas/#read_until","title":"<code>read_until</code>","text":"<ul> <li>Type: <code>number | string</code></li> <li>Optional: Yes</li> <li>Default: <code>0</code></li> <li>Description: Defines the ending point of the stream.<ul> <li><code>0</code>: Stream indefinitely (or until <code>ignore_live: true</code> and historical data is complete).</li> <li>Positive Number: Stop at this specific absolute block number.</li> <li>Negative Number: Stop at a block relative to the current head block.</li> <li>ISO 8601 Timestamp String: Stop at the block closest to this specified date and time.</li> </ul> </li> <li>See Also: Block Range Parameters</li> </ul>"},{"location":"dev/stream_client/streaming-deltas/#filters","title":"<code>filters</code>","text":"<ul> <li>Type: <code>RequestFilter[]</code> (Array of <code>RequestFilter</code> objects)</li> <li>Optional: Yes</li> <li>Default: <code>[]</code> (empty array, no additional data filters)</li> <li>Description: An array of filter objects to perform server-side filtering based on the content of the table row   data. Each <code>RequestFilter</code> object has:<ul> <li><code>field</code> (string): A dot-notation path to the field within the delta's <code>data</code> object (e.g., <code>\"data.balance\"</code>,   <code>\"payer\"</code>, <code>\"primary_key\"</code>). Hyperion might also provide special <code>@</code> prefixed fields for decoded data from   specific common tables (e.g., <code>@accounts.balance</code>).</li> <li><code>value</code> (string | number | boolean): The value to compare against.</li> <li><code>operator</code> (string, optional): The comparison operator. Defaults to <code>\"eq\"</code> (equals). Supported operators include:<ul> <li><code>\"eq\"</code>: equals</li> <li><code>\"ne\"</code>: not equals</li> <li><code>\"gt\"</code>: greater than</li> <li><code>\"lt\"</code>: less than</li> <li><code>\"gte\"</code>: greater than or equal to</li> <li><code>\"lte\"</code>: less than or equal to</li> <li><code>\"contains\"</code>: (for string fields) field contains the value</li> <li><code>\"starts_with\"</code>: (for string fields) field starts with the value</li> <li><code>\"ends_with\"</code>: (for string fields) field ends with the value</li> </ul> </li> </ul> </li> <li>Example:   <pre><code>filters: [\n  { field: \"data.owner\", value: \"myuseraccnt1\" }, // Row's owner field === 'myuseraccnt1'\n  { field: \"data.value\", value: 100, operator: \"gt\" } // Row's value field &gt; 100\n  { field: \"payer\", value: \"otheraccount\" } // RAM payer for the row is 'otheraccount'\n]\n</code></pre></li> <li>Note: Refer to   Hyperion's index template definitions   to understand available indexed fields for deltas and the structure of <code>data</code>.</li> </ul>"},{"location":"dev/stream_client/streaming-deltas/#filter_op","title":"<code>filter_op</code>","text":"<ul> <li>Type: <code>'and' | 'or'</code></li> <li>Optional: Yes</li> <li>Default: <code>\"and\"</code></li> <li>Description: Specifies the logical operator to use when multiple <code>filters</code> are provided.<ul> <li><code>\"and\"</code>: All filter conditions must be met.</li> <li><code>\"or\"</code>: At least one filter condition must be met.</li> </ul> </li> <li>Example:   <pre><code>// Deltas where data.fieldA IS 'X' OR data.fieldB IS 'Y'\nfilters: [\n  { field: \"data.fieldA\", value: \"X\" },\n  { field: \"data.fieldB\", value: \"Y\" }\n],\nfilter_op: \"or\"\n</code></pre></li> </ul>"},{"location":"dev/stream_client/streaming-deltas/#ignore_live","title":"<code>ignore_live</code>","text":"<ul> <li>Type: <code>boolean</code></li> <li>Optional: Yes</li> <li>Default: <code>false</code></li> <li>Description:<ul> <li>If <code>true</code>, the stream will stop after all historical data (up to <code>read_until</code> or the current head block if   <code>read_until</code> is 0) has been sent. It will not send live blocks.</li> <li>If <code>false</code>, the stream will transition to sending live blocks after historical data is complete.</li> </ul> </li> </ul>"},{"location":"dev/stream_client/streaming-deltas/#replayonreconnect","title":"<code>replayOnReconnect</code>","text":"<ul> <li>Type: <code>boolean</code></li> <li>Optional: Yes</li> <li>Default: <code>false</code></li> <li>Description:<ul> <li>If <code>true</code>, and the client disconnects and then reconnects, it will attempt to resend this stream request,   automatically adjusting <code>start_from</code> to the block number after the last successfully received block for this   stream.</li> <li>If <code>false</code>, the stream request will not automatically replay data from the offline time on reconnect. Current live data will keep streaming after reconnection, but you will miss the data from the offline period.</li> </ul> </li> <li>Recommendation: Set to <code>true</code> if you need the full missed sequence of data even after reconnections.</li> </ul>"},{"location":"dev/stream_client/streaming-deltas/#handling-the-delta-stream","title":"Handling the Delta Stream","text":"<p>The <code>await client.streamDeltas(request)</code> method returns a <code>HyperionStream</code> object. You interact with it similarly to action streams:</p> <ul> <li> <p><code>stream.on(\"message\", (data: IncomingData&lt;DeltaContent&gt;) =&gt; { ... })</code>:   This event is fired for each table delta received. The <code>data</code> object contains:</p> <ul> <li><code>data.uuid</code>: The unique identifier for this stream request.</li> <li><code>data.type</code>: Will be <code>\"delta\"</code>.</li> <li><code>data.mode</code>: <code>\"live\"</code> or <code>\"history\"</code>.</li> <li><code>data.content</code>: A <code>DeltaContent</code> object containing details of the table row change (e.g., <code>code</code>, <code>table</code>,   <code>scope</code>, <code>data</code>, <code>present</code>).<ul> <li><code>data.content.present</code>: <code>1</code> if the row exists (or was created/updated) after this delta, <code>0</code> if the row was   deleted.</li> </ul> </li> <li><code>data.irreversible</code>: <code>true</code> if this message is confirmed irreversible (relevant if <code>client.options.libStream</code> is   true).   See Handling Stream Data for more on <code>DeltaContent</code>.</li> </ul> </li> <li> <p><code>stream.on(\"error\", (error: any) =&gt; { ... })</code>:   Fired if an error occurs specific to this stream.</p> </li> <li> <p><code>stream.on(\"start\", (response: { status: string, reqUUID: string, startingBlock: number | string }) =&gt; { ... })</code>:   Fired when the Hyperion server acknowledges and successfully starts the stream request.</p> </li> </ul>"},{"location":"dev/stream_client/streaming-deltas/#stopping-a-delta-stream","title":"Stopping a Delta Stream","text":"<p>To stop receiving data for a specific delta stream:</p> <pre><code>// Assuming 'deltaStream' is the object returned by client.streamDeltas()\ndeltaStream.stop();\nconsole.log(\"Requested to stop delta stream:\", deltaStream.reqUUID);\n</code></pre>"},{"location":"dev/stream_client/streaming-deltas/#examples","title":"Examples","text":""},{"location":"dev/stream_client/streaming-deltas/#1-monitor-live-changes-to-a-specific-accounts-balances","title":"1. Monitor Live Changes to a Specific Account's Balances","text":"<pre><code>const stream = await client.streamDeltas({\n  code: \"eosio.token\",\n  table: \"accounts\",\n  scope: USER_NAME, // Scope is often the user's account name for balances\n  start_from: 0,   // Live\n  replayOnReconnect: true\n});\n\nstream.on(\"message\", (msg) =&gt; {\n  console.log(`Live balance update for ${USER_NAME}:`, msg.content.data.balance);\n});\n</code></pre>"},{"location":"dev/stream_client/streaming-deltas/#2-get-historical-deltas-for-a-specific-table-with-data-filtering","title":"2. Get Historical Deltas for a Specific Table with Data Filtering","text":"<pre><code>const stream = await client.streamDeltas({\n  code: \"atomicassets\", // Example contract\n  table: \"assets\",       // Example table\n  start_from: \"2023-01-01T00:00:00.000Z\",\n  read_until: \"2023-01-02T00:00:00.000Z\",\n  ignore_live: true,\n  filters: [\n    { field: \"data.owner\", value: OWNER_NAME }\n  ]\n});\n\nconsole.log(`Fetching historical asset deltas for owner ${OWNER_NAME}:`);\nfor await (const message of stream) {\n  if (message === null) break;\n  console.log(`  Asset ID ${message.content.primary_key}:`, message.content.data);\n}\n</code></pre>"},{"location":"dev/stream_client/streaming-deltas/#next-steps","title":"Next Steps","text":"<ul> <li>Handling Stream Data: Learn more about the <code>DeltaContent</code> structure and using the AsyncIterator pattern.</li> <li>Streaming Actions: If you need to track contract actions rather than table state.</li> <li>Client Configuration: Review all Client Configuration options. </li> </ul>"},{"location":"providers/get-started/","title":"Providers - Getting Started","text":"<p>Hyperion runs exclusively in Linux, we recommend Ubuntu 24.04. The infrastructure requires <code>systemd</code> to be enabled, if you are in WSL2 please follow this guide before the installation.</p> <p>To install Hyperion as a provider you can choose one of the following routes:</p> <p>Considering a development environment, we recommend performing the installation process with the Automated Installation Script.</p> <p>Automated Installation Script</p> <p>For production/advanced installations, use the Manual Installation steps</p> <p>Manual Installation</p> <p>Testing/Development</p> <p>For docker based deployments, use the Docker Guide</p> <p>Docker Installation</p>"},{"location":"providers/update/","title":"Updating Hyperion","text":"<p>Keeping your Hyperion instance up-to-date is important for receiving bug fixes, performance improvements, and new features. This guide outlines the recommended steps for updating your Hyperion installation managed via Git and PM2.</p>"},{"location":"providers/update/#1-before-you-update","title":"1. Before You Update","text":"<p>Check Release Notes First!</p> <p>Always check the Release Notes for the version you are updating to before starting the update process.</p> <p>Release notes may contain:</p> <ul> <li>Information about breaking changes.</li> <li>Required configuration file updates.</li> <li>Instructions for potential index migrations or specific repair steps.</li> <li>Updates to dependency requirements (Node.js, Elasticsearch, etc.).</li> </ul> <p>Failure to consult the release notes could lead to unexpected issues or downtime.</p> <p>Backup Configuration</p> <p>It's highly recommended to back up your configuration files before updating, especially if the release notes indicate configuration changes. <pre><code># Example backup command (run from Hyperion root directory)\nmkdir -p config_backups/$(date +%Y%m%d_%H%M%S)\ncp config/connections.json config_backups/$(date +%Y%m%d_%H%M%S)/\ncp -r config/chains config_backups/$(date +%Y%m%d_%H%M%S)/\necho \"Configuration backed up to config_backups/$(date +%Y%m%d_%H%M%S)/\"\n</code></pre></p> <p>Stop Services (Recommended)</p> <p>While not strictly mandatory for all updates, stopping the Hyperion services (especially the indexer) before updating dependencies and rebuilding is the safest approach to prevent potential inconsistencies. <pre><code># Example stopping services for 'wax' chain\n./stop.sh wax-indexer\n./stop.sh wax-api\n# Repeat for all chains you are running\n</code></pre> The <code>./stop.sh</code> script sends a graceful stop signal, allowing queues to drain. This might take some time for the indexer.</p>"},{"location":"providers/update/#2-update-process","title":"2. Update Process","text":""},{"location":"providers/update/#step-21-navigate-to-hyperion-directory","title":"Step 2.1: Navigate to Hyperion Directory","text":"<p>Ensure you are in the correct directory: <pre><code>cd ~/hyperion-history-api # Or your specific installation path\n</code></pre></p>"},{"location":"providers/update/#step-22-fetch-latest-code","title":"Step 2.2: Fetch Latest Code","text":"<p>Fetch the latest changes from the remote repository: <pre><code>git fetch origin\n</code></pre></p>"},{"location":"providers/update/#step-23-checkout-target-version","title":"Step 2.3: Checkout Target Version","text":"<p>Decide which version you want to update to:</p> <ul> <li>Latest Stable Release (Recommended): Find the latest version tag (e.g., <code>v3.5.0</code>) on the Releases page and check it out directly.     <pre><code>git checkout v3.5.0 # Replace v3.5.0 with the desired tag\n</code></pre></li> <li>Latest Development Version (Use with caution): Check out the <code>main</code> branch for the newest, potentially unstable code.     <pre><code>git checkout main\ngit pull origin main # Ensure you have the absolute latest from the branch\n</code></pre></li> </ul>"},{"location":"providers/update/#step-24-install-dependencies-and-build","title":"Step 2.4: Install Dependencies and Build","text":"<p>Use <code>npm ci</code> (Clean Install) to install the exact dependencies listed in <code>package-lock.json</code> and ensure a clean <code>node_modules</code> directory. This is generally safer than <code>npm install</code> for updates. The command will also trigger the build process automatically (<code>npm run build</code>).</p> <p><pre><code>npm ci\n</code></pre> This command will:</p> <ol> <li>Delete the existing <code>node_modules</code> folder.</li> <li>Install dependencies precisely as specified in <code>package-lock.json</code>.</li> <li>Run the <code>tsc</code> build process, compiling TypeScript code into the <code>build/</code> directory.</li> <li>Run permission fixes (<code>chmod +x</code> on scripts).</li> </ol> <p>Monitor the output for any installation or build errors.</p>"},{"location":"providers/update/#3-after-the-update","title":"3. After the Update","text":""},{"location":"providers/update/#step-31-restart-services","title":"Step 3.1: Restart Services","text":"<p>Restart the Hyperion Indexer and API processes using PM2.</p> <ul> <li>If you stopped the services earlier: Use the <code>./run.sh</code> script.     <pre><code># Example for 'wax' chain\n./run.sh wax-indexer\n./run.sh wax-api\n# Repeat for all chains\n</code></pre></li> <li>If you did not stop the services: You can use <code>pm2 restart</code>. This is generally okay for minor updates but restarting fully ensures all changes are loaded.     <pre><code># Example for 'wax' chain\npm2 restart wax-indexer\npm2 restart wax-api\n# Repeat for all chains\n</code></pre></li> </ul>"},{"location":"providers/update/#step-32-verify-the-update","title":"Step 3.2: Verify the Update","text":"<ol> <li>Check PM2 Status: Ensure all processes are <code>online</code>.     <pre><code>pm2 list\n</code></pre></li> <li>Check Logs: Monitor logs for any startup errors or unexpected behavior.     <pre><code>pm2 logs &lt;app-name&gt;\n# Example: pm2 logs wax-api\n</code></pre></li> <li> <p>Check Hyperion Version: Query the root endpoint of your API.     <pre><code>curl http://localhost:7000/ # Use your API host/port\n</code></pre>     Verify the <code>version</code> and <code>version_hash</code> match the updated code.</p> </li> <li> <p>Check API Health: Query the health endpoint.     <pre><code>curl -Ss http://localhost:7000/v2/health | jq\n</code></pre>     Ensure all components report an OK status.</p> </li> </ol>"},{"location":"providers/update/#4-updating-plugins-if-applicable","title":"4. Updating Plugins (If Applicable)","text":"<p>If you are using Hyperion Plugins managed by <code>hpm</code>, they might also need updating or rebuilding after a core Hyperion update, especially if there were significant changes to Hyperion's internal interfaces.</p> <ul> <li>Refer to the specific plugin's documentation for update instructions.</li> <li>You might need to run <code>hpm update &lt;plugin-alias&gt;</code> or potentially <code>hpm build-all</code> after updating Hyperion core.</li> </ul>"},{"location":"providers/update/#5-troubleshooting","title":"5. Troubleshooting","text":"<ul> <li>Build Errors: Check the output of <code>npm ci</code>. Ensure you have the correct Node.js version (&gt;= v22) and necessary build tools installed. Try removing <code>node_modules</code> (<code>rm -rf node_modules</code>) and running <code>npm ci</code> again.</li> <li>Service Start Failures: Check <code>pm2 logs &lt;app-name&gt;</code> immediately after attempting to start. Errors often point to configuration issues or problems connecting to dependencies (Elasticsearch, RabbitMQ, etc.). Verify dependency services are running and reachable.</li> <li>Unexpected Behavior: Consult the release notes for the version you updated to. If the issue persists, consider opening an issue on the Hyperion GitHub repository or talking directly on the Hyperion Telegram Group.</li> </ul>"},{"location":"providers/cli-tools/hyp-config/","title":"Hyperion Configuration CLI (<code>hyp-config</code>)","text":"<p>The <code>hyp-config</code> command-line interface (CLI) tool is your primary utility for initializing and managing Hyperion's configuration files. It helps you set up connections to infrastructure services, add and manage configurations for different blockchain chains, and define how Hyperion should index specific contract states.</p>"},{"location":"providers/cli-tools/hyp-config/#general-usage","title":"General Usage","text":"<p>Commands typically follow the pattern:</p> <p><code>./hyp-config &lt;command-group&gt; &lt;sub-command&gt; [arguments...] [options...]</code></p>"},{"location":"providers/cli-tools/hyp-config/#connections-commands","title":"<code>connections</code> Commands","text":"<p>This group of commands manages the main <code>config/connections.json</code> file. This file stores the connection details for essential infrastructure services that Hyperion relies on, such as RabbitMQ, Elasticsearch, Redis, and MongoDB.</p>"},{"location":"providers/cli-tools/hyp-config/#connections-init","title":"<code>connections init</code>","text":"<p>Initializes or re-initializes the <code>config/connections.json</code> file. It interactively prompts you for connection details for RabbitMQ, Elasticsearch, Redis, and MongoDB. After gathering the information, it tests connectivity to each service.</p> <ul> <li>Purpose: To create a valid <code>connections.json</code> file, ensuring Hyperion can communicate with its dependencies. This is often the first command you'll run when setting up a new Hyperion instance.</li> </ul> <p>Usage: <pre><code>./hyp-config connections init\n</code></pre> (Follow the on-screen prompts)</p> <p>or </p> <pre><code>./hyp-config connections init [options]\n</code></pre>"},{"location":"providers/cli-tools/hyp-config/#options-for-non-interactive-setup","title":"Options (for non-interactive setup):","text":"<ul> <li><code>--amqp-user &lt;user&gt;</code>: RabbitMQ username</li> <li><code>--amqp-pass &lt;password&gt;</code>: RabbitMQ password</li> <li><code>--amqp-vhost &lt;vhost&gt;</code>: RabbitMQ vhost</li> <li><code>--es-user &lt;user&gt;</code>: Elasticsearch username</li> <li><code>--es-pass &lt;password&gt;</code>: Elasticsearch password</li> <li><code>--es-protocol &lt;protocol&gt;</code>: Elasticsearch protocol (http/https)</li> <li><code>--redis-host &lt;host&gt;</code>: Redis server host</li> <li><code>--redis-port &lt;port&gt;</code>: Redis server port</li> <li><code>--mongo-enabled &lt;boolean&gt;</code>: Enable MongoDB integration (true/false)</li> <li><code>--mongo-host &lt;host&gt;</code>: MongoDB server host</li> <li><code>--mongo-port &lt;port&gt;</code>: MongoDB server port</li> <li><code>--mongo-user &lt;user&gt;</code>: MongoDB username</li> <li><code>--mongo-pass &lt;password&gt;</code>: MongoDB password</li> <li><code>--mongo-prefix &lt;prefix&gt;</code>: MongoDB database prefix</li> </ul> <p> Example (Non-Interactive with some options):</p> <p><pre><code>./hyp-config connections init --es-user myelasticuser --es-pass mysecret --mongo-enabled true\n</code></pre> (The tool will use provided options and prompt for any missing required details).</p>"},{"location":"providers/cli-tools/hyp-config/#connections-test","title":"<code>connections test</code>","text":"<p>Tests the connectivity to the infrastructure services defined in <code>connections.json</code> (Redis, Elasticsearch, RabbitMQ, MongoDB).</p> <ul> <li>Purpose: To verify that Hyperion can successfully connect to all its dependencies after <code>connections.json</code> has been configured.</li> <li>Behavior: Outputs the status of each connection test.</li> </ul> <p>Usage: <pre><code>./hyp-config connections test\n</code></pre></p>"},{"location":"providers/cli-tools/hyp-config/#connections-reset","title":"<code>connections reset</code>","text":"<p>Removes the existing <code>connections.json</code> file. Before deletion, a backup of the current file is created as <code>connections.json.bak</code> in the <code>config/configuration_backups/</code> directory. You will be prompted for confirmation.</p> <ul> <li>Purpose: To clear the current connection settings and start fresh, for example, if you need to reconfigure everything.</li> <li>Behavior: Prompts for confirmation. If confirmed, deletes the file after backing it up.</li> </ul> <p>Usage: <pre><code>./hyp-config connections reset\n</code></pre></p> <p>Warning</p> <p>If you remove an existing <code>connections.json</code> file but you already have chain files configured, those chains will now have the <code>pending</code> status. To solve this, after creating a new <code>connections.json</code> file, you can run the <code>chains new</code> command with <code>--use-pending</code> to link this chain again to the new <code>connections.json</code> file and change the chain status to configured again. Please refer to the chains new Section below for a more in-depth explanation <pre><code>`./hyp-config chains new &lt;shortName&gt; --http &lt;http_endpoint&gt; --ship &lt;ship_endpoint&gt; --use-pending` \n</code></pre></p>"},{"location":"providers/cli-tools/hyp-config/#chains-commands","title":"<code>chains</code> Commands","text":"<p>Manage individual chain configuration files typically located in <code>config/chains/</code> (e.g., <code>config/chains/wax.config.json</code>). Each file details how Hyperion should index and serve data for a specific blockchain.</p>"},{"location":"providers/cli-tools/hyp-config/#chains-new-shortname-http-http_endpoint-ship-ship_endpoint","title":"<code>chains new &lt;shortName&gt; --http &lt;http_endpoint&gt; --ship &lt;ship_endpoint&gt;</code>","text":"<p>Creates a new chain configuration file named <code>&lt;shortName&gt;.config.json</code> within the <code>config/chains/</code> directory. The new file is based on Hyperion's example template (<code>references/config.ref.json</code>).</p> <p>This command also needs the required connection details for this new chain (like SHIP and HTTP endpoints) and it generates the WS/control ports to <code>config/connections.json</code>. It also attempts to determine the parser version and chain ID automatically.</p> <ul> <li>Purpose: To quickly set up Hyperion to index a new blockchain.</li> <li>Behavior:<ol> <li>Tests connectivity to the provided HTTP and SHIP endpoints.</li> <li>Attempts to automatically determine the Chain ID and a suitable parser version.</li> <li>Creates <code>config/chains/&lt;shortName&gt;.config.json</code>.</li> <li>Updates <code>config/connections.json</code> with the new chain's endpoint details.</li> <li>Important: After running this command, you must review and customize the generated <code>config/chains/&lt;shortName&gt;.config.json</code> to tailor Hyperion's indexing behavior (e.g., scaling, filters, features) for that specific chain.</li> </ol> </li> </ul> <p>Usage: <pre><code>./hyp-config chains new &lt;shortName&gt; --http &lt;http_endpoint&gt; --ship &lt;ship_endpoint&gt;\n</code></pre></p> <p>Arguments:</p> <ul> <li><code>&lt;shortName&gt;</code>: A short, unique identifier for the chain (e.g., <code>wax</code>, <code>eos-testnet</code>).</li> </ul> <p>Options:</p> <ul> <li><code>--http &lt;http_endpoint&gt;</code>: Required. The HTTP API endpoint for the chain's nodeos instance (e.g., <code>https://wax.greymass.com</code>).</li> <li><code>--ship &lt;ship_endpoint&gt;</code>: Required. The State History Plugin (SHIP) WebSocket endpoint (e.g., <code>wss://ship.wax.eosusa.io</code>).</li> <li><code>--use-pending</code>: Optional. Updates the <code>config/connections.json</code> to include the chain and updates the status of an already existing chain file to configured</li> </ul>"},{"location":"providers/cli-tools/hyp-config/#chains-list","title":"<code>chains list</code>","text":"<p>Alias: <code>ls</code></p> <p>Lists all configured chains found in the <code>config/chains/</code> directory. It displays details for each chain, such as chain name, configured endpoints, parser version, and status (configured or pending).</p> <p>Usage: <pre><code>./hyp-config chains list [options]\n</code></pre></p> <p>Options:</p> <ul> <li><code>--valid</code>: Only display chains that have corresponding entries in <code>connections.json</code>.</li> <li><code>--fix-missing-fields</code>: Automatically add missing configuration fields with default values based on the reference configuration (<code>config.ref.json</code>).</li> </ul> <p>Example:</p> <pre><code>./hyp-config chains list --fix-missing-fields\n</code></pre>"},{"location":"providers/cli-tools/hyp-config/#chains-remove-shortname","title":"<code>chains remove &lt;shortName&gt;</code>","text":"<p>Removes the configuration file <code>config/chains/&lt;shortName&gt;.config.json</code> and deletes the corresponding chain entry from <code>config/connections.json</code>. Backups of both modified files are created in the <code>config/configuration_backups/</code> directory before deletion.</p> <ul> <li>Purpose: To remove a chain from Hyperion's configuration.</li> <li>Behavior: Prompts for confirmation.</li> </ul> <p>Usage: <pre><code>./hyp-config chains remove &lt;shortName&gt;\n</code></pre></p> <p>Arguments:</p> <ul> <li><code>&lt;shortName&gt;</code>: The alias of the chain configuration to remove.</li> </ul> <p>Example: <pre><code>./hyp-config chains remove wax-testnet\n</code></pre></p>"},{"location":"providers/cli-tools/hyp-config/#chains-test-shortname","title":"<code>chains test &lt;shortName&gt;</code>","text":"<p>Tests the configured HTTP and SHIP endpoints for a specific chain. It verifies:</p> <ol> <li>Connectivity to both endpoints.</li> <li>That the Chain ID reported by the HTTP endpoint matches the Chain ID from the SHIP endpoint(s).</li> <li>That both of these match the Chain ID stored in <code>config/connections.json</code> for this chain.</li> </ol> <ul> <li>Purpose: To ensure the configured endpoints for a chain are operational and consistent.</li> <li>Behavior: Outputs test results, highlighting any mismatches or connectivity issues.</li> </ul> <p>Usage: <pre><code>./hyp-config chains test &lt;shortName&gt;\n</code></pre></p> <p>Arguments:</p> <ul> <li><code>&lt;shortName&gt;</code>: The alias of the chain configuration to test.</li> </ul> <p>Example: <pre><code>./hyp-config chains test wax\n</code></pre></p>"},{"location":"providers/cli-tools/hyp-config/#chains-validate-shortname-options","title":"<code>chains validate &lt;shortName&gt; [options]</code>","text":"<p>Validates a chain configuration file against the Zod schema to ensure all required fields are present and have correct types. Can automatically fix missing or invalid fields using reference defaults.</p> <ul> <li>Purpose: To ensure a chain's configuration file meets Hyperion's requirements and has all necessary fields.</li> <li>Behavior: Validates the configuration and reports any issues. With the <code>--fix</code> option, it can automatically correct problems.</li> </ul> <p>Usage: <pre><code>./hyp-config chains validate &lt;shortName&gt; [options]\n</code></pre></p> <p>Arguments:</p> <ul> <li><code>&lt;shortName&gt;</code>: The alias of the chain configuration to validate.</li> </ul> <p>Options:</p> <ul> <li><code>--fix</code>: Automatically fix missing or invalid fields using reference configuration and sensible defaults. Creates a backup before making changes.</li> </ul> <p>Validation Features:</p> <ul> <li>Schema Validation: Validates the entire configuration structure using comprehensive Zod schemas</li> <li>Detailed Error Reporting: Groups validation errors by configuration path for easy debugging</li> <li>Configuration Summary: On successful validation, displays key configuration details</li> <li>JSON Syntax Checking: Catches and reports JSON parsing errors</li> <li>Auto-Fix Capability: Automatically adds missing fields with appropriate defaults</li> </ul> <p>Auto-Fix Behavior: When using the <code>--fix</code> option, the command will:</p> <ol> <li>Create a timestamped backup in <code>config/configuration_backups/</code></li> <li>Apply values from the reference configuration (<code>config.ref.json</code>) where available</li> <li>Use sensible defaults for fields not present in the reference</li> <li>Save the fixed configuration and report all changes made</li> </ol> <p>Examples:</p> <pre><code># Validate configuration only (shows errors if any)\n./hyp-config chains validate wax\n\n# Validate and automatically fix missing fields\n./hyp-config chains validate wax --fix\n</code></pre> <p>Sample Output (with --fix):</p> <pre><code>Validating chain config for wax...\n\ud83d\udd27 Attempting to fix configuration issues...\n\ud83d\udce6 Backup created: config/configuration_backups/wax.config.backup.1748562368468.json\n   \u2713 Fixed missing field: api.provider_logo = \"\" (default)\n   \u2713 Fixed missing field: settings.ship_request_rev = \"\" (default)\n   \u2713 Fixed missing field: settings.bypass_index_map = false (default)\n\ud83d\udcbe Fixed configuration saved to config/chains/wax.config.json\n\ud83c\udf89 Successfully fixed 3 field(s)!\n\u2705 Chain config for wax is valid!\n\n\ud83d\udccb Configuration Summary:\n   Chain: wax\n   API Port: 7000\n   Stream Port: 1234\n   Indexer Enabled: true\n   API Enabled: true\n   Debug Mode: false\n</code></pre>"},{"location":"providers/cli-tools/hyp-config/#contracts-commands","title":"<code>contracts</code> Commands","text":"<p>This group of commands manages contract-specific state indexing configurations. These settings are stored within each chain's configuration file (e.g., <code>config/chains/wax.config.json</code> under <code>features.contract_state.contracts</code>). This feature allows Hyperion to index the current state of specified contract tables into MongoDB for fast querying via <code>/v2/state/get_table_rows</code>.</p> <p>Requirement: MongoDB integration must be enabled and correctly configured in <code>config/connections.json</code>. The <code>features.contract_state.enabled</code> flag in the chain's config file must also be <code>true</code>.</p>"},{"location":"providers/cli-tools/hyp-config/#contracts-list-chainname","title":"<code>contracts list &lt;chainName&gt;</code>","text":"<p>Displays the current contract state indexing configuration for the specified chain. It shows which contract accounts and tables within those accounts are configured for state indexing, along with their MongoDB index definitions.</p> <ul> <li>Purpose: To review which contracts and tables are being tracked for their current state.</li> </ul> <p>Usage: <pre><code>./hyp-config contracts list &lt;chainName&gt;\n</code></pre></p> <p>Arguments:</p> <ul> <li><code>&lt;chainName&gt;</code>: The alias of the chain whose contract configuration should be listed.</li> </ul> <p>Example: <pre><code>./hyp-config contracts list wax\n</code></pre></p>"},{"location":"providers/cli-tools/hyp-config/#contracts-add-single-chainname-account-table-autoindex-indicesjson","title":"<code>contracts add-single &lt;chainName&gt; &lt;account&gt; &lt;table&gt; &lt;autoIndex&gt; [indicesJson]</code>","text":"<p>Adds or updates the state indexing configuration for a single table within a specified contract account for the given chain.</p> <ul> <li>Purpose: To configure Hyperion to track the current state of a specific contract table in MongoDB.</li> <li>Behavior: Modifies the <code>features.contract_state.contracts</code> section in <code>config/chains/&lt;chainName&gt;.config.json</code>.</li> </ul> <p>Usage: <pre><code>./hyp-config contracts add-single &lt;chainName&gt; &lt;account&gt; &lt;table&gt; &lt;autoIndex&gt; '[indicesJson]'\n</code></pre></p> <p>Note the single quotes around <code>[indicesJson]</code> to ensure the shell passes it as a single argument.</p> <p>Arguments:</p> <ul> <li><code>&lt;chainName&gt;</code>: The alias of the chain.</li> <li><code>&lt;account&gt;</code>: The contract account name.</li> <li><code>&lt;table&gt;</code>: The table name within the contract.</li> <li><code>&lt;autoIndex&gt;</code>: Boolean (<code>true</code> or <code>false</code>) indicating if default indices should be automatically created.<ul> <li><code>true</code>: Hyperion will attempt to automatically create MongoDB indexes based on the table's ABI structure.</li> <li><code>false</code>: You must provide custom index definitions via <code>indicesJson</code>.</li> </ul> </li> <li><code>[indicesJson]</code>: (optional if <code>autoIndex</code> is <code>true</code>, required if <code>false</code>): A JSON string defining custom MongoDB indexes.<ul> <li>Keys are field names (dot-notation for nested fields).</li> <li>Values define index type: <code>1</code> (ascending), <code>-1</code> (descending), <code>\"text\"</code> (text search), <code>\"date\"</code> (for date fields if special handling needed by Hyperion).</li> </ul> </li> </ul> <p>Example (Auto-indexing): <pre><code>./hyp-config contracts add-single wax eosio.token accounts true\n</code></pre></p> <p>Example (Manual Indexing): <pre><code>./hyp-config contracts add-single wax mycontract userdata false '{\"user_id\": 1, \"profile.email\": \"text\"}'\n</code></pre></p>"},{"location":"providers/cli-tools/hyp-config/#contracts-add-multiple-chainname-account-tablesjson","title":"<code>contracts add-multiple &lt;chainName&gt; &lt;account&gt; '&lt;tablesJson[]&gt;'</code>","text":"<p>Note the single quotes around <code>&lt;tablesJson&gt;</code> to ensure the shell passes it as a single argument.</p> <p>Adds or updates the state indexing configuration for multiple tables within a specific contract account using a single JSON string input.</p> <ul> <li>Purpose: Conveniently configure multiple tables for a contract at once.</li> <li>Behavior: Modifies <code>config/chains/&lt;chainName&gt;.config.json</code>.</li> </ul> <p>Usage: <pre><code>./hyp-config contracts add-multiple &lt;chainName&gt; &lt;account&gt; '&lt;tablesJson[]&gt;'\n</code></pre></p> <p>Arguments:</p> <ul> <li><code>&lt;chainName&gt;</code>: The alias of the chain.</li> <li><code>&lt;account&gt;</code>: The contract account name.</li> <li><code>&lt;tablesJson[]&gt;</code>: A JSON string representing an array of table configuration objects.<ul> <li><code>name</code> (string): The table name.</li> <li><code>autoIndex</code> (boolean): Whether to auto-index.</li> <li><code>indices</code> (object, optional if <code>autoIndex</code> is <code>true</code>, required if <code>false</code>): Custom MongoDB index definitions (same format as <code>indicesJson</code> in <code>add-single</code>).</li> </ul> </li> </ul> <p>Example: <pre><code>./hyp-config contracts add-multiple wax eosio.token '[{\"name\":\"accounts\",\"autoIndex\":true},{\"name\":\"stat\",\"autoIndex\":false,\"indices\":{\"issuer\":1,\"supply\":-1}}]'\n</code></pre></p>"},{"location":"providers/cli-tools/hyp-config/#configuration-editing","title":"Configuration Editing","text":"<p>Edit and manage configuration values within chain configuration files with automatic validation against the reference configuration.</p>"},{"location":"providers/cli-tools/hyp-config/#get-chain-configpath","title":"<code>get &lt;chain&gt; &lt;configPath&gt;</code>","text":"<p>Retrieves a specific configuration value from a chain's configuration file. The configuration path is validated against the reference configuration to ensure it exists and is valid.</p> <p>Usage:</p> <pre><code>./hyp-config get &lt;chain&gt; &lt;configPath&gt;\n</code></pre> <p>Arguments:</p> <ul> <li><code>&lt;chain&gt;</code>: The short name of the chain whose configuration to read.</li> <li><code>&lt;configPath&gt;</code>: The dot-notation path to the configuration value (e.g., <code>indexer.start_on</code>, <code>scaling.readers</code>, <code>api.server_port</code>).</li> </ul> <p>Examples:</p> <pre><code>./hyp-config get wax indexer.start_on\n./hyp-config get eos scaling.readers\n./hyp-config get telos api.server_port\n</code></pre>"},{"location":"providers/cli-tools/hyp-config/#set-chain-configpath-value","title":"<code>set &lt;chain&gt; &lt;configPath&gt; &lt;value&gt;</code>","text":"<p>Sets a specific configuration value in a chain's configuration file. The configuration path and value type are validated against the reference configuration. Creates an automatic backup before making changes.</p> <p>Usage:</p> <pre><code>./hyp-config set &lt;chain&gt; &lt;configPath&gt; &lt;value&gt;\n</code></pre> <p>Arguments:</p> <ul> <li><code>&lt;chain&gt;</code>: The short name of the chain whose configuration to modify.</li> <li><code>&lt;configPath&gt;</code>: The dot-notation path to the configuration value.</li> <li><code>&lt;value&gt;</code>: The new value to set. Can be a string, number, boolean, or JSON for complex values.</li> </ul> <p>Type Validation: The value must match the expected type from the reference configuration:</p> <ul> <li>Numbers: <code>1000</code>, <code>4096</code></li> <li>Booleans: <code>true</code>, <code>false</code></li> <li>Strings: <code>\"example_string\"</code></li> <li>Arrays: <code>[1,2,3]</code> or <code>[\"item1\",\"item2\"]</code></li> <li>Objects: <code>{\"key\":\"value\"}</code></li> </ul> <p>Examples:</p> <pre><code>./hyp-config set wax indexer.start_on 1000\n./hyp-config set eos scaling.readers 4\n./hyp-config set telos api.enabled true\n./hyp-config set wax api.limits '{\"get_actions\": 2000}'\n</code></pre>"},{"location":"providers/cli-tools/hyp-config/#set-default-chain-configpath","title":"<code>set-default &lt;chain&gt; &lt;configPath&gt;</code>","text":"<p>Resets a specific configuration value to its default value from the reference configuration. Creates an automatic backup before making changes.</p> <p>Usage:</p> <pre><code>./hyp-config set-default &lt;chain&gt; &lt;configPath&gt;\n</code></pre> <p>Arguments:</p> <ul> <li><code>&lt;chain&gt;</code>: The short name of the chain whose configuration to reset.</li> <li><code>&lt;configPath&gt;</code>: The dot-notation path to the configuration value to reset.</li> </ul> <p>Examples:</p> <pre><code>./hyp-config set-default wax indexer.start_on\n./hyp-config set-default eos api.server_port\n./hyp-config set-default telos scaling.readers\n</code></pre>"},{"location":"providers/cli-tools/hyp-config/#list-paths-chain-filter-category","title":"<code>list-paths &lt;chain&gt; [--filter &lt;category&gt;]</code>","text":"<p>Lists all valid configuration paths available for modification. Useful for discovering available configuration options and their current values and types.</p> <p>Usage:</p> <pre><code>./hyp-config list-paths &lt;chain&gt; [options]\n</code></pre> <p>Arguments:</p> <ul> <li><code>&lt;chain&gt;</code>: The short name of the chain (used for command context, but paths are from reference config).</li> </ul> <p>Options:</p> <ul> <li><code>--filter &lt;category&gt;</code>: Filter paths by category (e.g., <code>api</code>, <code>indexer</code>, <code>scaling</code>, <code>features</code>).</li> </ul> <p>Examples:</p> <pre><code>./hyp-config list-paths wax\n./hyp-config list-paths wax --filter indexer\n./hyp-config list-paths wax --filter scaling\n./hyp-config list-paths wax --filter api\n</code></pre> <p>Available Categories:</p> <ul> <li><code>api</code> - API server configuration</li> <li><code>indexer</code> - Indexer process configuration  </li> <li><code>settings</code> - General settings</li> <li><code>scaling</code> - Performance and scaling options</li> <li><code>features</code> - Feature toggles</li> <li><code>blacklists</code> - Action and delta blacklists</li> <li><code>whitelists</code> - Action and delta whitelists</li> <li><code>prefetch</code> - Prefetch settings</li> <li><code>hub</code> - Hub configuration</li> <li><code>plugins</code> - Plugin settings</li> <li><code>alerts</code> - Alert system configuration</li> </ul>"},{"location":"providers/cli-tools/hyp-config/#commands-alias","title":"Commands Alias","text":"<p>The following commands are alias for the commands above.</p> <ul> <li><code>list chains</code> (for <code>chains list</code>)<ul> <li>Alias: <code>ls chains</code></li> </ul> </li> <li><code>new chain &lt;shortName&gt;</code> (for <code>chains new &lt;shortName&gt;</code>)<ul> <li>Alias: <code>n chain &lt;shortName&gt;</code></li> </ul> </li> <li><code>remove chain &lt;shortName&gt;</code> (for <code>chains remove &lt;shortName&gt;</code>)<ul> <li>Alias: <code>rm chain &lt;shortName&gt;</code></li> </ul> </li> </ul>"},{"location":"providers/cli-tools/hyp-control/","title":"Indexer Control CLI (<code>hyp-control</code>)","text":"<p>The <code>hyp-control</code> command-line interface (CLI) tool allows you to interact with and manage running Hyperion indexer processes for specific chains.</p> <p>It communicates with the indexer's control port (configured in <code>config/connections.json</code>, defaults to port <code>7002</code> for the first chain, incrementing for subsequent chains).</p> <p>Important</p> <p>The Hyperion indexer process for the target chain must be running and its control port must be accessible from the machine where you execute <code>hyp-control</code> commands.</p>"},{"location":"providers/cli-tools/hyp-control/#general-usage","title":"General Usage","text":"<p>Commands typically follow the pattern:</p> <p><code>./hyp-control &lt;command-group&gt; &lt;sub-command&gt; &lt;chainName&gt; [options...]</code></p> <p>or</p> <p><code>./hyp-control &lt;command&gt; &lt;chainName&gt; [options...]</code></p> <p>The <code>&lt;chainName&gt;</code> argument usually refers to the short alias of the chain as defined in your Hyperion configuration (e.g., <code>wax</code>, <code>eos</code>).</p>"},{"location":"providers/cli-tools/hyp-control/#indexer-commands","title":"<code>indexer</code> Commands","text":"<p>This group of commands is used to control the operational state of an indexer.</p>"},{"location":"providers/cli-tools/hyp-control/#indexer-start-chainname","title":"<code>indexer start &lt;chainName&gt;</code>","text":"<p>Sends a command to the specified chain's indexer master process to ensure it is actively processing blocks. If the indexer is already running and processing, this command acknowledges its current state. If it was paused or in a state where it could start/resume, it initiates processing.</p> <ul> <li>Purpose: To start or confirm that the indexer is actively indexing data.</li> <li>Behavior: Sends a <code>start_indexer</code> event. The indexer will respond with <code>indexer-started</code> or <code>indexer-already-running</code>.</li> </ul> <p>Usage: <pre><code>./hyp-control indexer start &lt;chainName&gt; [--host &lt;indexerHost&gt;]\n</code></pre></p> <p>Arguments:</p> <ul> <li><code>&lt;chainName&gt;</code>: The alias of the chain whose indexer you want to start.</li> </ul> <p>Options:</p> <ul> <li><code>--host &lt;indexerHost&gt;</code>: (Optional) The hostname or IP address where the indexer's control port is listening. Defaults to <code>localhost</code> if not specified. Use this if <code>hyp-control</code> is run on a different machine than the indexer.</li> </ul> <p>Example: <pre><code>./hyp-control indexer start wax\n./hyp-control indexer start eos-testnet --host 192.168.1.100\n</code></pre></p>"},{"location":"providers/cli-tools/hyp-control/#indexer-stop-chainname","title":"<code>indexer stop &lt;chainName&gt;</code>","text":"<p>Sends a command to gracefully stop the indexer for the specified chain. The indexer will attempt to finish processing any data currently in its queues before shutting down its workers.</p> <ul> <li>Purpose: To safely stop the indexer, allowing it to drain its internal queues and prevent data loss. This is the recommended way to stop an indexer before maintenance or updates.</li> <li>Behavior: Sends a <code>stop_indexer</code> event. The indexer will log its shutdown progress and eventually respond with <code>indexer_stopped</code>. This process can take some time depending on queue sizes.</li> </ul> <p>Usage: <pre><code>./hyp-control indexer stop &lt;chainName&gt; [--host &lt;indexerHost&gt;]\n</code></pre></p> <p>Arguments &amp; Options: Same as <code>indexer start</code>.</p> <p>Example: <pre><code>./hyp-control indexer stop wax\n</code></pre></p>"},{"location":"providers/cli-tools/hyp-control/#sync-commands-state-synchronization","title":"<code>sync</code> Commands (State Synchronization)","text":"<p>This group of commands is used to manually trigger or re-trigger the synchronization of specific current state data into MongoDB. This is particularly useful if:</p> <ul> <li>State tracking features (e.g., <code>features.tables.accounts</code>, <code>features.contract_state.enabled</code>) were enabled after the indexer had already processed the relevant historical blocks.</li> <li>You suspect an inconsistency in the MongoDB state data and want to refresh it.</li> </ul> <p>Important:</p> <ul> <li>The corresponding feature must be enabled in the chain's configuration file (<code>config/chains/&lt;chainName&gt;.config.json</code>) for the sync command to have an effect for that specific data type.</li> <li>These commands will attempt to pause the relevant indexer components during the sync process to ensure data consistency and then resume them. If the indexer cannot be paused (e.g., it's offline), the sync will proceed with a warning.</li> </ul>"},{"location":"providers/cli-tools/hyp-control/#sync-accounts-chainname","title":"<code>sync accounts &lt;chainName&gt;</code>","text":"<p>Triggers a full scan of the blockchain (via nodeos <code>get_table_rows</code> on system contract token tables, typically <code>eosio.token::accounts</code> or similar, by analyzing ABIs) to update the token balance information stored in the <code>accounts</code> collection in MongoDB for the specified chain.</p> <ul> <li>Requires: <code>features.tables.accounts: true</code> in <code>config/chains/&lt;chainName&gt;.config.json</code>.</li> </ul> <p>Usage: <pre><code>./hyp-control sync accounts &lt;chainName&gt; [--host &lt;indexerHost&gt;]\n</code></pre> Arguments &amp; Options: Same as <code>indexer start</code>.</p>"},{"location":"providers/cli-tools/hyp-control/#sync-voters-chainname","title":"<code>sync voters &lt;chainName&gt;</code>","text":"<p>Triggers a full scan of the <code>eosio::voters</code> table (or equivalent) to update voter registration and staking information in the <code>voters</code> collection in MongoDB.</p> <ul> <li>Requires: <code>features.tables.voters: true</code> in <code>config/chains/&lt;chainName&gt;.config.json</code>.</li> </ul> <p>Usage: <pre><code>./hyp-control sync voters &lt;chainName&gt; [--host &lt;indexerHost&gt;]\n</code></pre> Arguments &amp; Options: Same as <code>indexer start</code>.</p>"},{"location":"providers/cli-tools/hyp-control/#sync-proposals-chainname","title":"<code>sync proposals &lt;chainName&gt;</code>","text":"<p>Triggers a full scan of the <code>eosio.msig::proposal</code> and <code>eosio.msig::approvals2</code> tables (or equivalents) to update multisig proposal information in the <code>proposals</code> collection in MongoDB.</p> <ul> <li>Requires: <code>features.tables.proposals: true</code> in <code>config/chains/&lt;chainName&gt;.config.json</code>.</li> </ul> <p>Usage: <pre><code>./hyp-control sync proposals &lt;chainName&gt; [--host &lt;indexerHost&gt;]\n</code></pre> Arguments &amp; Options: Same as <code>indexer start</code>.</p>"},{"location":"providers/cli-tools/hyp-control/#sync-contract-state-chainname","title":"<code>sync contract-state &lt;chainName&gt;</code>","text":"<p>Triggers a full scan for all contract tables explicitly defined for state indexing under <code>features.contract_state.contracts</code> in the <code>config/chains/&lt;chainName&gt;.config.json</code> file. It populates or updates their corresponding collections in MongoDB (e.g., <code>&lt;contractName&gt;-&lt;tableName&gt;</code>).</p> <ul> <li>Requires: <code>features.contract_state.enabled: true</code> and contract/table definitions in <code>config/chains/&lt;chainName&gt;.config.json</code>.</li> </ul> <p>Usage: <pre><code>./hyp-control sync contract-state &lt;chainName&gt; [--host &lt;indexerHost&gt;]\n</code></pre> Arguments &amp; Options: Same as <code>indexer start</code>.</p>"},{"location":"providers/cli-tools/hyp-control/#sync-all-chainname","title":"<code>sync all &lt;chainName&gt;</code>","text":"<p>Sequentially triggers <code>sync voters</code>, <code>sync accounts</code>, <code>sync proposals</code>, and <code>sync contract-state</code> for the specified chain.</p> <p>Usage: <pre><code>./hyp-control sync all &lt;chainName&gt; [--host &lt;indexerHost&gt;]\n</code></pre> Arguments &amp; Options: Same as <code>indexer start</code>.</p> <p>Example for Syncing: <pre><code># Sync token accounts for the WAX chain\n./hyp-control sync accounts wax\n\n# Sync all configured state types for the EOS chain, targeting a remote indexer\n./hyp-control sync all eos --host hyperion-indexer.internal.net\n</code></pre></p>"},{"location":"providers/cli-tools/hyp-control/#diagnostic-commands","title":"Diagnostic Commands","text":"<p>These commands provide insights into the indexer's performance and resource usage.</p>"},{"location":"providers/cli-tools/hyp-control/#get-usage-map-chainname","title":"<code>get-usage-map &lt;chainName&gt;</code>","text":"<p>Requests and displays a map of contract usage statistics from the indexer. This shows which smart contracts are generating the most action processing load, the percentage of total load, and which deserializer workers are assigned to them. It also includes information about the load distribution cycle.</p> <ul> <li>Purpose: To identify high-traffic contracts and understand how the indexer is distributing its workload.</li> </ul> <p>Usage: <pre><code>./hyp-control get-usage-map &lt;chainName&gt; [--host &lt;indexerHost&gt;]\n</code></pre> Arguments &amp; Options: Same as <code>indexer start</code>.</p>"},{"location":"providers/cli-tools/hyp-control/#get-memory-usage-chainname","title":"<code>get-memory-usage &lt;chainName&gt;</code>","text":"<p>Requests and displays the current memory usage (resident set size) for each active worker process within the indexer for the specified chain.</p> <ul> <li>Purpose: To monitor the memory footprint of different indexer components.</li> </ul> <p>Usage: <pre><code>./hyp-control get-memory-usage &lt;chainName&gt; [--host &lt;indexerHost&gt;]\n</code></pre> Arguments &amp; Options: Same as <code>indexer start</code>.</p>"},{"location":"providers/cli-tools/hyp-control/#get-heap-chainname","title":"<code>get-heap &lt;chainName&gt;</code>","text":"<p>Requests and displays V8 JavaScript engine heap statistics for each active worker process within the indexer. This includes used heap size, total heap size, and heap size limit.</p> <ul> <li>Purpose: To get a more detailed view of JavaScript memory allocation and identify potential memory leaks or pressure within the V8 engine.</li> </ul> <p>Usage: <pre><code>./hyp-control get-heap &lt;chainName&gt; [--host &lt;indexerHost&gt;]\n</code></pre> Arguments &amp; Options: Same as <code>indexer start</code>.</p> <p>Example for Diagnostics: <pre><code># View contract processing load on the WAX indexer\n./hyp-control get-usage-map wax\n\n# Check memory usage of the EOS indexer workers\n./hyp-control get-memory-usage eos\n</code></pre></p>"},{"location":"providers/cli-tools/hyp-control/#internal-indexer-control-pauseresume-advanced","title":"Internal Indexer Control (Pause/Resume - Advanced)","text":"<p>The <code>hyp-control</code> tool itself uses <code>pause_indexer</code> and <code>resume_indexer</code> events internally when performing <code>sync</code> operations. While these are not exposed as direct top-level commands in <code>hyp-control</code>, the underlying <code>IndexerController</code> client (used by <code>hyp-control</code>) has methods for them. Direct manual pausing/resuming is generally an advanced operation and should be used with caution as it can disrupt the normal flow of indexing if not managed correctly.</p> <ul> <li><code>pause_indexer</code>: Temporarily halts a specific type of processing (e.g., \"table-voters\", \"dynamic-table\") within the indexer.</li> <li><code>resume_indexer</code>: Resumes the previously paused processing type.</li> </ul> <p>These are typically orchestrated by the <code>sync</code> commands to ensure data consistency during state table rebuilds.</p>"},{"location":"providers/cli-tools/hyp-es-config/","title":"Elasticsearch Index Management CLI (<code>hyp-es-config</code>)","text":""},{"location":"providers/cli-tools/hyp-es-config/#overview","title":"Overview","text":"<p>The <code>hyp-es-config</code> command-line interface (CLI) tool is designed for managing Hyperion's Elasticsearch indices. Its primary functions include listing indices, repartitioning existing indices to new sizes, cleaning up old or new indices post-repartitioning, and monitoring active reindexing tasks.</p> <p>Why Repartition?</p> <p>Elasticsearch performance, especially for time-series data like blockchain history, can be significantly impacted by shard and index size. Over time, as data grows, individual index partitions might become too large, leading to slower queries and indexing. Repartitioning allows you to reorganize your data into more optimally sized indices, which can improve search performance and cluster stability.</p> <p>Key Concepts:</p> <ul> <li>Index Partitioning: Hyperion typically splits historical data (blocks, actions, deltas) into multiple Elasticsearch indices based on block number ranges. Each such index is a \"partition.\"</li> <li>Partition Size: The number of blocks contained within a single index partition (e.g., 1 million blocks per action index).</li> <li>Reindexing: The process of copying data from old indices to newly structured (repartitioned) indices. This is an I/O intensive operation.</li> </ul>"},{"location":"providers/cli-tools/hyp-es-config/#before-you-start-critical-prerequisites","title":"Before You Start: Critical Prerequisites","text":"<ul> <li> <p>Stop the Hyperion Indexer:</p> <p>\u203c\ufe0fBefore running any repartitioning or cleanup commands, you must stop the Hyperion indexer process for the chain you are working on. This prevents conflicts and data inconsistencies.</p> <pre><code># Example for the 'wax' chain\n./hyp-control indexer stop wax\n</code></pre> </li> <li> <p>Backup (Recommended) </p> <p>While <code>hyp-es-config</code> aims to be safe, backing up your Elasticsearch data or at least your Hyperion configuration files is always a good practice before major operations.</p> </li> <li> <p>Sufficient Disk Space </p> <p>Reindexing temporarily duplicates data. Ensure your Elasticsearch cluster has enough free disk space to accommodate the new indices before the old ones are removed.</p> </li> <li> <p>Elasticsearch Access: </p> <p>Ensure the machine running <code>hyp-es-config</code> can connect to your Elasticsearch cluster as configured in <code>config/connections.json</code>.</p> </li> </ul>"},{"location":"providers/cli-tools/hyp-es-config/#indices-registryjson-file","title":"<code>.indices-registry.json</code> File","text":"<p>The <code>hyp-es-config</code> tool creates and uses a file named <code>.indices-registry.json</code> located in a <code>.cli/</code> directory at the root of your Hyperion project.</p> <ul> <li>Purpose: This file tracks the state of repartitioning operations, specifically listing which indices are considered \"old\" (pre-repartition) and \"new\" (post-repartition) for each chain and index type (blocks, actions, deltas).</li> <li>Importance: It's essential for the <code>cleanup</code> command to correctly identify which set of indices to remove. Do not manually delete or modify this file unless you understand the implications.</li> <li>Creation: It's created automatically if it doesn't exist when repartitioning commands are run.</li> </ul>"},{"location":"providers/cli-tools/hyp-es-config/#commands","title":"Commands","text":""},{"location":"providers/cli-tools/hyp-es-config/#list-indices-list","title":"List Indices (<code>list</code>)","text":"<p>Alias: <code>ls</code></p> <p>Displays all Elasticsearch indices currently present in your cluster that Hyperion might be using or has used.</p> <ul> <li>Purpose: To get an overview of existing indices, their document counts, sizes, and health.</li> <li> <p>Usage:</p> <pre><code>./hyp-es-config list\n</code></pre> </li> </ul>"},{"location":"providers/cli-tools/hyp-es-config/#2-list-tasks-tasks","title":"2. List Tasks  (<code>tasks</code>)","text":"<p>Lists and monitors active reindexing tasks currently running within your Elasticsearch cluster. Reindexing operations initiated by <code>repartition</code> run as background tasks in Elasticsearch.</p> <ul> <li>Purpose: To check the progress and status of ongoing repartitioning efforts.</li> <li>Usage: <pre><code>./hyp-es-config tasks\n</code></pre> (Output will show task IDs, actions, running times, and descriptions.)</li> </ul>"},{"location":"providers/cli-tools/hyp-es-config/#repartition-indices-repartition-chainname","title":"Repartition Indices (<code>repartition &lt;chainName&gt;</code>)","text":"<p>Initiates the repartitioning process for block, action, and/or delta indices for the specified chain. This involves creating new indices with the target partition size(s) and starting Elasticsearch reindex tasks to copy data from the old indices to the new ones.</p> <ul> <li>Purpose: To reorganize historical data into new index partitions of a specified size, aiming to improve query performance and manageability.</li> <li>Behavior:<ol> <li>Analyzes existing indices for the chain and type.</li> <li>Determines the new index structure based on the provided partition sizes.</li> <li>If not run with <code>-y</code>, it will prompt for confirmation before proceeding.</li> <li>Updates the chain's configuration file (<code>config/chains/&lt;chainName&gt;.config.json</code>) with the new partition sizes.</li> <li>Creates new Elasticsearch index templates and target indices.</li> <li>Starts asynchronous Elasticsearch <code>_reindex</code> tasks to populate the new indices.</li> <li>Updates the <code>.cli/.indices-registry.json</code> file.  <p>The old indices are not deleted by this command; they remain until explicitly removed via the <code>cleanup</code> command.</p> </li> </ol> </li> </ul> <p>Usage: <pre><code>./hyp-es-config repartition &lt;chainName&gt; [options]\n</code></pre></p> <p>Arguments:</p> <ul> <li><code>&lt;chainName&gt;</code>: The alias of the chain whose indices you want to repartition (e.g., <code>wax</code>, <code>eos</code>).</li> </ul> <p>Options:</p> <ul> <li><code>--global &lt;size&gt;</code>: Sets the same new partition size (number of blocks) for all index types (block, action, delta).</li> <li><code>--blocks &lt;size&gt;</code>: Sets a specific new partition size for block indices.</li> <li><code>--actions &lt;size&gt;</code>: Sets a specific new partition size for action indices.</li> <li><code>--deltas &lt;size&gt;</code>: Sets a specific new partition size for delta indices.<ul> <li>Note on Action/Delta Sizes: These sizes are relative to block numbers. For example, if actions are partitioned by 1,000,000 blocks, an action index will contain actions from a 1,000,000 block range.</li> </ul> </li> <li><code>--force</code>: If target (new) indices already exist, delete and recreate them. Use with caution.</li> <li><code>--skip-missing</code>: If one type of index (e.g., deltas) is not found for the chain, continue repartitioning other found types instead of exiting.</li> <li><code>--skip-cleanup</code>: Suppresses the informational message about running the <code>cleanup</code> command after repartitioning.</li> <li><code>--continue-on-error</code>: If an error occurs during one part of the operation (e.g., reindexing one partition), attempt to continue with others.</li> <li><code>-y, --yes</code>: Skips all confirmation prompts, making the command non-interactive. Useful for scripts.</li> </ul> <p>Example:</p> <pre><code># Repartition all index types with the same size\n./hyp-es-config repartition telos --global 1000000 -y\n\n# Repartition specific index types with different sizes\n./hyp-es-config repartition telos --blocks 500000 --actions 1000000 --deltas 750000 -y\n</code></pre> <p>Attention</p> <p>When specifying sizes for <code>--actions</code> and <code>--deltas</code>, note that these sizes are always related to the block indices. The partitioning logic uses block sizes as the reference point, and the sizes for actions and deltas are aligned accordingly.</p> <p>After Running <code>repartition</code>:</p> <ul> <li>Use <code>./hyp-es-config tasks</code> to monitor the progress of the reindex tasks.</li> <li>Once all tasks are completed and you have verified the new indices are correct and Hyperion (after restart) is functioning as expected, proceed to the <code>cleanup</code> command.</li> </ul>"},{"location":"providers/cli-tools/hyp-es-config/#4-cleanup-indices-cleanup-chainname","title":"4. Cleanup Indices (<code>cleanup &lt;chainName&gt;</code>)","text":"<p>Removes either the old set of indices (pre-repartition) or the new set of indices (post-repartition) for the specified chain. This command relies on the <code>.cli/.indices-registry.json</code> file to identify which indices belong to which set.</p> <p>The importance of removing old indices</p> <p>The goal of repartitioning is to divide indices for better performance. To ensure everything is working correctly, old indices are kept. However, after validation, it is crucial to remove old indices to avoid unnecessary storage usage.</p> <p>Warning</p> <p>The cleanup will only remove old indices. To remove new ones, use the <code>--delete-new-indices</code> option.</p> <ul> <li>Purpose: To free up disk space by deleting the now-redundant set of indices after a successful repartition and verification.</li> <li>Behavior:<ul> <li>By default, it targets the \"old\" indices for deletion. Use  <code>--delete-new-indices</code> to delete new ones</li> <li>If not run with <code>-y</code>, it will prompt for confirmation before deleting indices.</li> <li>Updates the <code>.cli/.indices-registry.json</code> file to reflect the removed indices.</li> </ul> </li> </ul> <p>Usage:</p> <pre><code>./hyp-es-config cleanup &lt;chainName&gt; [options]\n</code></pre> <p>Arguments:</p> <ul> <li><code>&lt;chainName&gt;</code>: The short name of the chain for which to clean up indices.</li> </ul> <p>Options:</p> <ul> <li><code>--blocks</code>: Removes only block indices.</li> <li><code>--actions</code>: Removes only action indices.</li> <li><code>--deltas</code>: Remove only delta indices.     (If none of <code>--blocks</code>, <code>--actions</code>, <code>--deltas</code> are specified, all types are targeted for cleanup based on the registry.)</li> <li><code>--delete-new-indices</code>: \u203c\ufe0fUSE WITH EXTREME CAUTION. Reverses the cleanup target. Instead of deleting old indices, this will delete the newly created indices. This is typically only used if the repartitioning failed or resulted in problematic new indices, and you want to revert to using the old ones.</li> <li><code>-y, --yes</code>: Skips confirmation prompts.</li> <li><code>--continue-on-error</code>: If an error occurs deleting one index, attempt to continue deleting others.</li> </ul> <p>Example:</p> <pre><code># Remove old block indices\n./hyp-es-config cleanup telos --blocks -y\n\n# Remove new indices instead of old ones\n./hyp-es-config cleanup telos --delete-new-indices -y\n</code></pre>"},{"location":"providers/cli-tools/hyp-es-config/#important-notes-troubleshooting","title":"Important Notes &amp; Troubleshooting","text":"<ul> <li>Always Stop the Indexer: Failure to stop the Hyperion indexer before <code>repartition</code> or <code>cleanup</code> can lead to data corruption or incomplete operations.</li> <li>Disk Space: Repartitioning requires significant free disk space in Elasticsearch as data is temporarily duplicated.</li> <li>Time: Reindexing large amounts of data can take a very long time (hours or even days for large chains). Plan accordingly.</li> <li>Monitoring Tasks: Use <code>./hyp-es-config tasks</code> frequently during repartitioning to monitor progress. You can also use Elasticsearch's native Task Management API.</li> <li>Configuration File (<code>config/chains/&lt;chainName&gt;.config.json</code>): The <code>repartition</code> command updates the partition size settings in this file. If you manually revert a repartition (e.g., by deleting new indices), ensure this configuration file accurately reflects the partition sizes of the indices Hyperion should be using.</li> <li>If no partition size is provided, the tool attempts to fetch sizes from the configuration file.</li> <li><code>--continue-on-error</code>: While useful for partial successes, be sure to investigate any errors that occur even if the command continues.</li> <li>Log Files: Check Hyperion and Elasticsearch logs for detailed error messages if issues arise. Check the <code>logs/</code> directory.</li> </ul>"},{"location":"providers/cli-tools/hyp-es-config/#typical-workflow-and-examples","title":"Typical Workflow and Examples","text":"<ol> <li>Repartition (Example for 'wax' chain): <pre><code>./hyp-control indexer stop wax  # Stop the indexer first!\n./hyp-es-config repartition wax --global 2000000 -y\n</code></pre></li> <li> <p>Monitor Reindex Tasks: <pre><code>./hyp-es-config tasks\n</code></pre> (Wait until all reindex tasks for 'wax' are complete.)</p> </li> <li> <p>Verify (Important!):</p> <ul> <li>Restart the Hyperion indexer for 'wax': <code>./run.sh wax-indexer</code></li> <li>Restart the Hyperion API for 'wax': <code>./run.sh wax-api</code></li> <li>Thoroughly test Hyperion queries to ensure data integrity and accessibility with the new indices.</li> <li>Check Elasticsearch logs for any issues.</li> </ul> </li> <li> <p>Cleanup Old Indices (after successful verification): <pre><code>./hyp-control indexer stop wax  # Stop the indexer again for safety\n./hyp-es-config cleanup wax -y   # This will delete the old indices\n</code></pre> (After cleanup, you can restart the 'wax' indexer and API again.)</p> </li> </ol> <ul> <li> <p>Example: Cleanup only old action indices for 'eos':     <pre><code>./hyp-es-config cleanup eos --actions -y\n</code></pre></p> </li> <li> <p>Example: Revert - Delete new indices if repartitioning was problematic for 'telos': <pre><code>./hyp-es-config cleanup telos --delete-new-indices -y\n</code></pre> (Remember to adjust <code>config/chains/telos.config.json</code> back to old partition sizes if you do this, or re-run <code>repartition</code> correctly.)</p> </li> </ul>"},{"location":"providers/cli-tools/hyp-repair/","title":"Index Repair CLI (<code>hyp-repair</code>)","text":"<p>The <code>hyp-repair</code> command-line interface (CLI) is a specialized tool for diagnosing and repairing potential data integrity issues within Hyperion's Elasticsearch block indices. Primarily, it addresses two common problems:</p> <ol> <li>Forked Blocks: Blocks that were indexed but whose <code>prev_id</code> does not correctly link to the <code>block_id</code> of the preceding block number in the index. This often happens due to brief network forks where the indexer might initially follow a non-canonical chain segment. While Hyperion's live indexer attempts to handle forks automatically, past issues or interruptions might leave orphaned blocks.</li> <li>Missing Blocks: Gaps in the sequence of indexed blocks, where specific block numbers are entirely absent from the Elasticsearch <code>*-block-*</code> indices. This can occur due to indexer interruptions, network issues during indexing, or incomplete historical processing.</li> </ol> <p>Use Cases</p> <p>This tool is typically used when:</p> <ul> <li>You suspect data inconsistency due to past indexing interruptions or known issues.</li> <li>You want to verify the integrity of your indexed block sequence.</li> <li>You need to recover from a situation where the live indexer's fork handling might not have fully corrected an issue.</li> </ul> <p>Info</p> <p>You can also check missed blocks in the V2 Healthcheck <code>/v2/health</code></p>"},{"location":"providers/cli-tools/hyp-repair/#prerequisites-preparation","title":"Prerequisites &amp; Preparation","text":"<ol> <li>Hyperion Installation: Ensure you are running commands from the root directory of your Hyperion installation (e.g., <code>~/hyperion-history-api</code>).</li> <li>Elasticsearch Connection: The tool needs direct access to your Elasticsearch cluster as configured in <code>config/connections.json</code>.</li> <li>Nodeos Connection: For <code>scan</code> and <code>repair</code> commands involving fork validation, the tool needs access to the chain's Nodeos HTTP endpoint configured in <code>config/connections.json</code> to fetch canonical block information.</li> <li>Indexer Control Port Connection: For the <code>fill-missing</code> and <code>repair</code> commands (which trigger re-indexing ranges), the tool needs WebSocket access to the running Hyperion Indexer's control port (configured in <code>config/connections.json</code> under the chain's <code>control_port</code>, default <code>7002</code>).</li> <li>Backup Elasticsearch (Optional): Before running commands that modify data (<code>repair</code>, <code>fill-missing</code>), creating an Elasticsearch snapshot is recommended as a safety measure.</li> </ol>"},{"location":"providers/cli-tools/hyp-repair/#workflow-overview","title":"Workflow Overview","text":"<p>The general workflow for using <code>hyp-repair</code> is:</p> <ol> <li>Scan: Identify potential issues (forks or missing blocks) within a specified block range.<ul> <li><code>scan</code>: Performs a detailed scan comparing indexed blocks against each other (previous block ID linkage) and optionally against a live nodeos RPC endpoint to detect forks and inconsistencies.</li> <li><code>quick-scan</code>: Performs a faster scan focused only on identifying missing block number ranges using a binary search approach on indexed data.    These commands output JSON files detailing the found discrepancies.</li> </ul> </li> <li>Analyze: Review the output files generated by the scan to understand the scope of the issues.</li> <li>Repair/Fill: Execute commands to fix the identified issues (delete forked data, trigger re-indexing of missing ranges).<ul> <li><code>repair</code>: Deletes forked/incorrect data from various Hyperion indices (blocks, actions, deltas, etc.) based on a \"forked blocks\" JSON file generated by <code>scan</code>. It then triggers the Hyperion indexer (via its control port) to re-fetch and re-index the correct data for these ranges.</li> <li><code>fill-missing</code>: Triggers the Hyperion indexer to fetch and index blocks for ranges identified as missing, based on a \"missing blocks\" JSON file generated by <code>scan</code> or <code>quick-scan</code>.</li> </ul> </li> <li>Verify: Re-scan the affected range to confirm the repairs were successful.</li> </ol>"},{"location":"providers/cli-tools/hyp-repair/#repair-directory","title":"<code>.repair/</code> Directory","text":"<p>The <code>hyp-repair</code> tool, when run in <code>scan</code> or <code>quick-scan</code> mode, will typically save its findings (lists of forked or missing block ranges) into JSON files within a <code>.repair/</code> directory created at the root of your Hyperion project. These files are then used as input for the <code>repair</code> and <code>fill-missing</code> commands.</p>"},{"location":"providers/cli-tools/hyp-repair/#step-1-scan-for-issues","title":"Step 1: Scan for Issues","text":""},{"location":"providers/cli-tools/hyp-repair/#scan-chainname","title":"<code>scan &lt;chainName&gt;</code>","text":"<p>Performs a comprehensive scan of the indexed blocks for the specified chain to detect forked blocks and missing block number sequences. It compares <code>block_id</code> with <code>prev_id</code> of subsequent blocks and can validate against a live nodeos RPC endpoint.</p> <ul> <li>Purpose: To generate a detailed report (JSON file) of blocks that are part of a fork or block ranges that are missing.</li> <li>Behavior:<ul> <li>Reads blocks from Elasticsearch in batches.</li> <li>Compares <code>block_id</code> of a block with the <code>prev_id</code> of the next block in sequence.</li> <li>If an inconsistency is found, it consults the chain's configured nodeos HTTP API endpoint to determine the canonical block.</li> <li>Identifies ranges of purely missing block numbers.</li> <li>Outputs findings to JSON files (e.g., <code>.repair/&lt;chain&gt;-&lt;start&gt;-&lt;end&gt;-forked-blocks.json</code>, <code>.repair/&lt;chain&gt;-&lt;start&gt;-&lt;end&gt;-missing-blocks.json</code>).</li> </ul> </li> </ul> <p>Usage: <pre><code>./hyp-repair scan &lt;chainName&gt; [options]\n</code></pre></p> <p>Arguments:</p> <ul> <li><code>&lt;chainName&gt;</code>: The alias of the chain to scan.</li> </ul> <p>Options:</p> <ul> <li><code>-f, --first &lt;blockNumber&gt;</code>: The block number to start the scan from. Defaults to the first block found in the index.</li> <li><code>-l, --last &lt;blockNumber&gt;</code>: The block number to end the scan at. Defaults to the last block found in the index.</li> <li><code>-b, --batch &lt;size&gt;</code>: The number of blocks to fetch from Elasticsearch in each batch during the scan. Default is <code>2000</code>.</li> <li><code>-o, --out-file &lt;baseFilePath&gt;</code>: Base path and filename for the output JSON files (without extension). Defaults to <code>./.repair/&lt;chain&gt;-&lt;first&gt;-&lt;last&gt;</code>.</li> </ul> <p>Examples: <pre><code># Full scan (forks &amp; missing) for 'eos' chain using entire indexed range and default batch size\n./hyp-repair scan eos\n\n# Scan the WAX chain from block 100,000,000 to 110,000,000\n./hyp-repair scan wax -f 100000000 -l 110000000\n\n# Scan with custom output file base name and batch size\n./hyp-repair scan wax --first 100000000 --last 150000000 --batch 5000 --out-file /tmp/wax-repair-scan\n</code></pre> After the scan, check the <code>.repair/</code> directory or your specified output path for the JSON report files.</p>"},{"location":"providers/cli-tools/hyp-repair/#quick-scan-chainname","title":"<code>quick-scan &lt;chainName&gt;</code>","text":"<p>Performs a faster scan focused solely on identifying ranges of missing block numbers by checking for sequence continuity. It does not validate block IDs against a live node or detect forks.</p> <ul> <li>Purpose: To quickly find gaps in the indexed block sequence. Useful as a first pass or if forks are not suspected.</li> <li>Behavior:<ul> <li>Uses a binary search-like approach on Elasticsearch to find block number gaps.</li> <li>Outputs findings to a JSON file (e.g., <code>.repair/&lt;chain&gt;-&lt;start&gt;-&lt;end&gt;-missing-blocks.json</code>).</li> </ul> </li> </ul> <p>Usage: <pre><code>./hyp-repair quick-scan &lt;chainName&gt; [options]\n</code></pre></p> <p>Arguments:</p> <ul> <li><code>&lt;chainName&gt;</code>: The alias of the chain to scan.</li> </ul> <p>Options:</p> <ul> <li><code>-f, --first &lt;blockNumber&gt;</code>: The block number to start the scan from. Defaults to the first block found in the index.</li> <li><code>-l, --last &lt;blockNumber&gt;</code>: The block number to end the scan at. Defaults to the last block found in the index.</li> <li><code>-o, --out-file &lt;baseFilePath&gt;</code>: Base path and filename for the output JSON files (without extension). Defaults to <code>./.repair/&lt;chain&gt;-&lt;first&gt;-&lt;last&gt;</code>.</li> </ul> <p>Example: <pre><code># Quick scan (missing blocks only) for 'eos' chain\n./hyp-repair quick-scan eos\n\n# Quick scan with custom output\n./hyp-repair quick-scan eos --first 1 --last 200000000 --out-file /repairs/eos-missing\n</code></pre></p>"},{"location":"providers/cli-tools/hyp-repair/#results","title":"Results","text":""},{"location":"providers/cli-tools/hyp-repair/#output-path-forked-blocksjson","title":"<code>&lt;output-path&gt;-forked-blocks.json</code>","text":"<p>Lists ranges where block linkage (<code>prev_id</code> -&gt; <code>block_id</code>) is broken. Includes the block IDs identified as non-canonical within that range. Generated only by <code>scan</code>.     <pre><code>[\n  { \"start\": 123456, \"end\": 123458, \"ids\": [\"forked_block_id_1\", \"forked_block_id_2\"] }\n]\n</code></pre></p> <p></p>"},{"location":"providers/cli-tools/hyp-repair/#output-path-missing-blocksjson","title":"<code>&lt;output-path&gt;-missing-blocks.json</code>:","text":"<p>Lists ranges of consecutive missing block numbers.     <pre><code>[\n  { \"start\": 10001, \"end\": 10005, \"count\": 5 }\n]\n</code></pre></p>"},{"location":"providers/cli-tools/hyp-repair/#step-2-analyze-scan-results","title":"Step 2: Analyze Scan Results","text":"<p>Review the generated JSON files to understand the extent of missing or forked data.</p>"},{"location":"providers/cli-tools/hyp-repair/#view-filepath","title":"<code>view &lt;filePath&gt;</code>","text":"<p>Use the <code>view</code> command for a formatted table output:</p> <p>Usage: <pre><code>./hyp-repair view &lt;filePath&gt;\n</code></pre></p> <p>Arguments:</p> <ul> <li><code>&lt;filePath&gt;</code>: Path to the JSON report file.</li> </ul> <p>Example: <pre><code># View missing blocks file\n./hyp-repair view .repair/wax-100000000-150000000-missing-blocks.json\n\n# View forked blocks file\n./hyp-repair view .repair/wax-100000000-150000000-forked-blocks.json\n</code></pre></p>"},{"location":"providers/cli-tools/hyp-repair/#step-3-repair-issues","title":"Step 3: Repair Issues","text":""},{"location":"providers/cli-tools/hyp-repair/#31-repairing-forked-blocks","title":"3.1 Repairing Forked Blocks","text":""},{"location":"providers/cli-tools/hyp-repair/#repair-chainname-filepath","title":"<code>repair &lt;chainName&gt; &lt;filePath&gt;</code>","text":"<p>Repairs forked blocks based on a JSON file generated by the <code>scan</code> command. This command will:</p> <ol> <li>It reads the <code>-forked-blocks.json</code> file.</li> <li>For each range, it deletes the identified non-canonical block IDs from the <code>*-block-*</code> index in Elasticsearch.</li> <li>It deletes all associated actions, deltas, and state table entries (accounts, voters, etc.) within the block number range (<code>start</code> to <code>end</code>) of the fork from all relevant indices.</li> <li>After deletions, it instructs the indexer (via its control port) to re-fetch and re-index the block range (<code>start</code> to <code>end</code>), effectively replacing the forked data with the canonical data.</li> </ol> <p></p> <ul> <li>Purpose: To correct instances where Hyperion has indexed non-canonical blocks due to forks.</li> </ul> <p>Data Deletion</p> <p>This command permanently deletes data from your Elasticsearch indices. Always use <code>--dry</code> first and ensure you have backups.</p> <p>Usage: <pre><code>./hyp-repair repair &lt;chainName&gt; &lt;filePath&gt; [options]\n</code></pre></p> <p>Arguments:</p> <ul> <li><code>&lt;chainName&gt;</code>: The alias of the chain.</li> <li><code>&lt;filePath&gt;</code>: Path to the JSON file containing the forked block ranges (generated by <code>hyp-repair scan</code>).</li> </ul> <p>Options:</p> <ul> <li><code>-d, --dry</code>: Dry run. Shows what data would be deleted but does not perform any deletions or trigger re-indexing. Highly recommended for the first run.</li> <li><code>-h, --host &lt;hostAddress&gt;</code>: The hostname or IP address and port of the Hyperion indexer's control port (e.g., <code>localhost:7002</code> or <code>ws://indexer.internal:7002</code>). If only host is given, it assumes the default control port for that chain.</li> <li><code>-t, --check-tasks</code>: Checks if previous ES delete_by_query tasks are still running. Lists active Elasticsearch tasks.</li> </ul> <p>Example: <pre><code># Dry run: See what would be repaired for WAX using a specific report file\n./hyp-repair repair wax ./.repair/wax-100000000-110000000-forked-blocks.json --dry\n\n# Actual repair (after verifying dry run output):\n./hyp-repair repair wax ./.repair/wax-100000000-110000000-forked-blocks.json -h localhost:7002\n</code></pre></p>"},{"location":"providers/cli-tools/hyp-repair/#32-fill-missing-blocks","title":"3.2 Fill Missing Blocks","text":""},{"location":"providers/cli-tools/hyp-repair/#fill-missing-chainname-filepath","title":"<code>fill-missing &lt;chainName&gt; &lt;filePath&gt;</code>","text":"<p>Instructs the Hyperion indexer (via its control port) to fetch and index blocks for ranges identified as missing. This uses a JSON file generated by <code>hyp-repair scan</code> or <code>hyp-repair quick-scan</code>.</p> <ul> <li>Purpose: To fill gaps in Hyperion's indexed block data.</li> <li>Behavior: Sends commands to the indexer's control port to process the missing block ranges.</li> </ul> <p>Usage: <pre><code>./hyp-repair fill-missing &lt;chainName&gt; &lt;filePath&gt; [options]\n</code></pre></p> <p>Arguments:</p> <ul> <li><code>&lt;chainName&gt;</code>: The short name of the chain.</li> <li><code>&lt;filePath&gt;</code>: Path to the JSON file containing the missing block ranges.</li> </ul> <p>Options:</p> <ul> <li><code>-d, --dry</code>: Dry run. Simulates the process and indicates what would be done, but does not actually send commands to the indexer to fill blocks.</li> <li><code>-h, --host &lt;hostAddress&gt;</code>: The hostname or IP address and port of the Hyperion indexer's control port.</li> </ul> <p>Example: <pre><code># Dry run: See which missing block ranges would be processed for 'telos'\n./hyp-repair fill-missing telos ./.repair/telos_missing_scan-missing-blocks.json --dry\n\n# Actual fill operation:\n./hyp-repair fill-missing telos ./.repair/telos_missing_scan-missing-blocks.json -h localhost:7012 # Assuming Telos control port is 7012\n</code></pre></p> <p>Monitor the Hyperion indexer logs to observe the progress of filling missing blocks.</p>"},{"location":"providers/cli-tools/hyp-repair/#step-4-verify-repair","title":"Step 4: Verify Repair","text":"<p>Verify the results</p> <p>Run the scan again to verify that there are no more missing blocks or forks.</p> <p>After the <code>fill-missing</code> or <code>repair</code> commands complete and the indexer has processed the requested ranges, re-run the <code>scan</code> or <code>quick-scan</code> command on the same block range(s) to confirm that no further issues are detected.</p> <p><pre><code># Example re-scan\n./hyp-repair scan wax --first 100000000 --last 150000000\n</code></pre> If the scan reports no errors, the repair was successful.</p>"},{"location":"providers/cli-tools/hyp-repair/#important-considerations","title":"Important Considerations","text":"<ul> <li>Resource Intensive: Both scanning (especially full scans) and repairing (deleting and re-indexing) can be resource-intensive on your Elasticsearch cluster and the Nodeos instance. Plan these operations during periods of low load if possible.</li> <li>Time: Repairing large ranges or filling many missing blocks can take a significant amount of time.</li> <li><code>--dry</code> Runs: it is recommended to perform a <code>--dry</code> run for <code>repair</code> and <code>fill-missing</code> before executing them destructively to understand the scope of changes.</li> <li>Log Monitoring: Closely monitor Hyperion indexer logs and Elasticsearch logs during any repair or fill operations.</li> </ul>"},{"location":"providers/help/kibana/","title":"Kibana","text":"<p>The purpose here is to guide you through some basic steps using and configuring the Kibana.  For more detailed information, please, refer to the official documentation.</p>"},{"location":"providers/help/kibana/#running-kibana-with-systemd","title":"Running Kibana with <code>systemd</code>","text":"<p>To configure Kibana to start automatically when the system boots up, run the following commands: <pre><code>sudo /bin/systemctl daemon-reload\nsudo /bin/systemctl enable kibana.service\n</code></pre> Kibana can be started and stopped as follows: <pre><code>sudo systemctl start kibana.service\nsudo systemctl stop kibana.service\n</code></pre> These commands provide no feedback as to whether Kibana was started successfully or not.  Log information can be accessed via <code>journalctl -u kibana.service.</code></p>"},{"location":"providers/help/kibana/#opening-kibana","title":"Opening Kibana","text":"<p>Open http://localhost:5601 and check if you can access Kibana.</p> <p>If Kibana asks for credentials, the default user and password is:</p> <pre><code>user: elastic\npassword: changeme\n</code></pre> <p>If you installed Kibana through the <code>install_infra.sh</code> script, check your elastic_pass.txt file.</p> <p>If you can't access, check your credentials on your config file.</p>"},{"location":"providers/help/kibana/#creating-index-pattern","title":"Creating index pattern","text":"<ol> <li> <p>To create a new index pattern, go to <code>Management</code></p> <p></p> </li> <li> <p>Click on <code>Index Patterns</code> at the left menu</p> <p></p> </li> <li> <p>Click on <code>Create index pattern</code></p> <p></p> </li> <li> <p>Enter desired index pattern and click on <code>&gt; Next step</code></p> <p></p> <p>Tip</p> <p>Index Pattern List:  <code>eos-abi-*</code> <code>eos-action-*</code> <code>eos-block-*</code> <code>eos-logs-*</code> <code>eos-delta-*</code>   Where <code>eos</code> is the name of the chain.</p> </li> <li> <p>Select a time filter, if there are any, and click on <code>Create index pattern</code></p> <p></p> <p></p> </li> </ol>"},{"location":"providers/help/mongodb/","title":"MongoDB","text":"<p> Hyperion Configuration</p>"},{"location":"providers/help/mongodb/#mongodb-troubleshooting","title":"MongoDB Troubleshooting","text":"<p>This guide provides steps to diagnose and resolve common issues related to MongoDB when used with Hyperion. Remember, MongoDB is conditionally required by Hyperion; it's only used if you enable state tracking features (<code>features.tables.*</code> or <code>features.contract_state.enabled</code>) in your chain configuration file.</p> <p>For installation details, refer to the Manual Installation Guide.</p>"},{"location":"providers/help/mongodb/#basic-service-management-systemd","title":"Basic Service Management (<code>systemd</code>)","text":"<p>Ensure the MongoDB service (<code>mongod</code>) is running correctly on the host machine where it's installed.</p> <ul> <li> <p>Check Status: <pre><code>sudo systemctl status mongod\n</code></pre>     Look for <code>active (running)</code>.</p> </li> <li> <p>Start Service: <pre><code>sudo systemctl start mongod\n</code></pre></p> </li> <li> <p>Stop Service: <pre><code>sudo systemctl stop mongod\n</code></pre></p> </li> <li> <p>Enable on Boot: (Recommended)     <pre><code>sudo systemctl enable mongod\n</code></pre></p> </li> <li> <p>View Logs:</p> <ul> <li>Systemd Journal: <code>sudo journalctl -u mongod -f</code></li> <li>MongoDB Log File (Default): <code>sudo tail -f /var/log/mongodb/mongod.log</code></li> </ul> </li> </ul>"},{"location":"providers/help/mongodb/#connection-issues","title":"Connection Issues","text":"<p>If Hyperion (API or Indexer) cannot connect to MongoDB:</p> <ol> <li>Verify Service is Running: Use <code>sudo systemctl status mongod</code> on the MongoDB host.</li> <li>Check <code>config/connections.json</code>: Ensure the <code>mongodb</code> section has the correct <code>host</code>, <code>port</code>, <code>user</code>, <code>pass</code>, and <code>database_prefix</code>.     <pre><code>\"mongodb\": {\n  \"host\": \"127.0.0.1\", // Or the correct IP/hostname\n  \"port\": 27017,\n  \"database_prefix\": \"hyperion\", // Ensure this matches expectations\n  \"user\": \"hyperion_user\",   // Or blank if no auth\n  \"pass\": \"your_password\"  // Or blank if no auth\n}\n</code></pre></li> <li>Test Connectivity with <code>hyp-config</code>: Run this from your Hyperion installation directory. It specifically tests the configuration in <code>connections.json</code>.     <pre><code>./hyp-config connections test\n</code></pre>     Look for the MongoDB test result.</li> <li>Manual Connection Test (<code>mongosh</code>): From the Hyperion machine, try connecting directly using the MongoDB Shell.<ul> <li>Without Authentication: <pre><code>mongosh --host &lt;mongodb_host&gt; --port &lt;mongodb_port&gt;\n# Example: mongosh --host 192.168.1.100 --port 27017\n</code></pre></li> <li>With Authentication: <pre><code>mongosh --host &lt;mongodb_host&gt; --port &lt;mongodb_port&gt; --username &lt;user&gt; --password &lt;password&gt; --authenticationDatabase &lt;auth_db&gt;\n# Example: mongosh --host 192.168.1.100 --port 27017 -u hyperion_user -p your_password --authenticationDatabase admin\n</code></pre> If the manual connection fails:</li> <li>Check <code>bindIp</code>: On the MongoDB host, check the <code>/etc/mongod.conf</code> file. Ensure the <code>net.bindIp</code> setting includes the IP address of your Hyperion machine or <code>0.0.0.0</code> (less secure). Restart <code>mongod</code> after changes.</li> <li>Check Firewalls: Ensure firewalls on both the MongoDB host and the Hyperion machine allow traffic on the MongoDB port (default <code>27017</code>).</li> </ul> </li> </ol>"},{"location":"providers/help/mongodb/#authentication-errors","title":"Authentication Errors","text":"<p>If Hyperion logs show authentication failures:</p> <ol> <li>Verify Credentials: Double-check the <code>user</code> and <code>pass</code> in <code>config/connections.json</code> against the actual user credentials configured within MongoDB.</li> <li>Check User Existence &amp; Permissions: Use <code>mongosh</code> as an admin user on the MongoDB server to verify the Hyperion user exists and has the necessary roles (like <code>readWrite</code>) on the target Hyperion databases (e.g., <code>hyperion_wax</code>, <code>hyperion_eos</code>). Refer to MongoDB's documentation on managing users and roles.</li> </ol>"},{"location":"providers/help/mongodb/#missing-data-or-collections","title":"Missing Data or Collections","text":"<p>If <code>/v2/state/</code> endpoints return empty results or errors about missing collections (e.g., <code>accounts</code>, <code>voters</code>, <code>&lt;contract&gt;-&lt;table&gt;</code>):</p> <ol> <li>Check Feature Flags: Ensure the corresponding features are enabled in <code>config/chains/&lt;chain&gt;.config.json</code>:<ul> <li><code>features.tables.accounts: true</code> (for token balances/holders)</li> <li><code>features.tables.voters: true</code> (for voters)</li> <li><code>features.tables.proposals: true</code> (for proposals)</li> <li><code>features.contract_state.enabled: true</code> (for specific contract tables)</li> <li>Ensure the specific contracts/tables are defined under <code>features.contract_state.contracts</code> if using that feature.</li> </ul> </li> <li>Check Database/Collections Exist: Use <code>mongosh</code> on the MongoDB server:     <pre><code>mongosh --host ... # Connect first\nshow dbs # Look for your hyperion_&lt;chain&gt; database\nuse hyperion_&lt;chain&gt;\nshow collections # Check if expected collections (accounts, voters, proposals, &lt;contract&gt;-&lt;table&gt;) exist\n</code></pre></li> <li>Data Not Yet Indexed/Synchronized: If the features were enabled after the indexer processed the relevant historical blocks, the state data might be missing. You need to manually synchronize:<ul> <li>Requirement: The Hyperion Indexer for the target chain must be running and its control port (configured in <code>connections.json</code>, default 7002) must be accessible from where you run the command.</li> <li>Commands (run from Hyperion root dir): <pre><code># Sync specific contract state tables defined in config\n./hyp-control sync contract-state &lt;chain-name&gt;\n\n# Sync system state tables (if enabled in config)\n./hyp-control sync accounts &lt;chain-name&gt;\n./hyp-control sync voters &lt;chain-name&gt;\n./hyp-control sync proposals &lt;chain-name&gt;\n\n# Or sync everything configured\n./hyp-control sync all &lt;chain-name&gt;\n</code></pre></li> <li>This process can take time depending on the amount of data. Monitor indexer logs (<code>pm2 logs &lt;chain&gt;-indexer</code>).</li> </ul> </li> </ol>"},{"location":"providers/help/mongodb/#performance-issues-slow-queriesindexing","title":"Performance Issues (Slow Queries/Indexing)","text":"<p>If <code>/v2/state/</code> queries are slow or the indexer seems bottlenecked during state updates:</p> <ol> <li>Monitor Host Resources: Check RAM, CPU, and especially Disk I/O usage on the MongoDB host using tools like <code>htop</code>, <code>iostat</code>, <code>vmstat</code>. High I/O wait often indicates storage bottlenecks.</li> <li>Check MongoDB Logs: Look for slow query logs in <code>/var/log/mongodb/mongod.log</code>. You might need to configure the profiling level in <code>mongod.conf</code> to capture these.</li> <li>Verify Indexes: Proper indexes are crucial for query performance.<ul> <li>Use <code>mongosh</code> to check existing indexes:     <pre><code>use hyperion_&lt;chain&gt;\ndb.accounts.getIndexes() # Check indexes for 'accounts' collection\ndb.voters.getIndexes()   # Check indexes for 'voters' collection\ndb.&lt;contract&gt;-&lt;table&gt;.getIndexes() # Check indexes for a contract state table\n</code></pre></li> <li>Ensure indexes exist for fields commonly used in your queries (especially for <code>get_table_rows</code> filters).</li> <li>Use <code>./hyp-config contracts ...</code> to manage indexes for <code>contract_state</code> tables (including setting <code>autoIndex: true</code>).</li> <li>The <code>hyp-control sync</code> tools generally create necessary indexes for the system tables (<code>accounts</code>, <code>voters</code>, <code>proposals</code>).</li> </ul> </li> <li>Hardware Resources: Slow performance might simply indicate insufficient RAM, CPU, or slow disk speed for the workload, especially if indexing large contract state tables.</li> </ol>"},{"location":"providers/help/mongodb/#disk-space-issues","title":"Disk Space Issues","text":"<ol> <li>Check Disk Usage: Use <code>df -h</code> on the MongoDB host to check available disk space.</li> <li>Check Data Directory Size: Find the MongoDB data directory path in <code>/etc/mongod.conf</code> (usually <code>/var/lib/mongodb</code>) and check its size: <code>sudo du -sh /var/lib/mongodb</code>.</li> <li>Resolution: If disk space is low, you may need to add more storage, prune unnecessary data (if applicable, less common for state DBs than time-series ES data), or investigate specific collections that are consuming excessive space.</li> </ol>"},{"location":"providers/help/mongodb/#general-tips","title":"General Tips","text":"<ul> <li>Always check the Hyperion Indexer logs (<code>pm2 logs &lt;chain&gt;-indexer</code>) for specific errors related to MongoDB operations (bulk writes, etc.).</li> <li>Consult the official MongoDB Documentation for detailed information on configuration, performance tuning, and troubleshooting.</li> </ul> <p> Hyperion Configuration</p>"},{"location":"providers/help/rabbit/","title":"RabbitMQ","text":""},{"location":"providers/help/rabbit/#how-to-delete-all-the-queues-from-rabbitmq","title":"How to Delete all the queues from RabbitMQ?","text":"<p>Warning</p> <p>Be sure you really want to do this! This gonna reset all your rabbitMQ configuration, and return it to the default state.</p>"},{"location":"providers/help/rabbit/#1-using-policies-management-console","title":"1. Using Policies - Management Console","text":"<ul> <li>Go to Management Console at http://localhost:15672</li> <li>Click on Admin tab</li> <li>Policies tab (on the right side)</li> <li>Add Policy</li> <li>Fill Fields<ul> <li>Virtual Host: Select (Default is <code>/hyperion</code>)</li> <li>Name: Expire All Policies(Delete Later)</li> <li>Pattern: .*</li> <li>Apply to: Queues</li> <li>Definition: expires with value 1 (change type from String to Number)</li> </ul> </li> <li>Click on <code>Add / update policy</code></li> <li>Checkout Queues tab again, all queues must be deleted.</li> </ul> <p>Warning</p> <p>You must remove this policy after this operation.</p>"},{"location":"providers/help/rabbit/#2-using-command-line","title":"2. Using command line","text":"<p>First, list your queues: <pre><code>rabbitmqadmin list queues name \n</code></pre> Then from the list, you'll need to manually delete them one by one: <pre><code>rabbitmqadmin delete queue name='queuename' \n</code></pre> Because of the output format, doesn't appear you can grep the response from <code>list queues</code>. </p> <p>Alternatively, if you're just looking for a way to clear everything, use: <pre><code>rabbitmqctl stop_app\nrabbitmqctl reset\nrabbitmqctl start_app\n</code></pre></p> <p></p>"},{"location":"providers/install/auto_install/","title":"Automated Installation Script","text":"<p>This installation script is maintained here</p> <p>Tip</p> <p>Learning about the software components of the Hyperion architecture is recommended. This automatic setup will use defaults that are not suitable for all scenarios.</p> <p>Already have some dependencies installed?</p> <p>The usage of this script is recommended only for fresh installations.  If you already have some dependencies installed, please proceed with the manual setup</p> <p>WSL2</p> <p>For Windows installation using WSL2, refer to this guide</p>"},{"location":"providers/install/auto_install/#1-create-a-directory-for-the-installer-files","title":"1. Create a directory for the installer files","text":"<pre><code>mkdir -p ~/.hyperion-installer &amp;&amp; cd ~/.hyperion-installer\n</code></pre>"},{"location":"providers/install/auto_install/#2-unpack-the-latest-installer","title":"2. Unpack the latest installer","text":"<pre><code>wget -qO- https://github.com/eosrio/hyperion-auto-setup/raw/main/install.tar.gz | tar -xvz\n</code></pre>"},{"location":"providers/install/auto_install/#3-install-by-running","title":"3. Install by running Elasticsearch (<code>setup-elasticsearch.sh</code>) MongoDB (<code>setup-mongodb.sh</code>) Redis (<code>setup-redis.sh</code>)","text":"<pre><code>./install.sh\n</code></pre> <p>Info</p> <p>The installation script may ask you for the sudo password.</p> What Gets Installed <p>The installation script automatically sets up the following components:</p> <p>1. System Tools (<code>setup-tools.sh</code>) <ul> <li>jq: JSON processor for configuration parsing</li> <li>curl: HTTP client for downloading packages</li> <li>git: Version control system</li> <li>unzip: Archive extraction utility</li> <li>gnupg: GPG tools for package verification</li> <li>lsb-release: Linux distribution information</li> <li>net-tools: Network utilities</li> <li>apt-transport-https: HTTPS transport for APT</li> </ul> <p>2. Node.js Environment (<code>setup-nodejs.sh</code>) <ul> <li>FNM (Fast Node Manager): Node.js version manager</li> <li>Node.js 24.x: Latest LTS version (or 22.16+ if compatible)</li> <li>npm: Node.js package manager</li> <li>Configures shell environment for FNM</li> </ul> <p> 3. Process Manager (<code>setup-pm2.sh</code>) <ul> <li>PM2: Production process manager for Node.js applications</li> <li>Configures PM2 startup service for automatic application restart</li> <li>Sets up system-level PM2 daemon</li> </ul> <p> 4. Database Systems</p> <p> <ul> <li>Elasticsearch 9.x: Search and analytics engine (accepts v8/v9)</li> <li>Configures security settings and certificates</li> <li>Generates elastic user password (saved to <code>elastic.pass</code>)</li> <li>Enables and starts Elasticsearch service</li> </ul> <p> <ul> <li>MongoDB 8.x: Document database for configuration and metadata</li> <li>Adds official MongoDB APT repository</li> <li>Enables and starts MongoDB service</li> <li>Creates version tracking file</li> </ul> <p> <ul> <li>Redis 8.x: In-memory data store for caching (accepts v7+)</li> <li>Adds official Redis APT repository</li> <li>Enables and starts Redis service</li> <li>Creates version tracking file</li> </ul> <p> 5. Message Queue (<code>setup-rabbitmq.sh</code>) <ul> <li>RabbitMQ: Message broker for distributed processing</li> <li>Erlang: Required runtime environment</li> <li>Enables RabbitMQ management plugin</li> <li>Creates dedicated vhost and user:<ul> <li>VHost: <code>hyperion</code></li> <li>User: <code>hyperion_user</code></li> <li>Password: <code>hyperion_password</code></li> </ul> </li> <li>Configures proper permissions and administrator access</li> </ul> <p> 6. Hyperion History API (<code>setup-hyperion.sh</code>) <ul> <li>Clones the official Hyperion repository to <code>~/hyperion</code></li> <li>Installs Node.js dependencies via npm</li> <li>Ready for configuration and deployment</li> </ul>"},{"location":"providers/install/auto_install/#4-post-installation-steps","title":"4. Post-Installation Steps","text":"<p>After successful installation:</p> <ol> <li> <p>Navigate to Hyperion directory:    <pre><code>cd ~/hyperion\n</code></pre></p> </li> <li> <p>Configure Hyperion: Follow the official configuration documentation for detailed setup instructions</p> </li> <li> <p>Start Services: Ensure all services are running:    <pre><code>sudo systemctl status elasticsearch\nsudo systemctl status mongod\nsudo systemctl status redis-server\nsudo systemctl status rabbitmq-server\n</code></pre></p> </li> <li> <p>Access Credentials:</p> <ul> <li>Elasticsearch password: stored in <code>~/hyperion-installer/elastic.pass</code></li> <li>RabbitMQ: <code>hyperion_user</code> / <code>hyperion_password</code></li> </ul> </li> </ol> Elasticsearch password <p>The elastic account password will be saved on the <code>~/.hyperion-installer/elastic.pass</code> file, please save this on a safe location, as you might need it later on. If you need to reset this password you can do it with the following command:   <pre><code>sudo /usr/share/elasticsearch/bin/elasticsearch-reset-password -u elastic -a -s -b\n</code></pre></p> RabbitMQ Management UI <p>RabbitMQ Management UI will be available on port 15672</p> <ul> <li>user: <code>hyperion_user</code></li> <li>password: <code>hyperion_password</code></li> <li>vhost: <code>hyperion</code></li> <li>changing your credentials is recommended, specially if opening access to the management interface is planned</li> </ul>"},{"location":"providers/install/auto_install/#5-troubleshooting","title":"5. Troubleshooting","text":""},{"location":"providers/install/auto_install/#common-issues","title":"Common Issues","text":"<ul> <li>Unsupported OS: Only Ubuntu 22.04 and 24.04 are supported</li> <li>Insufficient Permissions: Ensure your user has sudo access</li> <li>Network Issues: Check internet connectivity for package downloads</li> <li>Port Conflicts: Default ports used:<ul> <li>Elasticsearch: 9200, 9300</li> <li>MongoDB: 27017</li> <li>Redis: 6379</li> <li>RabbitMQ: 5672, 15672</li> </ul> </li> </ul>"},{"location":"providers/install/auto_install/#logs-and-diagnostics","title":"Logs and Diagnostics","text":"<ul> <li>Service logs: <code>sudo journalctl -u &lt;service-name&gt;</code></li> <li>Elasticsearch logs: <code>/var/log/elasticsearch/</code></li> <li>MongoDB logs: <code>/var/log/mongodb/</code></li> <li>RabbitMQ logs: <code>/var/log/rabbitmq/</code></li> </ul>"},{"location":"providers/install/auto_install/#6-proceed-with-the-configuration","title":"6. Proceed with the configuration","text":"<p>Hyperion Configuration </p> <p></p>"},{"location":"providers/install/docker/","title":"Hyperion Docker","text":"<ul> <li>Hyperion Docker Repository</li> </ul> <p>Warning</p> <p>Hyperion Docker is not recommended for production environments, only for testing, debugging and local networks.</p> <p>Hyperion Docker is a multi-container Docker application designed to install and run Hyperion as quickly as possible. It will index data from a development chain where you can define your contracts, execute some actions, and observe the behavior when querying the Hyperion API.</p> <p>Recommended Operating System</p> <p>Ubuntu 24.04</p>"},{"location":"providers/install/docker/#architecture","title":"Architecture","text":""},{"location":"providers/install/docker/#layers","title":"Layers","text":"<p>To simplify, we divide the microservices involved with Hyperion into layers.</p> <ol> <li>Blockchain (Leap with state-history plugin)</li> <li>Hyperion (API/Indexer)</li> <li>Infrastructure (Redis, RabbitMQ, MongoDB, RedisCommander)</li> <li>Elasticsearch/Kibana (New mandatory external layer)</li> </ol> <p>The first layer would be the Blockchain itself - <code>Node Service</code>. For Hyperion to work, we need a blockchain to consume data from. In this layer, we have a single microservice:</p> <p>Leap Node</p> <p>Local blockchain for data consumption</p> <p>The second layer would be Hyperion itself, which is divided into 2 microservices:</p> <p>Hyperion API</p> <p>This service allows interaction with the indexed data.</p> <p>Hyperion Indexer</p> <p>As the name suggests, this service connects to the blockchain to fetch and index data.</p> <p>The third layer, which we can understand as <code>Infrastructure Services</code>, includes 3 main microservices:</p> <ol> <li>Redis</li> <li>RabbitMQ</li> <li>MongoDB</li> </ol> <p>The fourth layer (new) is composed of Elasticsearch and Kibana, which are installed separately using the official Elastic method. This change allows for greater flexibility and alignment with best practices recommended by Elastic.</p> Infrastructure Diagram <p></p> <p>Considering this structure, the Project Repository contains 3 folders representing the first three layers mentioned:</p> <ul> <li>hyperion</li> <li>infra</li> <li>nodeos</li> </ul> <p>In each directory mentioned above, we added a <code>docker-compose.yaml</code> file responsible for starting their respective microservices.</p> <p>For those who have never used <code>docker-compose</code>, it allows the creation of multiple containers simultaneously. These containers are declared as services.</p> Example <code>docker-compose.yaml</code> <p></p> <p>All services (containers) declared within a <code>docker-compose.yaml</code> share the same network by default. Since we separated the containers into different files, we need to create a network in Docker that will be shared. The procedure will be detailed below in the configuration process.</p>"},{"location":"providers/install/docker/#getting-started","title":"Getting Started","text":""},{"location":"providers/install/docker/#prerequisites","title":"Prerequisites","text":"<p>Ensure that Docker and Docker Compose are installed on your system.</p>"},{"location":"providers/install/docker/#installing-elasticsearch-and-kibana","title":"Installing Elasticsearch and Kibana","text":"<p>Hyperion depends on Elasticsearch for indexing and searching data. Starting with this version, we recommend that the installation and management of Elasticsearch and Kibana be done using the official Elastic method, ensuring greater flexibility and ease of maintenance.</p> <p>Execute the command below to install locally:</p> <pre><code>curl -fsSL https://elastic.co/start-local | sed 's/xpack.license.self_generated.type=trial/xpack.license.self_generated.type=basic/g' | sh\n</code></pre> <p>This command will create a folder called <code>elastic-start-local</code> with all the necessary files, including:</p> <ul> <li>Management scripts (<code>start.sh</code>, <code>stop.sh</code>, <code>uninstall.sh</code>)</li> <li><code>.env</code> file with access credentials</li> <li>Elastic's <code>docker-compose.yml</code> file</li> </ul> <p>After installation, start the Elasticsearch and Kibana services:</p> <pre><code>cd elastic-start-local\n./start.sh\n</code></pre> <p>You can verify that the services are working by accessing:</p> <ul> <li>Elasticsearch: http://localhost:9200</li> <li>Kibana: http://localhost:5601</li> </ul> <p>Access credentials will be available in the <code>.env</code> file. You will need them later to configure Hyperion.</p> <p>Tip</p> <p>To view the credentials in the <code>.env</code> file, you can use: <pre><code>cat elastic-start-local/.env\n</code></pre></p> <p>Production Environments</p> <p>For deployments in production environments, consult the official Elastic documentation for advanced configurations: Elastic Docs - Self-Managed</p>"},{"location":"providers/install/docker/#infrastructure-layer","title":"Infrastructure Layer","text":""},{"location":"providers/install/docker/#1-clone-the-repository","title":"1. Clone the repository","text":"<p>Clone the repository to your local machine in the same parent directory where elastic-start-local folder was created:</p> <pre><code># Make sure you're in the same directory where elastic-start-local was created\ncd ..  # If you're currently in the elastic-start-local directory\ngit clone https://github.com/eosrio/hyperion-docker.git\ncd hyperion-docker\n</code></pre> <p>Directory Structure</p> <p>Your directory structure should look like this: ```</p> <p>/your_parent_directory/     \u251c\u2500\u2500 elastic-start-local/   # Elasticsearch installation     \u2514\u2500\u2500 hyperion-docker/       # Hyperion Docker repository     <code>``     This structure is required for the</code>start.sh` script to work correctly.</p>"},{"location":"providers/install/docker/#2-verify-docker-is-running","title":"2. Verify Docker is running","text":"<p>Make sure Docker is running by executing the following command in the terminal:</p> <pre><code>docker ps\n</code></pre> <p>Expected result</p> <p></p>"},{"location":"providers/install/docker/#3-create-the-shared-network","title":"3. Create the shared network","text":"<p>Create a network that will be shared between containers by executing the command:</p> <pre><code>docker network create hyperion\n</code></pre> <p>Expected result</p> <p></p>"},{"location":"providers/install/docker/#4-create-the-microservices","title":"4. Create the microservices","text":"<p>Now, let's start creating the microservices of the infrastructure layer.</p> <p>Navigate to the infra directory of the repository and execute the following command:</p> <pre><code>cd infra\ndocker compose up -d\n</code></pre> <p>Flag <code>-d</code></p> <p>Note that we use the <code>-d</code> flag to run in detached mode, allowing us to continue using the command line session.</p> <p>This command will create the microservices (Redis, RabbitMQ, MongoDB) needed for Hyperion to work. Note that Elasticsearch and Kibana are no longer included in this layer, as they are now managed separately.</p> <p>The first time you run the command, it may take some time for everything to be configured. You can follow the execution log using the command:</p> <pre><code>docker compose logs -f\n</code></pre> <p>Press Ctrl+C to terminate the log reading process.</p>"},{"location":"providers/install/docker/#5-verify-the-services","title":"5. Verify the services","text":"<p>Check if the services are working:</p> <ul> <li>RabbitMQ - http://localhost:15672/</li> </ul> <p>After completing the Infrastructure Layer configuration, we can proceed to the Leap Layer (nodeos).</p>"},{"location":"providers/install/docker/#leap-layer-nodeos","title":"Leap Layer (nodeos)","text":"<p>Navigate to the nodeos directory in the repository and execute:</p> <pre><code>cd ../nodeos\ndocker compose up -d\n</code></pre> <p>This layer was added to the repository assuming that you don't have a configured blockchain from which the Hyperion Indexer will consume data.</p> <p>After the infrastructure and blockchain node are configured, we can finally start Hyperion.</p>"},{"location":"providers/install/docker/#hyperion-layer","title":"Hyperion Layer","text":"<p>This layer has 2 microservices, Hyperion API and Hyperion Indexer.</p>"},{"location":"providers/install/docker/#starting-hyperion-services","title":"Starting Hyperion services","text":"<p>To start the Hyperion services, we've created a convenient startup script that automatically handles the Elasticsearch password configuration. Navigate to the hyperion directory and execute the following command:</p> <pre><code>cd ../hyperion\n./start.sh\n</code></pre> <p>The <code>start.sh</code> script performs the following operations:</p> <ol> <li>Retrieves Elasticsearch password: Automatically reads the password from the <code>.env</code> file generated during the Elasticsearch installation (using the relative path <code>../../elastic-start-local/.env</code>)</li> <li>Updates configuration: Replaces the password placeholder in the <code>connections.json</code> file with the actual password</li> <li>Starts services: Executes <code>docker compose up -d</code> to start the Hyperion services</li> </ol> <p>Directory Structure Reminder</p> <p>This automatic configuration depends on the directory structure mentioned earlier, with both <code>elastic-start-local</code> and <code>hyperion-docker</code> directories at the same level.</p> <p>This automated process ensures that Hyperion is properly configured to communicate with the Elasticsearch instance without manual intervention.</p> <p>Alternative Manual Configuration</p> <p>If you prefer to configure manually, you can edit the <code>connections.json</code> file in the <code>config</code> directory and replace <code>\"ELASTIC_PASSWORD\"</code> with the password found in the <code>elastic-start-local/.env</code> file, then run <code>docker compose up -d</code>.</p>"},{"location":"providers/install/docker/#troubleshooting","title":"Troubleshooting","text":""},{"location":"providers/install/docker/#configuring-connectionsjson","title":"Configuring <code>connections.json</code>","text":"<p>The <code>connections.json</code> file is crucial for the proper functioning of Hyperion as it defines how Hyperion connects to all required services. If you encounter connection issues, or if you're using a custom infrastructure setup, you may need to adjust the host configurations in this file.</p> <p>Host Configuration</p> <p>The default configuration assumes that:</p> <ul> <li>RabbitMQ and Redis are running as Docker services named \"rabbitmq\" and \"redis\" respectively</li> <li>Elasticsearch and MongoDB are accessible via <code>host.docker.internal</code> (which resolves to the host machine from inside Docker containers)</li> </ul> <p>Here's how to modify the configuration for different scenarios:</p>"},{"location":"providers/install/docker/#using-services-outside-docker-or-with-different-names","title":"Using services outside Docker or with different names","text":"<pre><code>{\n  \"amqp\": {\n    \"host\": \"your-rabbitmq-host:5672\",  // Change if RabbitMQ is not running as \"rabbitmq\" service\n    // ...other AMQP settings\n  },\n  \"elasticsearch\": {\n    \"host\": \"your-elasticsearch-host:9200\",  // Change if Elasticsearch is at a different location\n    \"ingest_nodes\": [\n      \"your-elasticsearch-host:9200\"\n    ],\n    // ...other Elasticsearch settings\n  },\n  \"redis\": {\n    \"host\": \"your-redis-host\",  // Change if Redis is not running as \"redis\" service\n    \"port\": \"6379\"\n  },\n  \"mongodb\": {\n    \"host\": \"your-mongodb-host\",  // Change if MongoDB is at a different location\n    // ...other MongoDB settings\n  }\n}\n</code></pre>"},{"location":"providers/install/docker/#common-connection-issues-and-solutions","title":"Common connection issues and solutions","text":"<ul> <li>Connection refused errors: Verify that the service is running and that the hostname/port is correct</li> <li>Authentication failures: Ensure that usernames and passwords are correctly set in the configuration</li> <li>Docker networking issues: If services can't reach each other, verify they are on the same Docker network (<code>hyperion</code>)</li> <li>Host resolution issues: If using custom hostnames, ensure they are properly resolved (you may need to add entries to <code>/etc/hosts</code> or use Docker's DNS)</li> </ul> <p>Testing connections</p> <p>You can test connections to each service using appropriate tools: For Elasticsearch: <code>curl -u elastic:your_password http://host:9200</code> * For RabbitMQ: <code>curl -u rabbitmq:rabbitmq http://host:15672/api/overview</code> For Redis: Use <code>redis-cli -h host -p 6379 ping</code> * For MongoDB: Use <code>mongosh --host host --port 27017</code></p> <p>For issues related to Elasticsearch/Kibana, consult the official Elastic documentation:</p> <ul> <li>Elasticsearch Troubleshooting</li> <li>Kibana Troubleshooting</li> </ul> <p>For issues related to Hyperion:</p> <ul> <li>Check if all layers are working correctly</li> <li>Check if the Elasticsearch credentials are configured correctly in the <code>connections.json</code> file</li> <li>Check the service logs using <code>docker compose logs -f</code></li> </ul>"},{"location":"providers/install/docker/#next-steps","title":"Next steps","text":"<p>Feel free to modify the configurations according to your needs. All configuration files are located in <code>hyperion/config</code> or <code>nodeos/leap/config</code>.</p> <p>For more details, consult the Hyperion Configuration Section .</p>"},{"location":"providers/install/docker/#references-and-useful-links","title":"References and Useful Links","text":"<ul> <li>Official Elasticsearch Documentation</li> <li>Official Kibana Documentation</li> <li>Docker Compose Documentation</li> <li>Hyperion Repository</li> </ul>"},{"location":"providers/install/manual_install/","title":"Manual Installation","text":"<p>This section describes how to manually install Hyperion and its environment. If you want more control of your installation, this is the way to go.</p> <p>Warning</p> <p>If you are running more than one SHIP node, you can now configure the failover option directly in the <code>connections.json</code> file. Please refer to the section detailing the new parameters by clicking here</p> <p>Info</p> <p>Review the guidelines for configuring Hyperion for Provider Registration on QRY, a new decentralized ecosystem that provides access to a variety of data services and APIs. Follow the steps outlined in Config Provider steps</p> <p>Attention</p> <p>Recommended OS: Ubuntu 24.04</p>"},{"location":"providers/install/manual_install/#dependencies","title":"Dependencies","text":"<p>Below you can find the list of all Hyperion's dependencies:</p> <ul> <li>Elasticsearch 8.7+ | Required for historical data indexing.</li> <li>Kibana 8.7+ | Optional, for visualizing Elasticsearch data.</li> <li>RabbitMQ 4.1+ | Required for message queuing between indexer stages.</li> <li>Redis  | Required for caching and inter-process communication.</li> <li>MongoDB Community Server | Conditionally Required. Needed only if enabling state tracking features (<code>features.tables.*</code> or <code>features.contract_state.enabled</code> in chain config). See MongoDB section below for details.</li> <li>Node.js 22+ | Required runtime environment.</li> <li>PM2 | Required process manager for Node.js applications.</li> <li>NODEOS (spring 1.2.0 or leap 5.0.3) | Required Antelope node with State History Plugin (SHIP) enabled.</li> </ul> <p>On the next steps you will install and configure each one of them.</p> <p>Note</p> <p>The Hyperion Indexer requires Node.js and pm2 to be on the same machine. All other dependencies (Elasticsearch, RabbitMQ, Redis and EOSIO) can be installed on different machines, preferably on a high speed and low latency network. Keep in mind that indexing speed will vary greatly depending on this configuration.</p>"},{"location":"providers/install/manual_install/#elasticsearch","title":"Elasticsearch","text":"<p>Follow the detailed installation instructions on the official Elasticsearch documentation and return to this guide before running it.</p> <p>Info</p> <p>Elasticsearch is not started automatically after installation. We recommend running it with systemd.</p> <p>Note</p> <p>It is very important to know the Elasticsearch directory layout and to understand how the configuration works.</p>"},{"location":"providers/install/manual_install/#configuration","title":"Configuration","text":""},{"location":"providers/install/manual_install/#1-elasticsearch-configuration","title":"1. Elasticsearch configuration","text":"<p>Edit the following lines on <code>/etc/elasticsearch/elasticsearch.yml</code>:</p> <pre><code>cluster.name: CLUSTER_NAME\nbootstrap.memory_lock: true\n</code></pre> <p>The memory lock option will prevent any Elasticsearch heap memory from being swapped out.</p> <p>Warning</p> <p>Setting <code>bootstrap.memory_lock: true</code> will make Elasticsearch try to use all the RAM configured for JVM on startup ( check next step). This can cause the application to crash if you allocate more RAM than available.</p> <p>Note</p> <p>A different approach is to disable swapping on your system.</p> <p>Testing</p> <p>After starting Elasticsearch, you can see whether this setting was applied successfully by checking the value of <code>mlockall</code> in the output from this request:</p> <pre><code>curl -X GET \"localhost:9200/_nodes?filter_path=**.mlockall&amp;pretty\"\n</code></pre>"},{"location":"providers/install/manual_install/#2-heap-size-configuration","title":"2. Heap size configuration","text":"<p>For a optimized heap size, check how much RAM can be allocated by the JVM on your system. Run the following command:</p> <pre><code>java -Xms16g -Xmx16g -XX:+UseCompressedOops -XX:+PrintFlagsFinal Oops | grep Oops\n</code></pre> <p>Check if <code>UseCompressedOops</code> is true on the results and change <code>-Xms</code> and <code>-Xmx</code> to the desired value.</p> <p>Note</p> <p>Elasticsearch includes a bundled version of OpenJDK from the JDK maintainers. You can find it on <code>/usr/share/elasticsearch/jdk</code>.</p> <p>After that, change the heap size by editting the following lines on <code>/etc/elasticsearch/jvm.options</code>:</p> <pre><code>-Xms16g\n-Xmx16g\n</code></pre> <p>Note</p> <p>Xms and Xmx must have the same value.</p> <p>Warning</p> <p>Avoid allocating more than 31GB when setting your heap size, even if you have enough RAM.</p>"},{"location":"providers/install/manual_install/#3-allow-memory-lock","title":"3. Allow memory lock","text":"<p>Override systemd configuration by running <code>sudo systemctl edit elasticsearch</code> and add the following lines:</p> <pre><code>[Service]\nLimitMEMLOCK=infinity\n</code></pre> <p>Run the following command to reload units:</p> <pre><code>sudo systemctl daemon-reload\n</code></pre>"},{"location":"providers/install/manual_install/#4-start-elasticsearch","title":"4. Start Elasticsearch","text":"<p>Start Elasticsearch and check the logs:</p> <pre><code>sudo systemctl start elasticsearch.service\nsudo less /var/log/elasticsearch/CLUSTE_NAME.log\n</code></pre> <p>Enable it to run at startup:</p> <pre><code>sudo systemctl enable elasticsearch.service\n</code></pre> <p>And finally, test the REST API:</p> <pre><code>curl -X GET \"localhost:9200/?pretty\"\n</code></pre> <p>Note</p> <p>Don't forget to check if memory lock worked.</p> <p>The expected result should be something like this:</p> <pre><code>{\n  \"name\": \"ip-172-31-5-121\",\n  \"cluster_name\": \"CLUSTER_NAME\",\n  \"cluster_uuid\": \"FFl8DNcOQV-dVk3p1JDNMA\",\n  \"version\": {\n    \"number\": \"7.14.1\",\n    \"build_flavor\": \"default\",\n    \"build_type\": \"deb\",\n    \"build_hash\": \"606a173\",\n    \"build_date\": \"2021-08-26T00:43:15.323135Z\",\n    \"build_snapshot\": false,\n    \"lucene_version\": \"8.9.0\",\n    \"minimum_wire_compatibility_version\": \"6.8.0\",\n    \"minimum_index_compatibility_version\": \"6.0.0-beta1\"\n  },\n  \"tagline\": \"You Know, for Search\"\n}\n</code></pre>"},{"location":"providers/install/manual_install/#5-set-up-minimal-security","title":"5. Set up minimal security","text":"<p>The Elasticsearch security features are disabled by default. To avoid security problems, we recommend enabling the security pack.</p> <p>To do that, add the following line to the end of the <code>/etc/elasticsearch/elasticsearch.yml</code> file:</p> <pre><code>xpack.security.enabled: true\n</code></pre> <p>Restart Elasticsearch and set the passwords for the cluster:</p> <pre><code>sudo systemctl restart elasticsearch.service\nsudo /usr/share/elasticsearch/bin/elasticsearch-setup-passwords auto\n</code></pre> <p>Keep track of these passwords, we\u2019ll need them again soon.</p> <p>Note</p> <p>You can alternatively use the <code>interactive</code> parameter to manually define your passwords.</p> <p>Attention</p> <p>The minimal security scenario is not sufficient for production mode clusters. Check the documentation for more information.</p>"},{"location":"providers/install/manual_install/#kibana","title":"Kibana","text":"<p>Follow the detailed installation instructions on the official Kibana documentation. Return to this documentation before running it.</p> <p>Info</p> <p>Kibana is not started automatically after installation. We recomend running it with systemd.</p> <p>Note</p> <p>Like on Elasticsearch, it is very important to know the Kibana directory layout and to understand how the configuration works.</p>"},{"location":"providers/install/manual_install/#configuration_1","title":"Configuration","text":""},{"location":"providers/install/manual_install/#1-elasticsearch-security","title":"1. Elasticsearch security","text":"<p>If you have enabled the security pack on Elasticsearch, you need to set up the password on Kibana. Edit the folowing lines on the <code>/etc/kibana/kibana.yml</code> file:</p> <pre><code>elasticsearch.username: \"kibana_system\"\nelasticsearch.password: \"password\"\n</code></pre>"},{"location":"providers/install/manual_install/#2-start-kibana","title":"2. Start Kibana","text":"<p>Start Kibana and check the logs:</p> <pre><code>sudo systemctl start kibana.service\nsudo less /var/log/kibana/kibana.log\n</code></pre> <p>Enable it to run at startup:</p> <pre><code>sudo systemctl enable kibana.service\n</code></pre>"},{"location":"providers/install/manual_install/#rabbitmq","title":"RabbitMQ","text":"<p>Attention</p> <p>From Hyperion 3.3.10, RabbitMQ version 3.12+ is required.</p> <p>Follow the detailed installation instructions on the official RabbitMQ documentation.</p> <p>RabbitMQ should automatically start after installation. Check the documentation for more details on how to manage its service.</p>"},{"location":"providers/install/manual_install/#configuration_2","title":"Configuration","text":""},{"location":"providers/install/manual_install/#1-enable-the-webui","title":"1. Enable the WebUI","text":"<pre><code>sudo rabbitmq-plugins enable rabbitmq_management\n</code></pre>"},{"location":"providers/install/manual_install/#2-add-vhost","title":"2. Add vhost","text":"<pre><code>sudo rabbitmqctl add_vhost hyperion\n</code></pre>"},{"location":"providers/install/manual_install/#3-create-a-user-and-password","title":"3. Create a user and password","text":"<pre><code>sudo rabbitmqctl add_user USER PASSWORD\n</code></pre>"},{"location":"providers/install/manual_install/#4-set-the-user-as-administrator","title":"4. Set the user as administrator","text":"<pre><code>sudo rabbitmqctl set_user_tags USER administrator\n</code></pre>"},{"location":"providers/install/manual_install/#5-set-the-user-permissions-to-the-vhost","title":"5. Set the user permissions to the vhost","text":"<pre><code>sudo rabbitmqctl set_permissions -p hyperion USER \".*\" \".*\" \".*\"\n</code></pre>"},{"location":"providers/install/manual_install/#6-check-access-to-the-webui","title":"6. Check access to the WebUI","text":"<p>Try to access RabbitMQ WebUI at http://localhost:15672 with the user and password you just created.</p>"},{"location":"providers/install/manual_install/#redis","title":"Redis","text":"<pre><code>sudo apt install redis-server\n</code></pre> <p>Redis will also start automatically after installation.</p>"},{"location":"providers/install/manual_install/#configuration_3","title":"Configuration","text":""},{"location":"providers/install/manual_install/#1-update-redis-supervision-method","title":"1. Update Redis supervision method","text":"<p>Change the <code>supervised</code> configuration from <code>supervised no</code> to  <code>supervised systemd</code> on <code>/etc/redis/redis.conf</code>.</p> <p>Note</p> <p>By default, Redis binds to the localhost address. You need to edit <code>bind</code> in the config file if you want to listen to other network.</p>"},{"location":"providers/install/manual_install/#2-restart-redis","title":"2. Restart Redis","text":"<pre><code>sudo systemctl restart redis.service\n</code></pre>"},{"location":"providers/install/manual_install/#mongodb-conditional-requirement","title":"MongoDB (Conditional Requirement)","text":"<p>Requirement Check</p> <p>MongoDB is only required if you plan to enable specific state-tracking features in your Hyperion chain configuration (<code>config/chains/&lt;chain&gt;.config.json</code>). These features include:</p> <ul> <li>System Tables: Setting <code>features.tables.proposals</code>, <code>features.tables.accounts</code>, or <code>features.tables.voters</code> to <code>true</code>.</li> <li>Contract State: Setting <code>features.contract_state.enabled</code> to <code>true</code>.</li> </ul> <p>If you do not enable these features, you can skip the MongoDB installation. Hyperion will rely solely on Elasticsearch for historical action/delta data.</p> <p>MongoDB is used by Hyperion to store the current state of specific on-chain data (like proposals, voter info, token balances, and configured contract tables), enabling fast lookups via the <code>/v2/state/*</code> API endpoints.</p>"},{"location":"providers/install/manual_install/#installation-ubuntu-example","title":"Installation (Ubuntu Example)","text":"<p>Follow the official MongoDB Community Edition installation guide for your specific Ubuntu version: Install MongoDB Community Edition on Ubuntu</p> <p>The general steps involve:</p> <ol> <li>Importing the MongoDB public GPG Key.</li> <li>Creating a list file for MongoDB.</li> <li>Reloading the local package database (<code>sudo apt-get update</code>).</li> <li>Installing the MongoDB packages (<code>sudo apt-get install -y mongodb-org</code>).</li> </ol>"},{"location":"providers/install/manual_install/#configuration-service-management","title":"Configuration &amp; Service Management","text":""},{"location":"providers/install/manual_install/#1-start-mongodb","title":"1. Start MongoDB","text":"<p>After installation, start the MongoDB service:</p> <pre><code>sudo systemctl start mongod\n</code></pre> <p>Check if the service started successfully:</p> <pre><code>sudo systemctl status mongod\n</code></pre> <p>Look for <code>active (running)</code> in the output. You can also try connecting using the MongoDB Shell: <code>mongosh</code>.</p> <p>To ensure MongoDB starts automatically on system boot:</p> <pre><code>sudo systemctl enable mongod\n</code></pre> <p>For production environments, it is highly recommended to enable access control and configure authentication for MongoDB. This typically involves creating an administrative user and updating the configuration file (<code>/etc/mongod.conf</code>) to enforce authentication. The basic installation often allows unauthenticated access from localhost.</p> <p>Hyperion connection details for MongoDB (host, port, user, password, database prefix) will be configured later in the <code>config/connections.json</code> file using the <code>./hyp-config connections init</code> tool or by manual editing.</p>"},{"location":"providers/install/manual_install/#nodejs","title":"NodeJS","text":"<pre><code># installs fnm (Fast Node Manager)\ncurl -fsSL https://fnm.vercel.app/install | bash\n\n# activate fnm\nsource ~/.bashrc\n\n# download and install Node.js\nfnm use --install-if-missing 22\n\n# verifies the right Node.js version is in the environment\nnode -v # should print `v22.11.0`\n\n# verifies the right npm version is in the environment\nnpm -v # should print `10.9.0`\n</code></pre>"},{"location":"providers/install/manual_install/#pm2","title":"PM2","text":"<pre><code>npm install pm2@latest -g\n</code></pre>"},{"location":"providers/install/manual_install/#configuration_4","title":"Configuration","text":""},{"location":"providers/install/manual_install/#1-configure-for-system-startup","title":"1. Configure for system startup","text":"<pre><code>pm2 startup\n</code></pre>"},{"location":"providers/install/manual_install/#antelope-nodeos","title":"Antelope Nodeos","text":""},{"location":"providers/install/manual_install/#antelopeio-spring","title":"AntelopeIO Spring","text":"<p>Get the latest version from the repository</p> <pre><code>wget https://github.com/AntelopeIO/spring/releases/download/v1.2.0/antelope-spring_1.2.0_amd64.deb\nsudo apt install ./antelope-spring_1.2.0_amd64.deb\n</code></pre>"},{"location":"providers/install/manual_install/#leap","title":"Leap","text":"<pre><code>wget https://github.com/AntelopeIO/leap/releases/download/v5.0.3/leap_5.0.3_amd64.deb\nsudo apt install ./leap_5.0.3_amd64.deb\n</code></pre>"},{"location":"providers/install/manual_install/#configuration_5","title":"Configuration","text":"<p>Add the following configuration to the <code>config.ini</code> file:</p> <pre><code>state-history-dir = \"state-history\"\ntrace-history = true\nchain-state-history = true\nstate-history-endpoint = 127.0.0.1:8080\nplugin = eosio::chain_api_plugin\nplugin = eosio::state_history_plugin\n</code></pre>"},{"location":"providers/install/manual_install/#hyperion","title":"Hyperion","text":"<p>If everything runs smoothly, it's time to install Hyperion! </p> <p>To do that, simply run the following commands:</p> <pre><code>git clone https://github.com/eosrio/hyperion-history-api.git\ncd hyperion-history-api\nnpm ci\n</code></pre>"},{"location":"providers/install/manual_install/#proceed-with-hyperion-configuration","title":"Proceed with Hyperion Configuration","text":"<p>Hyperion Setup </p>"},{"location":"providers/install/wsl2/","title":"WSL2 + systemd","text":"<p>Guide on enabling systemd if you are using WSL2 on Windows.</p> <p>Note</p> <p>Optimizing for performance is beyond the scope of this guide, this is intended for learning and development on Hyperion. systemd is now available in WSL2 and required for this guide. Read more</p>"},{"location":"providers/install/wsl2/#1-make-sure-wsl2-is-updated","title":"1. Make sure WSL2 is updated","text":"<p>Windows Terminal</p> <pre><code>wsl --update\n</code></pre>"},{"location":"providers/install/wsl2/#2-install-and-launch-an-ubuntu-24041-instance","title":"2. Install and launch an Ubuntu 24.04.1 instance","text":"<p>Ubuntu 24.04.1 LTS on Microsoft Store</p>"},{"location":"providers/install/wsl2/#3-enable-systemd","title":"3. Enable systemd","text":"<p>The infrastructure requires <code>systemd</code> to be enabled</p> <p>Linux Terminal</p> <p>As root edit the file /etc/wsl.conf <pre><code>sudo nano /etc/wsl.conf\n</code></pre> Add the following lines <pre><code>[boot]\nsystemd=true\n</code></pre></p>"},{"location":"providers/install/wsl2/#4-restart-the-wsl2-engine","title":"4. Restart the WSL2 engine","text":"<p>Windows Terminal</p> <p>Shutdown all instances <pre><code>wsl --shutdown\n</code></pre> Start your instance <pre><code>wsl -d Ubuntu-22.04\n</code></pre></p>"},{"location":"providers/install/wsl2/#5-proceed-with-the-installation","title":"5. Proceed with the installation","text":"<p>Automatic Installation Script </p> <p>Manual Installation </p>"},{"location":"providers/plugins/explorer/","title":"Hyperion Block Explorer","text":"<ul> <li>Hyperion Explorer Repository</li> </ul> <p>Hyperion lightweight Explorer is a user-friendly block explorer focused on speed and reliability.</p> <ul> <li>Requires Hyperion v3.5.2 or above</li> </ul>"},{"location":"providers/plugins/explorer/#deployment-instructions","title":"Deployment Instructions","text":"<pre><code># Clone the repository inside the hyperion-history-api folder\ncd hyperion-history-api\n\ngit clone https://github.com/eosrio/hyperion-explorer.git explorer\ncd explorer\n\nnpm install\nnpm run build\n\n# Start the server with node directly\nnpm run serve:ssr:hyperion-explorer\n\n# Start the server with pm2 (recommended for production)\npm2 start\n</code></pre>"},{"location":"providers/plugins/explorer/#per-chain-configuration","title":"Per Chain Configuration","text":"<pre><code># edit the config file at hyperion-history-api/config/chains/&lt;chain&gt;.config.json\n# under api section add\n\"explorer\": {\n      \"upstream\": \"http://127.0.0.1:4777\",\n      \"theme\": \"default\"\n}\n# custom theme can be added to explorer/themes using the pattern &lt;name&gt;.theme.mjs\n</code></pre>"},{"location":"providers/setup/chain/","title":"Chain Config Reference","text":"<p> Hyperion Configuration</p>"},{"location":"providers/setup/chain/#chain-configuration-reference-configchainschainconfigjson","title":"Chain Configuration Reference <code>config/chains/&lt;chain&gt;.config.json</code>","text":"<p>This file contains all the specific settings for one particular blockchain that Hyperion will index. You will typically create and manage the basic structure of this file using the <code>./hyp-config chains new ...</code> command, but you must review and customize this file to tailor Hyperion's behavior, performance, and features for your specific needs and the characteristics of the chain being indexed.</p> <p>Review Required</p> <p>Review each section below and adjust parameters according to your hardware resources, chain traffic, and desired features.</p> <p>The configuration is divided into several main sections:</p>"},{"location":"providers/setup/chain/#api-server-configuration","title":"API Server Configuration","text":"<p>Settings related to the Hyperion HTTP API server instances for this chain.</p> <p><code>api:</code></p> <ul> <li><code>enabled</code>: (<code>true</code>|<code>false</code>) - Whether to launch API server instances for this chain via PM2. Default: <code>true</code>.</li> <li><code>pm2_scaling</code>: (integer) - Number of API server instances to launch in cluster mode via PM2. Adjust based on expected load and CPU cores. Default: <code>1</code>.</li> <li><code>chain_name</code>: (string) - Required. Display name for the chain used in API responses and documentation (e.g., \"WAX Mainnet\").</li> <li><code>server_addr</code>: (string) - Required. IP address the API server should bind to (e.g., <code>\"127.0.0.1\"</code> for local only, <code>\"0.0.0.0\"</code> for all interfaces). Default: <code>\"127.0.0.1\"</code>.</li> <li><code>server_port</code>: (integer) - Required. Port number for the HTTP API server. Default: <code>7000</code>.</li> <li><code>stream_port</code>: (integer) - Required. Port number for the WebSocket streaming server (if <code>features.streaming.enable</code> is true). Default: <code>1234</code>.</li> <li><code>stream_scroll_limit</code>: (integer) - Maximum number of historical documents (actions/deltas) allowed in a single streaming request scroll query. <code>-1</code> disables the limit. Default: <code>-1</code>.</li> <li><code>stream_scroll_batch</code>: (integer) - Number of documents to fetch per Elasticsearch scroll request during historical streaming. Default: <code>500</code>.</li> <li><code>server_name</code>: (string) - Required. Publicly accessible domain name and port for this API (e.g., <code>\"wax.hyperion.example.com:443\"</code>). Used in documentation links. Default: <code>\"127.0.0.1:7000\"</code>.</li> <li><code>provider_name</code>: (string) - Required. Name of the entity providing this Hyperion instance. Used in documentation. Default: <code>\"Example Provider\"</code>.</li> <li><code>provider_url</code>: (string) - Required. URL associated with the provider. Used in documentation. Default: <code>\"https://example.com\"</code>.</li> <li><code>provider_logo</code>: (string) - URL to a logo image for the provider. Used in documentation.</li> <li><code>chain_logo_url</code>: (string) - Required. URL to a logo image for the chain. Used in documentation.</li> <li><code>chain_api</code>: (string) - Override the default Nodeos HTTP endpoint (defined in <code>connections.json</code>) specifically for <code>/v1/chain</code> requests handled by this API instance. Leave empty (<code>\"\"</code>) to use the default.</li> <li><code>push_api</code>: (string) - Define a separate Nodeos endpoint specifically for handling <code>push_transaction</code> requests. Useful for directing write operations to dedicated nodes. Leave empty (<code>\"\"</code>) to use the <code>chain_api</code> or the default from <code>connections.json</code>.</li> <li><code>explorer</code>: (object) - Configuration for the block explorer API.<ul> <li><code>home_redirect</code>: (<code>true</code>|<code>false</code>) - Redirect the API root <code>/</code> to <code>/explorer</code>. Default: <code>false</code>.</li> <li><code>upstream</code>: (string) - Required URL to Hyperion explorer server.</li> <li><code>theme</code>: (string) - explorer theme for this chain (e.g., <code>vaulta</code>) Check the themes folder on the explorer repository.</li> <li><code>oracle</code>: (object) - Oracle data configuration for displaying price information.<ul> <li><code>custom_mode</code>: (string) - Custom oracle mode configuration.</li> <li><code>pair</code>: (string) - Trading pair to display (e.g., \"WAXUSDT\").</li> <li><code>contract</code>: (string) - Oracle contract name.</li> <li><code>table</code>: (string) - Oracle data table name.</li> <li><code>path</code>: (string) - Data path within oracle results.</li> <li><code>type</code>: (string) - Oracle data type.</li> <li><code>factor</code>: (number) - Multiplication factor for price display.</li> </ul> </li> </ul> </li> <li><code>enable_caching</code>: (<code>true</code>|<code>false</code>) - Required. Enable Redis caching for API responses. Default: <code>true</code>.</li> <li><code>cache_life</code>: (integer) - Required. Default cache expiration time in seconds. Default: <code>1</code>.</li> <li> <p><code>limits</code>: (object) - Required. Maximum number of results returned per page for various API endpoints. Helps prevent excessively large responses.</p> <ul> <li><code>get_table_rows</code>: (integer) - Limit for table row queries.</li> <li><code>get_top_holders</code>: (integer) - Limit for top token holders queries.</li> <li><code>get_links</code>: (integer) - Limit for permission link queries.</li> <li><code>get_actions</code>: (integer) - Limit for action history queries.</li> <li><code>get_blocks</code>: (integer) - Limit for block queries.</li> <li><code>get_created_accounts</code>: (integer) - Limit for created accounts queries.</li> <li><code>get_deltas</code>: (integer) - Limit for state delta queries.</li> <li><code>get_key_accounts</code>: (integer) - Limit for key-to-account queries.</li> <li><code>get_proposals</code>: (integer) - Limit for proposal queries.</li> <li><code>get_tokens</code>: (integer) - Limit for token queries.</li> <li><code>get_transfers</code>: (integer) - Limit for transfer queries.</li> <li><code>get_voters</code>: (integer) - Limit for voter queries.</li> <li><code>get_trx_actions</code>: (integer) - Limit for transaction action queries.</li> </ul> </li> <li> <p><code>access_log</code>: (<code>true</code>|<code>false</code>) - Required. Enable logging of API requests to <code>logs/&lt;chain&gt;/api.access.log</code>. Default: <code>false</code>.</p> </li> <li><code>chain_api_error_log</code>: (<code>true</code>|<code>false</code>) - Log errors received when proxying requests to the Nodeos <code>chain_api</code>. Default: <code>false</code>.</li> <li><code>log_errors</code>: (<code>true</code>|<code>false</code>) - Log internal API server errors. Default: <code>false</code>.</li> <li><code>custom_core_token</code>: (string) - Override the default core token symbol (e.g., \"EOS\", \"WAX\") used for display purposes (e.g., legacy key conversion).</li> <li><code>disable_rate_limit</code>: (<code>true</code>|<code>false</code>) - Disable API rate limiting. Default: <code>false</code>.</li> <li><code>rate_limit_rpm</code>: (integer) - Maximum requests per minute per IP address. Default: <code>1000</code>.</li> <li><code>rate_limit_allow</code>: (array) - List of IP addresses exempt from rate limiting.</li> <li><code>disable_tx_cache</code>: (<code>true</code>|<code>false</code>) - Disable the Redis cache specifically for <code>get_transaction</code> lookups. Default: <code>false</code>.</li> <li><code>tx_cache_expiration_sec</code>: (integer) - Expiration time in seconds for transaction details cached in Redis. Default: <code>3600</code> (1 hour).</li> <li><code>v1_chain_cache</code>: (array) - Configure specific caching TTLs (time-to-live in milliseconds) for proxied <code>/v1/chain</code> endpoints.<ul> <li><code>path</code>: (string) - The endpoint path (e.g., <code>\"get_block\"</code>).</li> <li><code>ttl</code>: (integer) - Cache duration in milliseconds.</li> </ul> </li> <li><code>node_max_old_space_size</code>: (integer) - Max heap size (in MB) for the Node.js API server process (<code>--max-old-space-size</code>). Default: <code>1024</code>.</li> <li><code>node_trace_deprecation</code>: (<code>true</code>|<code>false</code>) - Enable Node.js deprecation tracing (<code>--trace-deprecation</code>). Default: <code>false</code>.</li> <li><code>node_trace_warnings</code>: (<code>true</code>|<code>false</code>) - Enable Node.js warning tracing (<code>--trace-warnings</code>). Default: <code>false</code>.</li> </ul>"},{"location":"providers/setup/chain/#indexer-process-configuration","title":"Indexer Process Configuration","text":"<p>Settings controlling the behavior of the Hyperion Indexer process(es) for this chain.</p> <p><code>indexer</code>:</p> <ul> <li><code>enabled</code>: (<code>true</code>|<code>false</code>) - Whether to launch indexer instances for this chain via PM2. Default: <code>true</code>.</li> <li><code>start_on</code>: (integer) - Required. Block number to start indexing from. <code>0</code> starts from the earliest available block in SHIP or the last indexed block on Elasticsearch.  Useful for catching up or partial history. Default: <code>0</code>.</li> <li><code>stop_on</code>: (integer) - Required. Block number to stop indexing at. <code>0</code> disables the stop block. Default: <code>0</code>.</li> <li><code>rewrite</code>: (<code>true</code>|<code>false</code>) - Required. If <code>true</code>, forces the indexer to re-process blocks within the <code>start_on</code>/<code>stop_on</code> range, even if they were previously indexed. Useful for repairs or reprocessing after configuration changes. Default: <code>false</code>.</li> <li><code>purge_queues</code>: (<code>true</code>|<code>false</code>) - Required. If <code>true</code>, clears all RabbitMQ queues associated with this chain before the indexer starts. Useful for ensuring a clean start, especially after configuration changes or errors. Default: <code>false</code>.</li> <li><code>live_reader</code>: (<code>true</code>|<code>false</code>) - Required. Instructs the reader process to continuously request blocks from SHIP, aiming to stay near the head of the chain. Default: <code>false</code>. Should generally be <code>true</code> for ongoing indexing.</li> <li><code>live_only_mode</code>: (<code>true</code>|<code>false</code>) - Required. If <code>true</code>, disables historical range processing and only reads live blocks sequentially. Requires <code>live_reader: true</code>. Default: <code>false</code>.</li> <li><code>abi_scan_mode</code>: (<code>true</code>|<code>false</code>) - Required. Optimizes the indexer for quickly scanning the chain history solely to capture contract ABIs. Disables trace and most delta processing. Recommended for the first run on a new chain to ensure ABIs are available for later deserialization. Must be manually set back to <code>false</code> for normal history indexing. Default: <code>true</code>.</li> <li><code>fetch_block</code>: (<code>true</code>|<code>false</code>) - Required. Request block header information from SHIP. Required for most operations. Default: <code>true</code>.</li> <li><code>fetch_traces</code>: (<code>true</code>|<code>false</code>) - Required. Request action trace information from SHIP. Required for action history. Default: <code>true</code>.</li> <li><code>fetch_deltas</code>: (<code>true</code>|<code>false</code>) - Request state delta information from SHIP. Required for state tracking and delta features. Default: <code>true</code>.</li> <li><code>disable_reading</code>: (<code>true</code>|<code>false</code>) - Required. Completely disable the SHIP reader process(es). Useful if you only want to run deserializer/indexer workers to process existing data in queues. Default: <code>false</code>.</li> <li><code>disable_indexing</code>: (<code>true</code>|<code>false</code>) - Required. Disable the final indexing stage (writing to Elasticsearch/MongoDB). Useful for debugging the deserialization stage. Default: <code>false</code>.</li> <li><code>process_deltas</code>: (<code>true</code>|<code>false</code>) - Required. Process state deltas received from SHIP. Required for state features. Default: <code>true</code>.</li> <li><code>max_inline</code>: (integer) - Maximum depth of inline actions to process. Helps limit processing load on chains with deeply nested inline actions. Default: not set.</li> <li><code>node_max_old_space_size</code>: (integer) - Max heap size (in MB) for the Node.js Indexer master process. Default: <code>4096</code>.</li> <li><code>node_trace_deprecation</code>: (<code>true</code>|<code>false</code>) - Enable Node.js deprecation tracing. Default: <code>false</code>.</li> <li><code>node_trace_warnings</code>: (<code>true</code>|<code>false</code>) - Enable Node.js warning tracing. Default: <code>false</code>.</li> </ul>"},{"location":"providers/setup/chain/#settings-global-core-configuration","title":"Settings - Global &amp; Core Configuration","text":"<p>Core operational settings for this chain's Hyperion instance.</p> <p><code>settings:</code></p> <ul> <li><code>preview</code>: (<code>true</code>|<code>false</code>) - Required. Preview mode, prints worker map and exit. Default: <code>false</code></li> <li><code>chain</code>: (string) - Required. Short alias for the chain (e.g., <code>wax</code>, <code>eos</code>). Must match the key used in <code>config/connections.json</code>.</li> <li><code>eosio_alias</code>: (string) - Deprecated. Use <code>system_contract</code> instead. The account name considered the \"system\" account (e.g., <code>eosio</code>, <code>wax</code>).</li> <li><code>system_contract</code>: (string) - The account name considered the \"system\" account (e.g., <code>eosio</code>). Used for filtering some default system actions/deltas. Default: <code>\"eosio\"</code>.</li> <li><code>parser</code>: (string) - Required. Version of the SHIP data parser to use. Should match the version used by your Nodeos SHIP plugin (e.g., <code>\"3.2\"</code> for Leap 3.2+). Default: <code>\"3.2\"</code>.</li> <li><code>auto_stop</code>: (integer) - Required. Automatically stop the Indexer (master process) after X seconds of inactivity (no new blocks received). <code>0</code> disables auto-stop. Default: <code>0</code>.</li> <li><code>index_version</code>: (string) - Required. A suffix appended to Elasticsearch index names (e.g., <code>wax-action-v1</code>). Allows for versioning or separating different index sets. Default: <code>\"v1\"</code>.</li> <li><code>debug</code>: (<code>true</code>|<code>false</code>) - Required. Enable verbose debug logging. Default: <code>false</code>.</li> <li><code>rate_monitoring</code>: (<code>true</code>|<code>false</code>) - Required. Print internal processing rates (blocks/s, actions/s, etc.) to the console. Default: <code>true</code>.</li> <li><code>bp_logs</code>: (<code>true</code>|<code>false</code>) - Required. Enable specific logging related to block producers (requires <code>bp_monitoring</code>). Default: <code>false</code>.</li> <li><code>bp_monitoring</code>: (<code>true</code>|<code>false</code>) - Enable monitoring and logging of missed blocks and producer schedule changes. Default: <code>false</code>.</li> <li><code>ipc_debug_rate</code>: (integer) - Interval (in milliseconds) for logging inter-process communication statistics. <code>0</code> disables it. Default: <code>60000</code>.</li> <li><code>allow_custom_abi</code>: (<code>true</code>|<code>false</code>) - Required. Allow loading ABIs from local files (<code>src/indexer/custom-abi/&lt;chain&gt;/&lt;contract&gt;-&lt;start&gt;-&lt;end&gt;.json</code>) which override on-chain ABIs for specific block ranges. Default: <code>false</code>.</li> <li><code>max_ws_payload_mb</code>: (integer) - Required. Maximum allowed WebSocket message size (in MB) when connecting to SHIP. Default: <code>256</code>.</li> <li><code>ds_profiling</code>: (<code>true</code>|<code>false</code>) - Required. Enable detailed profiling of the deserialization process (writes to <code>ds_profiling.csv</code>). Default: <code>false</code>.</li> <li><code>auto_mode_switch</code>: (<code>true</code>|<code>false</code>) - Required. Allow the indexer master process to automatically switch <code>abi_scan_mode</code> off after its initial run. Default: <code>false</code>.</li> <li><code>use_global_agent</code>: (<code>true</code>|<code>false</code>) - Use <code>global-agent</code> for proxy support in outgoing HTTP/HTTPS requests (e.g., to Nodeos). Requires <code>global-agent</code> configuration via environment variables. Default: <code>false</code>.</li> <li><code>process_prefix</code>: (string) - Prefix for process names in PM2. Useful for distinguishing multiple Hyperion instances.</li> <li><code>ignore_snapshot</code>: (<code>true</code>|<code>false</code>) - Ignore snapshot data when available from SHIP. Default: <code>false</code>.</li> <li><code>ship_request_rev</code>: (string) - Specific revision request for SHIP protocol. Leave empty for default.</li> <li><code>index_partition_size</code>: (integer) - Required. The block number range size used for partitioning time-series indices (actions, deltas). E.g., <code>10000000</code> creates indices like <code>chain-action-v1-000001</code>, <code>chain-action-v1-000002</code>. Affects index management and query performance. Default: <code>10000000</code>.</li> <li><code>max_retained_blocks</code>: (integer) - Maximum number of blocks to retain in Elasticsearch before old indices are deleted. Must be a multiple of <code>index_partition_size</code>. <code>0</code> disables automatic cleanup. Default: <code>0</code>.</li> <li><code>es_replicas</code>: (integer) - Required. Number of replicas to configure for Elasticsearch indices created by Hyperion. <code>0</code> means no replicas. Default: <code>0</code>.</li> <li><code>tiered_index_allocation</code>: (object) - Configure tiered index allocation for Elasticsearch hot/warm architecture.<ul> <li><code>enabled</code>: (<code>true</code>|<code>false</code>) - Enable tiered allocation. Default: <code>false</code>.</li> <li><code>max_age_days</code>: (integer) - Maximum age in days before moving to warm tier.</li> <li><code>max_age_blocks</code>: (integer) - Maximum age in blocks before moving to warm tier.</li> <li><code>require_node_attributes</code>: (object) - Required node attributes for allocation.</li> <li><code>include_node_attributes</code>: (object) - Preferred node attributes for allocation.</li> <li><code>exclude_node_attributes</code>: (object) - Node attributes to exclude from allocation.</li> </ul> </li> </ul>"},{"location":"providers/setup/chain/#blacklists-whitelists-filtering","title":"Blacklists &amp; Whitelists - Filtering","text":"<p>Define rules to include or exclude specific actions or deltas from being indexed.</p> <p>Format: Rules use the format <code>contract::action</code> or <code>contract::*</code>. For deltas, use <code>contract::table</code>.</p> <ul> <li><code>blacklists</code>:<ul> <li><code>actions</code>: (array) - Required. List of action patterns to exclude. Default: <code>[]</code></li> <li><code>deltas</code>: (array) - Required. List of delta patterns to exclude. Default: <code>[]</code></li> </ul> </li> <li><code>whitelists</code>:<ul> <li><code>actions</code>: (array) - Required. List of action patterns to include. If non-empty, only matching actions are indexed. Default: <code>[]</code></li> <li><code>deltas</code>: (array) - Required. List of delta patterns to include. If non-empty, only matching deltas are indexed. Default: <code>[]</code></li> <li><code>max_depth</code>: (integer) - Required. Maximum inline action depth to check against whitelists. Default: <code>10</code>.</li> <li><code>root_only</code>: (<code>true</code>|<code>false</code>) - Required. If <code>true</code>, only check root-level actions against the whitelist. If <code>false</code>, check inline actions as well. Default: <code>false</code>.</li> </ul> </li> </ul> <p>Default Blacklists</p> <p>Hyperion automatically blacklists <code>eosio.null::*</code> actions.</p>"},{"location":"providers/setup/chain/#scaling-performance-resource-tuning","title":"Scaling - Performance &amp; Resource Tuning","text":"<p>Parameters controlling the number of worker processes and queue behavior, critical for tuning performance and resource usage.</p> <p><code>scaling:</code></p> <ul> <li><code>readers</code>: (integer) - Required. Number of parallel SHIP reader processes to fetch block ranges. Default: <code>1</code>.</li> <li><code>ds_queues</code>: (integer) - Required. Number of separate RabbitMQ queues for deserialization tasks. Default: <code>1</code>.</li> <li><code>ds_threads</code>: (integer) - Required. Number of deserializer worker processes per deserializer queue. Default: <code>1</code>.</li> <li><code>ds_pool_size</code>: (integer) - Required. Number of blocks buffered in memory by each deserializer thread before processing. Higher values increase RAM usage but can smooth out processing bursts. Default: <code>1</code>.</li> <li><code>indexing_queues</code>: (integer) - Required. Number of separate RabbitMQ queues for the final indexing stage (writing to ES/Mongo). Default: <code>1</code>.</li> <li><code>ad_idx_queues</code>: (integer) - Required. Number of indexer worker processes per action/delta indexing queue. Default: <code>1</code>.</li> <li><code>dyn_idx_queues</code>: (integer) - Required. Number of indexer worker processes per dynamic state table indexing queue. Default: <code>1</code>.</li> <li><code>max_autoscale</code>: (integer) - Required. Maximum number of additional reader processes the master can spawn if queues back up. Default: <code>4</code>.</li> <li><code>batch_size</code>: (integer) - Required. Number of blocks requested in a single range by a reader process during catch-up. Default: <code>5000</code>.</li> <li><code>resume_trigger</code>: (integer) - Required. Queue size threshold below which a paused component (like readers) will resume. Default: <code>5000</code>.</li> <li><code>auto_scale_trigger</code>: (integer) - Required. Queue size threshold that triggers the master to potentially spawn additional reader processes (up to <code>max_autoscale</code>). Default: <code>20000</code>.</li> <li><code>block_queue_limit</code>: (integer) - Required. Maximum number of blocks allowed in the reader-to-deserializer queues before readers pause. Default: <code>10000</code>.</li> <li><code>max_queue_limit</code>: (integer) - Required. Global maximum size for any single processing queue before components feeding it might pause. Default: <code>100000</code>.</li> <li><code>routing_mode</code>: (<code>\"round_robin\"</code>|<code>\"heatmap\"</code>) - Required. Algorithm for distributing blocks from readers to deserializer queues. <code>\"round_robin\"</code> distributes evenly. <code>\"heatmap\"</code> attempts to send blocks with similar action patterns to the same queue (can improve deserialization caching but might lead to uneven load). Default: <code>\"round_robin\"</code>.</li> <li><code>polling_interval</code>: (integer) - Required. Interval (in milliseconds) at which the master process checks queue sizes for scaling decisions. Default: <code>10000</code>.</li> </ul>"},{"location":"providers/setup/chain/#features-flags","title":"Features Flags","text":"<p><code>features</code>: </p>"},{"location":"providers/setup/chain/#streaming","title":"Streaming","text":"<ul> <li><code>streaming</code>:<ul> <li><code>enable</code>: (<code>true</code>|<code>false</code>) - Required. Enable the WebSocket live streaming API. Default: <code>false</code>.</li> <li><code>traces</code>: (<code>true</code>|<code>false</code>) - Required. Stream action traces. Default: <code>false</code>.</li> <li><code>deltas</code>: (<code>true</code>|<code>false</code>) - Required. Stream state deltas. Default: <code>false</code>.</li> </ul> </li> </ul>"},{"location":"providers/setup/chain/#tables","title":"Tables","text":"<ul> <li><code>tables</code>: Enable indexing of specific system table states into MongoDB. Requires MongoDB.<ul> <li><code>permissions</code>: (<code>true</code>|<code>false</code>) - Required. Index account permissions state. Default: <code>true</code>.</li> <li><code>proposals</code>: (<code>true</code>|<code>false</code>) - Required. Index <code>eosio.msig</code> proposals state. Default: <code>true</code>.</li> <li><code>accounts</code>: (<code>true</code>|<code>false</code>) - Required. Index token balances state (<code>accounts</code> table in token contracts). Default: <code>true</code>.</li> <li><code>voters</code>: (<code>true</code>|<code>false</code>) - Required. Index <code>eosio</code> voters table state. Default: <code>true</code>.</li> <li><code>user_resources</code>: (<code>true</code>|<code>false</code>) - Index user resource allocation state. Default: <code>false</code>.</li> </ul> </li> </ul>"},{"location":"providers/setup/chain/#contract-state","title":"Contract State","text":"<ul> <li> <p><code>contract_state</code>: Enable indexing of arbitrary contract table states into MongoDB. Requires MongoDB.</p> <ul> <li><code>enabled</code>: (<code>true</code>|<code>false</code>) - Required. Master switch for this feature. Default: <code>false</code>.</li> <li><code>contracts</code>: (object) - Required. Defines which contracts and tables to index state for. Managed via <code>./hyp-config contracts ...</code>.</li> </ul> <pre><code>\"contracts\": {\n  \"eosio.token\": { // Contract account name\n    \"accounts\": { // Table name\n      \"auto_index\": true, // Auto-create basic Mongo indexes?\n      \"indices\": { // Custom Mongo indexes\n        \"balance\": -1 // Field name: direction (1=asc, -1=desc, \"text\", \"date\", \"2dsphere\")\n      }\n    },\n    \"stat\": { /* ... more tables ... */ }\n  },\n  \"another.contract\": { /* ... more contracts ... */ }\n}\n</code></pre> </li> </ul>"},{"location":"providers/setup/chain/#other-features","title":"Other Features","text":"<ul> <li><code>index_deltas</code>: (<code>true</code>|<code>false</code>) - Required. Index state deltas to Elasticsearch (for <code>/v2/history/get_deltas</code>). Default: <code>true</code>.</li> <li><code>index_transfer_memo</code>: (<code>true</code>|<code>false</code>) - Required. Include the <code>memo</code> field within the <code>@transfer</code> data of indexed actions. Increases index size. Default: <code>true</code>.</li> <li><code>index_all_deltas</code>: (<code>true</code>|<code>false</code>) - Required. Index all state deltas, regardless of whitelists/blacklists (if <code>index_deltas</code> is true). Default: <code>true</code>.</li> <li><code>deferred_trx</code>: (<code>true</code>|<code>false</code>) - Required. Index information about deferred transactions. Default: <code>false</code>.</li> <li><code>failed_trx</code>: (<code>true</code>|<code>false</code>) - Required. Index information about failed transactions. Default: <code>false</code>.</li> <li><code>resource_limits</code>: (<code>true</code>|<code>false</code>) - Required. Index account resource limits state (<code>userres</code> table). Default: <code>false</code>.</li> <li><code>resource_usage</code>: (<code>true</code>|<code>false</code>) - Required. Index account resource usage state (<code>userres</code> table). Default: <code>false</code>.</li> <li><code>contract_console</code>: (<code>true</code>|<code>false</code>) - Index contract console output for debugging purposes. Default: <code>false</code>.</li> </ul>"},{"location":"providers/setup/chain/#prefetch-internal-buffer-sizes","title":"Prefetch - Internal Buffer Sizes","text":"<p>Configure internal buffer sizes between processing stages. Defaults are usually sufficient.</p> <p><code>prefetch:</code></p> <ul> <li><code>read</code>: (integer) - Required. Max blocks buffered by reader before sending to deserializer queue. Default: <code>50</code>.</li> <li><code>block</code>: (integer) - Required. Max blocks buffered by deserializer before sending to indexer queue. Default: <code>100</code>.</li> <li><code>index</code>: (integer) - Required. Max items buffered by indexer worker before writing to ES/Mongo. Default: <code>500</code>.</li> </ul>"},{"location":"providers/setup/chain/#hub-qry-hub-integration-optional","title":"Hub - QRY Hub Integration (Optional)","text":"<p>Settings for connecting this Hyperion instance to the QRY Hub.</p> <p><code>hub:</code></p> <ul> <li><code>enabled</code>: (<code>true</code>|<code>false</code>) - Required. Enable connection to the QRY Hub. Default: <code>false</code>.</li> <li><code>instance_key</code>: (string) - Required. Your unique instance private key obtained from QRY provider registration. Required if enabled.</li> <li><code>hub_url</code>: (string) - Override the default QRY Hub URL if using a custom hub instance.</li> <li><code>custom_indexer_controller</code>: (string) - Override the default address (<code>localhost:&lt;control_port&gt;</code>) the API server uses to connect to the indexer's control port for status updates needed by the Hub.</li> </ul>"},{"location":"providers/setup/chain/#plugin-configuration","title":"Plugin Configuration","text":"<p>Enable and configure specific settings for installed plugins managed by <code>hpm</code>. The structure depends on the plugin itself.</p> <pre><code>\"plugins\": {\n  \"my-plugin-alias\": { // Alias used during `hpm install`\n    \"enabled\": true,\n    \"some_plugin_setting\": \"value\"\n  }\n}\n</code></pre>"},{"location":"providers/setup/chain/#alerts-chain-specific-alert-triggers","title":"Alerts - Chain-Specific Alert Triggers","text":"<p>Define when specific alert events trigger and which providers (configured in <code>connections.json</code> or here) should receive them for this chain.</p> <p><code>alerts:</code></p> <ul> <li><code>triggers</code>: Object containing trigger definitions <ul> <li>(e.g., <code>onApiStart</code>, <code>onIndexerError</code>).<ul> <li><code>\"enabled\"</code>: (<code>true</code>|<code>false</code>) - Enable this specific trigger.</li> <li><code>\"cooldown\"</code>: (integer) - Minimum seconds between alerts of this type.</li> <li><code>\"emitOn\"</code>: (array) - List of provider names (e.g., <code>[\"telegram\", \"http\"]</code>) to send this alert to.</li> </ul> </li> </ul> </li> <li><code>providers</code>: Can optionally define provider connection details here instead of <code>connections.json</code> if they are specific to this chain. See the Connections Reference for provider structure.</li> </ul>"},{"location":"providers/setup/chain/#full-reference-example","title":"Full Reference Example","text":"<p>Tip</p> <p>For multiple chains, you should have one config file for each chain.</p> <p>For a complete example showing all default values, refer to the reference file <code>references/config.ref.json</code></p> <pre><code>{\n  \"api\": {\n    \"enabled\": true,\n    \"pm2_scaling\": 1,\n    \"chain_name\": \"EXAMPLE Chain\",\n    \"server_addr\": \"127.0.0.1\",\n    \"server_port\": 7000,\n    \"stream_port\": 1234,\n    \"stream_scroll_limit\": -1,\n    \"stream_scroll_batch\": 500,\n    \"server_name\": \"127.0.0.1:7000\",\n    \"provider_name\": \"Example Provider\",\n    \"provider_url\": \"https://example.com\",\n    \"chain_api\": \"\",\n    \"push_api\": \"\",\n    \"chain_logo_url\": \"\",\n    \"explorer\": {\n      \"home_redirect\": false,\n      \"upstream\": \"\",\n      \"theme\": \"\"\n    },\n    \"enable_caching\": true,\n    \"cache_life\": 1,\n    \"limits\": {\n      \"get_actions\": 1000,\n      \"get_voters\": 100,\n      \"get_links\": 1000,\n      \"get_deltas\": 1000,\n      \"get_trx_actions\": 200\n    },\n    \"access_log\": false,\n    \"chain_api_error_log\": false,\n    \"log_errors\": false,\n    \"custom_core_token\": \"\",\n    \"enable_export_action\": false,\n    \"disable_rate_limit\": false,\n    \"rate_limit_rpm\": 1000,\n    \"rate_limit_allow\": [],\n    \"disable_tx_cache\": false,\n    \"tx_cache_expiration_sec\": 3600,\n    \"v1_chain_cache\": [\n      {\"path\": \"get_block\",\"ttl\": 3000},\n      {\"path\": \"get_info\",\"ttl\": 500}\n    ],\n    \"node_max_old_space_size\": 1024,\n    \"node_trace_deprecation\": false,\n    \"node_trace_warnings\": false\n  },\n  \"indexer\": {\n    \"enabled\": true,\n    \"start_on\": 0,\n    \"stop_on\": 0,\n    \"rewrite\": false,\n    \"purge_queues\": false,\n    \"live_reader\": false,\n    \"live_only_mode\": false,\n    \"abi_scan_mode\": true,\n    \"fetch_block\": true,\n    \"fetch_traces\": true,\n    \"fetch_deltas\": true,\n    \"disable_reading\": false,\n    \"disable_indexing\": false,\n    \"process_deltas\": true,\n    \"node_max_old_space_size\": 4096,\n    \"node_trace_deprecation\": false,\n    \"node_trace_warnings\": false\n  },\n  \"settings\": {\n    \"preview\": false,\n    \"chain\": \"eos\",\n    \"parser\": \"3.2\",\n    \"auto_stop\": 0,\n    \"index_version\": \"v1\",\n    \"debug\": false,\n    \"bp_logs\": false,\n    \"bp_monitoring\": false,\n    \"ipc_debug_rate\": 60000,\n    \"allow_custom_abi\": false,\n    \"rate_monitoring\": true,\n    \"max_ws_payload_mb\": 256,\n    \"ds_profiling\": false,\n    \"auto_mode_switch\": false,\n    \"use_global_agent\": false,\n    \"index_partition_size\": 10000000,\n    \"max_retained_blocks\": 0,\n    \"es_replicas\": 0\n  },\n  \"blacklists\": {\n    \"actions\": [],\n    \"deltas\": []\n  },\n  \"whitelists\": {\n    \"actions\": [],\n    \"deltas\": [],\n    \"max_depth\": 10,\n    \"root_only\": false\n  },\n  \"scaling\": {\n    \"readers\": 1,\n    \"ds_queues\": 1,\n    \"ds_threads\": 1,\n    \"ds_pool_size\": 1,\n    \"indexing_queues\": 1,\n    \"ad_idx_queues\": 1,\n    \"dyn_idx_queues\": 1,\n    \"max_autoscale\": 4,\n    \"batch_size\": 5000,\n    \"resume_trigger\": 5000,\n    \"auto_scale_trigger\": 20000,\n    \"block_queue_limit\": 10000,\n    \"max_queue_limit\": 100000,\n    \"routing_mode\": \"round_robin\",\n    \"polling_interval\": 10000\n  },\n  \"features\": {\n    \"streaming\": {\n      \"enable\": false,\n      \"traces\": false,\n      \"deltas\": false\n    },\n    \"tables\": {\n      \"proposals\": true,\n      \"accounts\": true,\n      \"voters\": true,\n      \"permissions\": true,\n      \"user_resources\": false\n    },\n    \"contract_state\": {\n      \"contracts\": {}\n    },\n    \"index_deltas\": true,\n    \"index_transfer_memo\": true,\n    \"index_all_deltas\": true,\n    \"deferred_trx\": false,\n    \"failed_trx\": false,\n    \"resource_limits\": false,\n    \"resource_usage\": false\n  },\n  \"prefetch\": {\n    \"read\": 50,\n    \"block\": 100,\n    \"index\": 500\n  },\n  \"hub\": {\n    \"enabled\": false,\n    \"instance_key\": \"\",\n    \"custom_indexer_controller\": \"\"\n  },\n  \"plugins\": {},\n  \"alerts\": {\n    \"triggers\": {\n      \"onApiStart\": {\n        \"enabled\": true,\n        \"cooldown\": 30,\n        \"emitOn\": [\"http\"]\n      },\n      \"onIndexerError\": {\n        \"enabled\": true,\n        \"cooldown\": 30,\n        \"emitOn\": [\"telegram\", \"email\", \"http\"]\n      }\n    },\n    \"providers\": {\n      \"telegram\": {\n        \"enabled\": false,\n        \"botToken\": \"\",\n        \"destinationIds\": [1]\n      },\n      \"http\": {\n        \"enabled\": false,\n        \"server\": \"http://localhost:6200\",\n        \"path\": \"/notification\",\n        \"useAuth\": false,\n        \"user\": \"\",\n        \"pass\": \"\"\n      },\n      \"email\": {\n        \"enabled\": false,\n        \"sourceEmail\": \"sender@example.com\",\n        \"destinationEmails\": [\n          \"receiverA@example.com\",\n          \"receiverB@example.com\"\n        ],\n        \"smtp\": \"smtp-relay.gmail.com (UPDATE THIS)\",\n        \"port\": 465,\n        \"tls\": true,\n        \"user\": \"\",\n        \"pass\": \"\"\n      }\n    }\n  }\n}\n</code></pre> <p> Hyperion Configuration Connect to QRY Hub </p>"},{"location":"providers/setup/connections/","title":"Connections Reference","text":"<p> Hyperion Configuration</p>"},{"location":"providers/setup/connections/#connections-configuration-reference-configconnectionsjson","title":"Connections Configuration Reference <code>config/connections.json</code>","text":"<p>The <code>config/connections.json</code> file is the central place where Hyperion defines how to connect to its essential infrastructure components (RabbitMQ, Elasticsearch, Redis, MongoDB) and the specific Antelope blockchain nodes it needs to interact with.</p> <p>Management</p> <p>While you can edit this file manually, it's highly recommended to use the <code>./hyp-config connections init</code> command for initial setup and <code>./hyp-config connections test</code> to verify connectivity. Chain-specific entries under the <code>chains</code> key are typically added automatically when using <code>./hyp-config chains new &lt;short-name&gt; ...</code>.</p> <p>Info</p> <p>Failover option:  Now you can add an array of strings or array of objects to configure failover on Hyperion. Check <code>Chain Parameters</code> below on \"ship\" paramert to configure. </p> <p>This file contains several top-level objects:</p>"},{"location":"providers/setup/connections/#rabbitmq-parameters","title":"RabbitMQ parameters","text":"<p>Configures the connection to your RabbitMQ server, which acts as the message queue backbone for the Hyperion indexer pipeline.</p> <p><code>amqp:</code></p> <ul> <li><code>\"host\"</code>: Required. Address and AMQP port of the RabbitMQ server (e.g., <code>\"127.0.0.1:5672\"</code>).</li> <li><code>\"api\"</code>: Required. Address and port for the RabbitMQ Management HTTP API (e.g., <code>\"127.0.0.1:15672\"</code>). Used for queue management and monitoring.</li> <li><code>\"protocol\"</code>: Protocol for the RabbitMQ Management API (<code>\"http\"</code> or <code>\"https\"</code>). Defaults to <code>\"http\"</code>.</li> <li><code>\"user\"</code>: Required. Username for RabbitMQ authentication.</li> <li><code>\"pass\"</code>: Required. Password for RabbitMQ authentication.</li> <li><code>\"vhost\"</code>: Required. The virtual host within RabbitMQ that Hyperion should use (e.g., <code>\"hyperion\"</code>). This isolates Hyperion's queues. Needs to be created in RabbitMQ beforehand (e.g., via <code>sudo rabbitmqctl add_vhost hyperion</code>).</li> <li><code>\"frameMax\"</code>: Optional. Maximum frame size for AMQP connection (e.g., <code>\"0x10000\"</code>). Adjust if needed for large messages, but the default is usually sufficient.</li> </ul>"},{"location":"providers/setup/connections/#elasticsearch-parameters","title":"Elasticsearch parameters","text":"<p>Configures the connection to your Elasticsearch cluster, where historical action, delta, and block data are stored.</p> <p><code>elasticsearch:</code></p> <ul> <li><code>\"protocol\"</code>: Required. Protocol used to connect (<code>\"http\"</code> or <code>\"https\"</code>).</li> <li><code>\"host\"</code>: Required. Address and port of at least one Elasticsearch node (e.g., <code>\"127.0.0.1:9200\"</code>). The client will discover other nodes.</li> <li><code>\"ingest_nodes\"</code>: Required. An array of Elasticsearch node addresses (host:port) specifically designated for data ingestion. Can be the same as <code>\"host\"</code> if you have a single node or all nodes handle ingestion (e.g., <code>[\"127.0.0.1:9200\"]</code>). Hyperion distributes indexing load across these nodes.</li> <li><code>\"user\"</code>: Elasticsearch username if security is enabled (e.g., <code>\"elastic\"</code>). Leave empty (<code>\"\"</code>) if security is disabled.</li> <li><code>\"pass\"</code>: Elasticsearch password if security is enabled. Leave empty (<code>\"\"</code>) if security is disabled.</li> </ul>"},{"location":"providers/setup/connections/#redis-parameters","title":"Redis parameters","text":"<p>Configures the connection to your Redis server, used for caching, transaction lookups, and inter-process communication.</p> <p><code>redis:</code></p> <ul> <li><code>\"host\"</code>: Required. Address of the Redis server (e.g., <code>\"127.0.0.1\"</code>).</li> <li><code>\"port\"</code>: Required. Port number for the Redis server (e.g., <code>6379</code>).</li> </ul>"},{"location":"providers/setup/connections/#mongodb-parameters","title":"MongoDB parameters","text":"<p>Conditional Requirement</p> <p>MongoDB connection details are only required if you enable state-tracking features (<code>features.tables.*</code> or <code>features.contract_state.enabled</code>) in a specific chain's configuration file (<code>config/chains/&lt;chain&gt;.config.json</code>). If these features are disabled for all chains, this section can be omitted or left blank, but the top-level <code>mongodb</code> key might still be expected by some tools like <code>hyp-config</code>.</p> <p>Configures the connection to your MongoDB server, used for storing the current state of specific on-chain data.</p> <p><code>mongodb:</code></p> <ul> <li><code>\"enabled\"</code>: Required (if using state features). Enable MongoDB features for state-tracking features (e.g., <code>\"true\"</code> or <code>\"false\"</code>).</li> <li><code>\"host\"</code>: Required (if using state features). Address of the MongoDB server (e.g., <code>\"127.0.0.1\"</code>).</li> <li><code>\"port\"</code>: Required (if using state features). Port number for the MongoDB server (e.g., <code>27017</code>).</li> <li><code>\"database_prefix\"</code>: Required (if using state features). Prefix for database names created by Hyperion. The final database name will be <code>&lt;database_prefix&gt;_&lt;chain_alias&gt;</code> (e.g., <code>\"hyperion_wax\"</code>). Defaults to <code>\"hyperion\"</code>.</li> <li><code>\"user\"</code>: MongoDB username if authentication is enabled. Leave empty (<code>\"\"</code>) otherwise.</li> <li><code>\"pass\"</code>: MongoDB password if authentication is enabled. Leave empty (<code>\"\"</code>) otherwise.</li> </ul>"},{"location":"providers/setup/connections/#chain-parameters","title":"Chain Parameters","text":"<p>This object contains connection details specific to each blockchain that Hyperion will index. The key for each entry must match the <code>settings.chain</code> alias defined in the corresponding <code>config/chains/&lt;key&gt;.config.json</code> file.</p> <pre><code>{\n    \"chains\": {\n      \"&lt;chain_alias&gt;\": { ... chain details ... },\n      \"&lt;another_chain_alias&gt;\": { ... chain details ... }\n    }\n}\n</code></pre> <ul> <li> <p><code>&lt;chain_alias&gt;</code> Object:</p> <ul> <li><code>\"name\"</code>: Required. A descriptive name for the chain (e.g., <code>\"WAX Mainnet\"</code>).</li> <li><code>\"chain_id\"</code>: Required. The unique Chain ID hash. This is automatically populated by <code>hyp-config chains new</code> and verified during startup and testing.</li> <li><code>\"http\"</code>: Required. The URL for the chain's Nodeos HTTP API endpoint (e.g., <code>\"http://127.0.0.1:8888\"</code>). Used for fetching ABIs, chain info, etc.</li> <li> <p><code>\"ship\"</code>: Required. The WebSocket endpoint(s) for the Nodeos State History Plugin (SHIP). This is where the indexer gets its primary block data feed.</p> <ul> <li>Simple Format (Single Endpoint): A single WebSocket URL string. No automatic failover.</li> </ul> <pre><code>\"ship\": \"ws://127.0.0.1:8080\"\n</code></pre> <ul> <li>Failover Format (Multiple Endpoints): An array of objects, each with a <code>label</code> (optional description) and a <code>url</code> (the WebSocket URL). Hyperion will attempt to connect to them sequentially if the primary connection fails.</li> </ul> <pre><code>\"ship\": [\n  {\"label\": \"primary-ship\", \"url\": \"ws://ship-primary.example.com:8080\"},\n  {\"label\": \"backup-ship\", \"url\": \"ws://ship-backup.example.com:8080\"}\n]\n</code></pre> <ul> <li>Alternatively, a simple array of strings can be used, but labels are lost:</li> </ul> <pre><code>\"ship\": [\"ws://ship-primary.example.com:8080\", \"ws://ship-backup.example.com:8080\"]\n</code></pre> </li> <li> <p><code>\"WS_ROUTER_HOST\"</code>: Required. The hostname or IP address the API server should use to connect to the Hyperion Indexer's internal WebSocket router (ws-router worker). Defaults to <code>\"127.0.0.1\"</code>. Important: If your API and Indexer run on different machines, set this to the Indexer machine's reachable IP/hostname.</p> </li> <li><code>\"WS_ROUTER_PORT\"</code>: Required. The TCP port the API server should use to connect to the Indexer's internal WebSocket router. Defaults to <code>7001</code> for the first chain added via <code>hyp-config</code>, increments thereafter.</li> <li><code>\"control_port\"</code>: Required. The TCP port the Hyperion Indexer listens on for control commands (used by <code>./hyp-control</code> tool). Defaults to <code>7002</code> for the first chain, increments thereafter.</li> </ul> </li> </ul>"},{"location":"providers/setup/connections/#alert-parameters","title":"Alert parameters","text":"<p>This section configures providers (Telegram, Email, HTTP) for sending system alerts (e.g., API start, Indexer errors).</p> <p>Note</p> <p>The trigger conditions for these alerts are usually defined within the specific chain's configuration file (<code>config/chains/&lt;chain&gt;.config.json</code>) under its own <code>alerts</code> section. Ensure consistency if configuring providers in both locations. It's generally recommended to define providers here in <code>connections.json</code> if they are shared across multiple chains, or in the chain-specific config if they are unique to that chain.</p> <p><code>alerts:</code></p> <ul> <li><code>triggers</code>: (Usually defined in chain config, shown here for completeness based on reference) Defines when alerts fire <ul> <li>(e.g., <code>onApiStart</code>, <code>onIndexerError</code>).<ul> <li><code>\"enabled\"</code>: <code>true</code> or <code>false</code>.</li> <li><code>\"cooldown\"</code>: number, minimum seconds between alerts of the same type.</li> <li><code>\"emitOn\"</code>: Array of provider names (e.g., <code>[\"telegram\", \"http\"]</code>) to send this trigger to. </li> </ul> </li> </ul> </li> <li><code>providers</code>: Defines how alerts are sent.<ul> <li><code>telegram</code>:<ul> <li><code>\"enabled\"</code>: <code>true</code> or <code>false</code>.</li> <li><code>\"botToken\"</code>: Your Telegram Bot Token.</li> <li><code>\"destinationIds\"</code>: Array of numeric Telegram Chat IDs to send alerts to.</li> </ul> </li> <li><code>http</code>:<ul> <li><code>\"enabled\"</code>: <code>true</code> or <code>false</code>.</li> <li><code>\"server\"</code>: URL of the receiving HTTP endpoint (e.g., <code>\"http://my-alert-server.com\"</code>).</li> <li><code>\"path\"</code>: Path on the server (e.g., <code>\"/notification\"</code>).</li> <li><code>\"useAuth\"</code>: <code>true</code> if basic authentication is required.</li> <li><code>\"user\"</code>: Username for basic auth.</li> <li><code>\"pass\"</code>: Password for basic auth.</li> </ul> </li> <li><code>email</code>:<ul> <li><code>\"enabled\"</code>: <code>true</code> or <code>false</code>.</li> <li><code>\"sourceEmail\"</code>: Email address to send from.</li> <li><code>\"destinationEmails\"</code>: Array of recipient email addresses.</li> <li><code>\"smtp\"</code>: SMTP server hostname (e.g., <code>\"smtp.example.com\"</code>).</li> <li><code>\"port\"</code>: SMTP server port (e.g., <code>465</code> or <code>587</code>).</li> <li><code>\"tls\"</code>: <code>true</code> for implicit TLS (usually port 465), <code>false</code> otherwise (STARTTLS might be used on port 587 depending on server).</li> <li><code>\"user\"</code>: SMTP username for authentication.</li> <li><code>\"pass\"</code>: SMTP password for authentication.</li> </ul> </li> </ul> </li> </ul>"},{"location":"providers/setup/connections/#full-reference-example-referencesconnectionsrefjson","title":"Full Reference Example (<code>references/connections.ref.json</code>)","text":"<p>See references/connections.ref.json on Github</p> <pre><code>{\n  \"amqp\": {\n    \"host\": \"127.0.0.1:5672\",\n    \"api\": \"127.0.0.1:15672\",\n    \"protocol\": \"http\",\n    \"user\": \"username\",\n    \"pass\": \"password\",\n    \"vhost\": \"hyperion\",\n    \"frameMax\": \"0x10000\"\n  },\n  \"elasticsearch\": {\n    \"protocol\": \"http\",\n    \"host\": \"127.0.0.1:9200\",\n    \"ingest_nodes\": [\n      \"127.0.0.1:9200\"\n    ],\n    \"user\": \"elastic\",\n    \"pass\": \"password\"\n  },\n  \"redis\": {\n    \"host\": \"127.0.0.1\",\n    \"port\": 6379\n   },\n\n   \"mongodb\": {\n    \"enabled\": false,\n    \"host\": \"127.0.0.1\",\n    \"port\": 27017,\n    \"database_prefix\": \"hyperion\",\n    \"user\": \"\",\n    \"pass\": \"\"\n  },\n  \"chains\": {\n    \"eos\": {\n      \"name\": \"\",\n      \"chain_id\": \"\",\n      \"http\": \"http://127.0.0.1:8888\",\n      \"ship\": [\n        {\"label\": \"primary\", \"url\": \"ws://127.0.0.1:8080\"}\n      ],\n      \"WS_ROUTER_HOST\": \"127.0.0.1\",\n      \"WS_ROUTER_PORT\": 7001,\n      \"control_port\": 7002\n    }\n  },\n  \"alerts\": {\n    \"triggers\": {\n      \"onApiStart\": {\n        \"enabled\": true,\n        \"cooldown\": 30,\n        \"emitOn\": [\"http\"]\n      },\n      \"onIndexerError\": {\n        \"enabled\": true,\n        \"cooldown\": 30,\n        \"emitOn\": [\"telegram\", \"email\", \"http\"]\n      }\n    },\n    \"providers\": {\n      \"telegram\": {\n        \"enabled\": false,\n        \"botToken\": \"\",\n        \"destinationIds\": [1]\n      },\n      \"http\": {\n        \"enabled\": false,\n        \"server\": \"\",\n        \"path\": \"\",\n        \"useAuth\": false,\n        \"user\": \"\",\n        \"pass\": \"\"\n      },\n      \"email\": {\n        \"enabled\": false,\n        \"sourceEmail\": \"\",\n        \"destinationEmails\": [],\n        \"smtp\": \"\",\n        \"port\": 465,\n        \"tls\": true,\n        \"user\": \"\",\n        \"pass\": \"\"\n      }\n    }\n  }\n}\n</code></pre> <p> Hyperion Configuration Chain Config Reference </p>"},{"location":"providers/setup/hyperion_configuration/","title":"Hyperion Configuration","text":"<p>This guide walks you through configuring Hyperion after you have successfully installed the necessary dependencies and cloned/built the Hyperion codebase.</p> <p>We will primarily use the <code>hyp-config</code> command-line tool provided with Hyperion. This tool helps initialize connection settings, manage configurations for different blockchain chains, and set up contract state indexing.</p> <p>CLI Tool</p> <p>Ensure you are in the root directory of your Hyperion installation (<code>hyperion-history-api/</code>) when running <code>./hyp-config</code> commands. Run <code>./hyp-config --help</code> for a full list of commands and options.</p> <p>New failover option</p> <p>Check Connections Reference to add the new failover option.</p>"},{"location":"providers/setup/hyperion_configuration/#step-1-configure-infrastructure-connections","title":"Step 1: Configure Infrastructure Connections","text":"<p>Hyperion needs to know how to connect to shared infrastructure components like Elasticsearch, RabbitMQ, Redis, and optionally MongoDB.</p>"},{"location":"providers/setup/hyperion_configuration/#11-initialize-connectionsjson","title":"1.1 Initialize <code>connections.json</code>","text":"<p>Run the <code>init</code> command. It will prompt you for connection details and test connectivity to each service. If <code>config/connections.json</code> already exists, it will ask for confirmation before overwriting.</p> <pre><code># Navigate to your Hyperion installation directory first\ncd ~/hyperion-history-api # Or your specific path\n\n./hyp-config connections init\n</code></pre> <p>Follow the interactive prompts to enter details for RabbitMQ, Elasticsearch, Redis, and MongoDB (if you plan to use state features).</p> <p>The initialization command will create a <code>connections.json</code> file that follows the template in the Connections Reference Guide.</p> <p>MongoDB setup</p> <p>If you skip MongoDB setup here but later enable state features, you will need to manually edit <code>config/connections.json</code> to add the MongoDB connection details.</p>"},{"location":"providers/setup/hyperion_configuration/#12-test-connections","title":"1.2 Test Connections","text":"<p>After initialization or manual editing, verify that Hyperion can connect to all configured services:</p> <pre><code>./hyp-config connections test\n</code></pre> <p>Address any connection errors reported by this command before proceeding.</p>"},{"location":"providers/setup/hyperion_configuration/#13-optional-reset-connections","title":"1.3 (Optional) Reset Connections","text":"<p>If you need to start over with connection settings, you can reset the file (a backup will be created):</p> <pre><code>./hyp-config connections reset\n</code></pre> <p>Connections Reference</p> <p>For detailed information on the structure and parameters within <code>config/connections.json</code>, see the Connections Reference Guide.</p>"},{"location":"providers/setup/hyperion_configuration/#step-2-add-and-configure-chains","title":"Step 2: Add and Configure Chains","text":"<p>Each blockchain you want Hyperion to index requires its own configuration file located in <code>config/chains/</code>.</p>"},{"location":"providers/setup/hyperion_configuration/#21-add-a-new-chain-configuration","title":"2.1 Add a New Chain Configuration","text":"<p>Use the <code>chains new</code> command, providing a short alias for your chain (e.g., <code>wax</code>, <code>eos</code>, <code>jungle4</code>) and the required HTTP API and SHIP WebSocket endpoints for its node.</p> <pre><code>./hyp-config chains new &lt;short-name&gt; --http &lt;node-http-url&gt; --ship &lt;node-ship-url&gt;\n\n# Example for WAX Mainnet:\n./hyp-config chains new eos --http \"http://127.0.0.1:8888\" --ship \"ws://127.0.0.1:8080\"\n</code></pre> <p>This command performs several actions:</p> <ol> <li>Tests connectivity to the provided HTTP and SHIP endpoints.</li> <li>Determines the Chain ID and default parser version.</li> <li>Creates the chain configuration file: <code>config/chains/&lt;short-name&gt;.config.json</code> based on a template.</li> <li>Adds the chain's connection details (HTTP, SHIP, Chain ID, default ports) to <code>config/connections.json</code>.</li> </ol>"},{"location":"providers/setup/hyperion_configuration/#22-customize-chain-configuration","title":"2.2 Customize Chain Configuration","text":"<p>Tailor Hyperion for your specific needs</p> <p>The newly created <code>config/chains/&lt;short-name&gt;.config.json</code> file contains default settings. You MUST review and edit this file if you need to tailor Hyperion's behavior for your specific needs.</p> <p>Key sections to review and potentially modify:</p> <ul> <li><code>api</code>: Server address/port, provider info, caching, rate limits, logos.</li> <li><code>indexer</code>: <code>start_on</code>/<code>stop_on</code> blocks, <code>rewrite</code>, <code>live_reader</code>, filtering options.</li> <li><code>settings</code>: Parser version, debug flags, index partitioning.</li> <li><code>scaling</code>: Number of reader/indexer workers, queue sizes, batch sizes (critical for performance).</li> <li><code>features</code>: Enable/disable functionalities like state table indexing (<code>tables</code>, <code>contract_state</code>), streaming, specific data indexing (<code>index_transfer_memo</code>).</li> <li><code>blacklists/whitelists</code>: Define specific contracts/actions/tables to include or exclude.</li> </ul> <p>Chain Config Reference</p> <p>Consult the Chain Config Reference for a detailed explanation of all available parameters.</p>"},{"location":"providers/setup/hyperion_configuration/#23-verify-chain-configurations","title":"2.3 Verify Chain Configurations","text":"<p>After adding chains and editing their configurations, list them to ensure they are recognized:</p> <pre><code>./hyp-config chains list\n</code></pre> <p>Test the specific endpoints configured for a chain:</p> <pre><code>./hyp-config chains test &lt;short-name&gt;\n\n# Example:\n./hyp-config chains test wax\n</code></pre>"},{"location":"providers/setup/hyperion_configuration/#24-optional-remove-a-chain","title":"2.4 (Optional) Remove a Chain","text":"<p>To remove a chain's configuration file and its entry in connections.json (backups are created):</p> <pre><code>./hyp-config chains remove &lt;short-name&gt;\n</code></pre>"},{"location":"providers/setup/hyperion_configuration/#step-3-configure-state-indexing-optional","title":"Step 3: Configure State Indexing (Optional)","text":"<p>Hyperion can index the current state of specific contract tables into MongoDB, enabling fast queries via the <code>/v2/state/</code> API endpoints (especially <code>get_table_rows</code>). This is optional and complements the historical data stored in Elasticsearch.</p> <p>Requirement: MongoDB</p> <p>Enabling state indexing requires a correctly configured MongoDB connection in <code>config/connections.json</code> (See Step 1).</p>"},{"location":"providers/setup/hyperion_configuration/#31-enable-state-features","title":"3.1 Enable State Features","text":"<p>Edit your <code>config/chains/&lt;short-name&gt;.config.json</code> file:</p> <ul> <li>To index system tables (balances, voters, proposals), set the relevant flags under <code>features.tables</code> to <code>true</code>.</li> <li>To index specific contract tables, set <code>features.contract_state.enabled</code> to <code>true</code>.</li> </ul>"},{"location":"providers/setup/hyperion_configuration/#32-define-contractstables-for-state-indexing","title":"3.2 Define Contracts/Tables for State Indexing","text":"<p>Use the <code>hyp-config</code> contracts commands to specify which contract tables should have their state indexed in MongoDB.</p> <ul> <li>List current configuration:</li> </ul> <pre><code>./hyp-config contracts list &lt;chain-name&gt;\n</code></pre> <ul> <li>Add/Update a single table: (Indices JSON defines MongoDB indexes: <code>1</code>=asc, <code>-1</code>=desc, <code>\"text\"</code>=text, etc.)</li> </ul> <pre><code>./hyp-config contracts add-single &lt;chainName&gt; &lt;account&gt; &lt;table&gt; &lt;autoIndex&gt; [indicesJson]\n\n# Example: Index 'accounts' table in 'eosio.token', auto-create basic indexes\n./hyp-config contracts add-single wax eosio.token accounts true\n\n# Example: Index 'mydata' in 'mycontract', disable auto-index, add custom index on 'field1'\n./hyp-config contracts add-single wax mycontract mydata false '{\"field1\": 1}'\n</code></pre> <ul> <li>Add/Update multiple tables: (Uses a JSON array string for table definitions)</li> </ul> <pre><code>./hyp-config contracts add-multiple &lt;chainName&gt; &lt;account&gt; '&lt;tablesJsonArray&gt;'\n\n# Example for eosio.token:\n./hyp-config contracts add-multiple wax eosio.token '[{\"name\":\"accounts\",\"autoIndex\":true,\"indices\":{\"balance\":-1}},{\"name\":\"stat\",\"autoIndex\":true,\"indices\":{}}]'\n</code></pre> <p>Note</p> <p>Changes made using <code>hyp-config</code> contracts modify the <code>features.contract_state.contracts</code> section of your <code>config/chains/&lt;chain&gt;.config.json</code> file.</p>"},{"location":"providers/setup/hyperion_configuration/#33-synchronize-state-data","title":"3.3 Synchronize State Data","text":"<ul> <li>Initial Sync: If you enable state indexing after the indexer has already processed blocks containing the relevant contract data, the MongoDB state tables might be incomplete. You will need to run a manual synchronization using the <code>hyp-control</code> tool after starting the indexer (See Step 4).</li> </ul> <pre><code># After indexer is running and connected via control port\n./hyp-control sync contract-state &lt;chain-name&gt;\n./hyp-control sync accounts &lt;chain-name&gt;  # If features.tables.accounts = true\n./hyp-control sync voters &lt;chain-name&gt;    # If features.tables.voters = true\n./hyp-control sync proposals &lt;chain-name&gt; # If features.tables.proposals = true\n</code></pre> <ul> <li>Indexing During Run: If state features are enabled before the indexer starts processing the relevant blocks, it should populate the MongoDB state tables automatically as it processes the deltas.</li> </ul>"},{"location":"providers/setup/hyperion_configuration/#step-4-running-hyperion-services","title":"Step 4: Running Hyperion Services","text":"<p>Hyperion uses the PM2 process manager. The primary way to manage services is via the ecosystem configuration file.</p> <p>We provide scripts to simplify the process of starting and stopping your Hyperion Indexer or API instance.</p>"},{"location":"providers/setup/hyperion_configuration/#41-starting-individual-components-using-convenience-scripts","title":"4.1 Starting Individual Components (using convenience scripts)","text":"<p>You can use the provided scripts to start specific components for a single chain:</p> <ul> <li> <p>To run the indexer, execute <code>./run.sh [chain name]-indexer</code></p> </li> <li> <p>To run the api, execute <code>./run.sh [chain name]-api</code></p> </li> </ul> <p>Examples</p> <pre><code># Start indexer for \"wax\" chain:\n./run.sh wax-indexer\n\n# Start API server(s) for \"wax\" chain:\n./run.sh wax-api\n</code></pre> <p>Warning</p> <p>The Hyperion Indexer is configured to perform an abi scan (<code>\"abi_scan_mode\": true</code>) as default. The <code>abi_scan_mode</code> is required to perform full history indexing. If you are starting from a snapshot, you can disable it and use the live reader directly (<code>live_reader: true</code>)</p>"},{"location":"providers/setup/hyperion_configuration/#42-stopping-components","title":"4.2 Stopping Components","text":"<p>Use the <code>stop.sh</code> script to stop an instance or the <code>pm2</code> trigger:</p> <p>Examples</p> <pre><code># Stop API for \"wax\" chain:\n./stop.sh wax-api\n\n# Stop indexer for \"eos\" chain (allows graceful queue flush):\n./stop.sh eos-indexer\n# OR\npm2 trigger eos-indexer stop\n</code></pre> <p>Indexer Stop Behavior</p> <p>Stopping the indexer with <code>./stop.sh</code> or <code>pm2 trigger</code> sends a signal allowing it to finish processing items currently in memory and queues before shutting down fully. This can take time. Use <code>pm2 stop &lt;app-name&gt;</code> for an immediate (but potentially data-lossy) stop.</p>"},{"location":"providers/setup/hyperion_configuration/#step-5-verify-operation","title":"Step 5: Verify Operation","text":""},{"location":"providers/setup/hyperion_configuration/#51-check-process-status","title":"5.1 Check Process Status","text":"<p>List running processes managed by PM2:</p> <pre><code>pm2 list\n</code></pre> <p>Ensure your <code>&lt;chain&gt;-indexer</code> and <code>&lt;chain&gt;-api</code> processes are online.</p>"},{"location":"providers/setup/hyperion_configuration/#52-monitor-logs","title":"5.2 Monitor Logs","text":"<p>View logs for specific components:</p> <pre><code>pm2 logs &lt;app-name&gt;\n# Example:\npm2 logs wax-indexer\npm2 logs wax-api\n</code></pre> <p>Look for healthy indexing progress messages in the indexer logs and successful API startup messages (<code>Hyperion API for &lt;chain&gt; ready...</code>) in the API logs.</p>"},{"location":"providers/setup/hyperion_configuration/#53-check-api-health","title":"5.3 Check API Health","text":"<p>Query the health endpoint of your running API server (default port 7000):</p> <pre><code>curl -Ss \"http://127.0.0.1:7000/v2/health\" | jq # Use your API server IP/port\n</code></pre> <p>Review the status of Elasticsearch, RabbitMQ, Redis, Nodeos, and SHIP connections.</p>"},{"location":"providers/setup/hyperion_configuration/#54-perform-basic-queries","title":"5.4 Perform Basic Queries","text":"<p>Test fundamental history endpoints:</p> <pre><code># Get latest action\ncurl -Ss \"http://127.0.0.1:7000/v2/history/get_actions?limit=1\" | jq\n\n# Get latest delta\ncurl -Ss \"http://127.0.0.1:7000/v2/history/get_deltas?limit=1\" | jq\n</code></pre> <p>If you enabled state features, test relevant <code>/v2/state</code> endpoints (e.g., <code>get_tokens</code>, <code>get_voters</code>, <code>get_table_rows</code>).</p>"},{"location":"providers/setup/hyperion_configuration/#55-check-data-stores-optional","title":"5.5 Check Data Stores (Optional)","text":"<ul> <li>Use Kibana (if installed) to explore Elasticsearch indices (<code>&lt;chain&gt;-action-*</code>, <code>&lt;chain&gt;-delta-*</code>, etc.).</li> <li>Use <code>redis-cli</code> or RedisCommander (if installed via Docker) to inspect cached data.</li> <li>Use <code>mongosh</code> to inspect MongoDB databases (<code>hyperion_&lt;chain&gt;</code>) and collections (<code>accounts</code>, <code>voters</code>, <code>proposals</code>, <code>&lt;contract&gt;-&lt;table&gt;</code>).</li> </ul>"},{"location":"providers/setup/hyperion_configuration/#step-6-configure-advanced-features-optional","title":"Step 6: Configure Advanced Features (Optional)","text":"<p>Once the basic setup is running and verified, you can enable advanced features by editing your <code>config/chains/&lt;chain&gt;.config.json</code> file and restarting the relevant services (<code>pm2 restart &lt;app-name&gt;</code>).</p>"},{"location":"providers/setup/hyperion_configuration/#live-streaming","title":"Live Streaming","text":"<ul> <li>Enable flags under <code>features.streaming.</code></li> </ul> <pre><code>{\n    \"features\": {\n        \"streaming\": {\n        \"enable\": true,\n        \"traces\": true,\n        \"deltas\": true\n        }\n    }\n}\n</code></pre> <ul> <li>By default, the stream api will be available on the port 1234, this can be configured by the <code>api.stream_port</code> property in the chain config file.</li> <li>Once you're done configuring, just restart both the indexer and api.</li> <li> <p>A quick test using <code>curl 127.0.0.1:1234/stream/</code> should result in the output <code>{\"code\":0,\"message\":\"Transport unknown\"}</code> meaning the port is ready for websocket connections. Alternatively, you can check the api logs after restart for a <code>Websocket manager loaded!</code> message</p> </li> <li> <p>Configure your reverse proxy (like NGINX) to handle WebSocket upgrades for the <code>/stream/</code> path.</p> </li> </ul> NGINX <pre><code># Example NGINX location block for streaming\nlocation /stream/ {\n    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n    proxy_set_header Host $host;\n    proxy_pass http://127.0.0.1:1234/stream/; # Use configured stream_port\n    proxy_http_version 1.1;\n    proxy_set_header Upgrade $http_upgrade;\n    proxy_set_header Connection \"upgrade\";\n}\n</code></pre> <ul> <li>See the Stream Client Guide for usage.</li> </ul>"},{"location":"providers/setup/hyperion_configuration/#plugins","title":"Plugins","text":"<ul> <li>Install and manage plugins using the <code>./hpm</code> tool (Refer to <code>hpm --help</code> and future plugin documentation).</li> <li>Enable and configure installed plugins under the <code>plugins</code> section in your chain config file.</li> </ul>"},{"location":"providers/setup/hyperion_configuration/#qry-hub-integration","title":"QRY Hub Integration","text":"<ul> <li>Enable and configure connection under the <code>hub</code> section in your chain config file.</li> <li>See the Connect to QRY Guide.</li> </ul>"},{"location":"providers/setup/hyperion_configuration/#next-steps","title":"Next Steps","text":"<p>With Hyperion configured and running, you can now:</p> <ul> <li>Explore the full API Reference in the Swagger UI (<code>/v2/docs</code>).</li> <li>Learn how to query the API effectively in the Developer Guide.</li> <li>Consult the Troubleshooting Guides if you encounter issues.</li> </ul> <p>API Reference  Developer Guide  Troubleshooting Guides </p> <p></p>"},{"location":"providers/setup/qry_connection/","title":"What is QRY","text":"<p>QRY is a decentralized ecosystem designed to provide seamless access to various data services and APIs. The initial release of QRY prioritizes the Antelope Web3 ecosystem, granting users dependable access to these services while offering rewards to incentivize service providers.</p> <p>For Antelope Service Providers, QRY introduces new revenue streams for essential services like Hyperion, Chain API, and Atomic Assets, which have previously lacked defined rewards.</p> <p>With QRY actively under development, the ecosystem is currently in its foundational phase. The QRY team invites interested providers to register and contribute to the ongoing development of the QRY HUB.</p>"},{"location":"providers/setup/qry_connection/#qry-hub-hyperion-dashboard","title":"QRY HUB Hyperion Dashboard","text":"<p>This guide provides a step-by-step walkthrough for Provider Registration and establishing a connection to the QRY HUB.</p>"},{"location":"providers/setup/qry_connection/#register-and-connect-your-provider","title":"Register and Connect Your Provider","text":"<p>This guide outlines the steps required to register as a provider and participate in the QRY ecosystem:</p> <ol> <li>Complete Your Provider Profile</li> <li>Add Your Service Instances</li> <li>Configure Hyperion (version 3.5.0)</li> <li>Restart the Hyperion API</li> <li>Access the QRY HUB</li> <li>Connect to the QRY HUB Bot</li> </ol>"},{"location":"providers/setup/qry_connection/#detailed-steps","title":"Detailed Steps","text":""},{"location":"providers/setup/qry_connection/#complete-your-provider-profile","title":"Complete Your Provider Profile","text":"<ul> <li>Navigate to the QRY Provider Registration Page.</li> <li>Choose the blockchain network to register, such as the Jungle in this example.</li> <li>Log in with your producer account using either Anchor wallet for simplicity or <code>cleos</code> for a more secure, air-gapped method. </li> <li>Follow the steps below to log in with <code>cleos</code>:<ul> <li>Select <code>cleos</code>, enter your Account Name and Permission, and generate the Cleos Command.</li> <li>Copy the generated command and paste it into your <code>cleos</code> CLI to produce your signature. Submit this signature on the registration page.</li> </ul> </li> <li>Complete the Provider Registration Page and select the Register button.</li> </ul>"},{"location":"providers/setup/qry_connection/#add-your-service-instances","title":"Add Your Service Instances","text":"<ul> <li>After registering, go to the Manage Instances page and select Add Instance.</li> <li>Complete the Register Instance Page by adding a Keypair, selecting \"Hyperion\" as the Service Type, and entering your Public API.</li> <li>Save the keys generated here for QRY HUB authentication.</li> </ul>"},{"location":"providers/setup/qry_connection/#configure-hyperion","title":"Configure Hyperion","text":"<ul> <li>Ensure that Hyperion is upgraded to version 3.5.0 to connect with the QRY HUB. Hyperion 3.5.0 Repository.</li> <li> <p>Edit the <code>chains/wax.config.json</code> file to include the instance key under the <code>\"hub\"</code> section.</p> <pre><code>\"hub\": {\n  \"instance_key\": \"PVT_K1_2VGf8RznDRPkNNofhRwvHYs2puU5YQkVcB7bnWFhmFKE6gPdm1\"\n},\n</code></pre> </li> </ul>"},{"location":"providers/setup/qry_connection/#restart-the-hyperion-api","title":"Restart the Hyperion API","text":"<ul> <li> <p>Restart the Hyperion API to connect to the QRY HUB:</p> <pre><code>cd ~/hyperion-history-api\npm2 start --only wax-api --update-env\n</code></pre> </li> <li> <p>PM2 logs will confirm the API\u2019s connection to the QRY HUB.</p> </li> </ul>"},{"location":"providers/setup/qry_connection/#access-the-qry-hub","title":"Access the QRY HUB","text":"<ul> <li>Go to the QRY HUB Page to view the status of your service and ensure your API is operational.</li> </ul>"},{"location":"providers/setup/qry_connection/#connect-to-the-qry-hub-bot","title":"Connect to the QRY HUB Bot","text":"<ul> <li>The QRY HUB Bot provides updates on your service status. If a Telegram account was configured, start receiving alerts by launching the QRY HUB Bot.</li> </ul> <p>You are now successfully registered and active within the QRY Ecosystem. Join the QRY Network Telegram Group for further updates and support.</p>"}]}