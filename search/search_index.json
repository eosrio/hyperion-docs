{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hyperion History API \u00b6 Scalable Full History API Solution for EOSIO based blockchains Made with \u2665 by EOS Rio 1. Overview \u00b6 Hyperion is a full history solution for indexing, storing and retrieving EOSIO blockchain`s historical data. EOSIO protocol is highly scalable, reaching up to tens of thousands of transactions per second, demanding high-performance indexing and optimized storage and querying solutions. Hyperion is developed to tackle those challenges providing open-source software to be operated by block producers, infrastructure providers and dApp developers. Focused on delivering faster search times, lower bandwidth overhead and easier usability for UI/UX developers, Hyperion implements an improved data structure: actions are stored in a flattened format; a parent field is added to the inline actions to point to the parent global sequence if the inline action data is identical to the parent, it is considered a notification and thus removed from the database. No blocks or transaction data are stored; all information can be reconstructed from actions. 2. Architecture \u00b6 The following components are required to have a fully functional Hyperion API deployment. Tip For small use cases, it is fine to run all components on a single machine. But for larger chains and production environments, we recommend setting them up into different servers under a high-speed local network. 2.1 - Elasticsearch Cluster \u00b6 The ES cluster is responsible for storing all indexed data. Direct access to the Hyperion API and Indexer must be provided. We recommend nodes in the cluster to have at least 32GB of RAM and 8 CPU cores. SSD/NVME drives are recommended for maximum indexing throughput. For production environments, a multi-node cluster is highly recommended. 2.2 - Hyperion Indexer \u00b6 The indexer is a Node.js based app that processes data from the state history plugin and allows it to be indexed. The PM2 process manager is used to launch and operate the indexer. The configuration flexibility is very extensive, so system recommendations depend on the use case and data load. It requires access to at least one ES node, RabbitMQ, and the state history node. 2.3 - Hyperion API \u00b6 Parallelizable API server that provides the V2 and V1 (legacy history plugin) endpoints. It is launched by PM2 and can also operate in cluster mode. It requires direct access to at least one ES node for the queries and all other services for a full health check. 2.4 - RabbitMQ \u00b6 Use as messaging queue and data transport between the indexer stages. 2.5 - EOSIO State History \u00b6 Nodeos plugin used to collect action traces and state deltas. It provides data via websocket to the indexer. 3. How to use \u00b6 3.1 For Providers \u00b6 Script For fresh installs, we recommend using the installation script. To do that, refer to the script section . Manual Installation If you already have a previous version of Hyperion installed or if you want to set up the whole environment manually, please, refer to the manual installation section . Docker For a light docker version of Hyperion, click here . 3.2 For Developers \u00b6 For developers, click here","title":"Home"},{"location":"#hyperion-history-api","text":"Scalable Full History API Solution for EOSIO based blockchains Made with \u2665 by EOS Rio","title":"Hyperion History API"},{"location":"#1-overview","text":"Hyperion is a full history solution for indexing, storing and retrieving EOSIO blockchain`s historical data. EOSIO protocol is highly scalable, reaching up to tens of thousands of transactions per second, demanding high-performance indexing and optimized storage and querying solutions. Hyperion is developed to tackle those challenges providing open-source software to be operated by block producers, infrastructure providers and dApp developers. Focused on delivering faster search times, lower bandwidth overhead and easier usability for UI/UX developers, Hyperion implements an improved data structure: actions are stored in a flattened format; a parent field is added to the inline actions to point to the parent global sequence if the inline action data is identical to the parent, it is considered a notification and thus removed from the database. No blocks or transaction data are stored; all information can be reconstructed from actions.","title":"1. Overview"},{"location":"#2-architecture","text":"The following components are required to have a fully functional Hyperion API deployment. Tip For small use cases, it is fine to run all components on a single machine. But for larger chains and production environments, we recommend setting them up into different servers under a high-speed local network.","title":"2. Architecture"},{"location":"#21-elasticsearch-cluster","text":"The ES cluster is responsible for storing all indexed data. Direct access to the Hyperion API and Indexer must be provided. We recommend nodes in the cluster to have at least 32GB of RAM and 8 CPU cores. SSD/NVME drives are recommended for maximum indexing throughput. For production environments, a multi-node cluster is highly recommended.","title":"2.1 - Elasticsearch Cluster"},{"location":"#22-hyperion-indexer","text":"The indexer is a Node.js based app that processes data from the state history plugin and allows it to be indexed. The PM2 process manager is used to launch and operate the indexer. The configuration flexibility is very extensive, so system recommendations depend on the use case and data load. It requires access to at least one ES node, RabbitMQ, and the state history node.","title":"2.2 - Hyperion Indexer"},{"location":"#23-hyperion-api","text":"Parallelizable API server that provides the V2 and V1 (legacy history plugin) endpoints. It is launched by PM2 and can also operate in cluster mode. It requires direct access to at least one ES node for the queries and all other services for a full health check.","title":"2.3 - Hyperion API"},{"location":"#24-rabbitmq","text":"Use as messaging queue and data transport between the indexer stages.","title":"2.4 - RabbitMQ"},{"location":"#25-eosio-state-history","text":"Nodeos plugin used to collect action traces and state deltas. It provides data via websocket to the indexer.","title":"2.5 - EOSIO State History"},{"location":"#3-how-to-use","text":"","title":"3. How to use"},{"location":"#31-for-providers","text":"Script For fresh installs, we recommend using the installation script. To do that, refer to the script section . Manual Installation If you already have a previous version of Hyperion installed or if you want to set up the whole environment manually, please, refer to the manual installation section . Docker For a light docker version of Hyperion, click here .","title":"3.1 For Providers"},{"location":"#32-for-developers","text":"For developers, click here","title":"3.2 For Developers"},{"location":"chain/","text":"Detailed description of the chains/example.config.json \u00b6 This section is a quick guide of the config.json file. Here you are going to find a brief explanation of each parameter with its default value and an example of real usage. 1. API Configuration \u00b6 \"chain_name\": \"EXAMPLE Chain\" \"server_addr\": \"127.0.0.1\" \"server_port\": 7000 \"server_name\": \"127.0.0.1:7000\" \"provider_name\": \"Example Provider\" \"provider_url\": \"https://example.com\" \"chain_logo_url\": \"\" \"enable_caching\": true \u21d2 Set API cache \"cache_life\": 1 \u21d2 Define the cache life \"limits\" \u21d2 Set API response limits \"get_actions\": 1000 \"get_voters\": 100 \"get_links\": 1000 \"get_deltas\": 1000 \"access_log\": false \u21d2 Enable log API access. \"enable_explorer\": false 2. Settings \u00b6 \"preview\": false \u21d2 Preview mode - prints worker map and exit \"chain\": \"eos\" \u21d2 Chain name (The same used on ecosystem.config.js) \"eosio_alias\": \"eosio\" \"parser\": \"1.8\" \u21d2 Version of the parser to be used \"auto_stop\": 300 \u21d2 Automatically stop Indexer after X seconds if no more blocks are being processed (0=disable) \"index_version\": \"v1\" \u21d2 Set the index version \"debug\": false \u21d2 Enable debug mode \"rate_monitoring\": true \u21d2 Print data rates for each phase every 5s \"bp_logs\": false \u21d2 Enable logs \"bp_monitoring\": false \u21d2 Allow monitoring and logging of missed blocks and rounds \"ipc_debug_rate\": 60000 \u21d2 interval between IPC messaging rate debug messages, setting it to false or 0 will disable the logging \"allow_custom_abi\": false \u21d2 allow using custom ABIs from the custom-abi/ folder, they must match the pattern - - .json. Those ABIs will overwrite on-chain data for the given range. 3. Blacklists \u00b6 Blacklist for actions and deltas \"actions\": [] \"deltas\": [] 4. Whitelists \u00b6 Whitelist for actions and deltas actions\": [] \"deltas\": [] 5. Scaling options \u00b6 \"batch_size\": 10000 \u21d2 Parallel reader batch size in blocks \"queue_limit\": 50000 \u21d2 Queue size limit on rabbitmq \"readers\": 1 \u21d2 Number of readers \"ds_queues\": 1 \u21d2 Number of deserializer queues \"ds_threads\": 1 \u21d2 Number of deserializer threads \"ds_pool_size\": 1 \u21d2 Deserializer pool size \"indexing_queues\": 1 \u21d2 Number of indexing queues \"ad_idx_queues\": 1 \u21d2 Multiplier for action indexing queues \"max_autoscale\": 4 \u21d2 Max number of readers to autoscale \"auto_scale_trigger\": 20000 \u21d2 Number of itens on queue to trigger autoscale \"routing_mode\": \"heatmap\" \u21d2 We recommend using the heatmap routing algorithm since it uses less memory. If you see performance issues on large chains with very unbalanced action distribution (majority of the action on a single contract) you can change it to round_robin to improve throughput. Keep in mind that the round_robin method will use more RAM as it has larger ds_pool_size values 6. Indexer configuration \u00b6 \"start_on\": 0 \u21d2 Start indexing on block (0=disable) \"stop_on\": 0 \u21d2 Stop indexing on block (0=disable) \"rewrite\": false \u21d2 Force rewrite the target replay range \"purge_queues\": true \u21d2 Clear rabbitmq queues before starting the indexer \"live_reader\": false \u21d2 Enable live reader \"live_only_mode\": false \u21d2 Only reads realtime data serially \"abi_scan_mode\": true \u21d2 Enable abi scan mode ( Indicated on the first run ) \"fetch_block\": true \u21d2 Request full blocks from the state history plugin \"fetch_traces\": true \u21d2 Request traces from the state history plugin \"disable_reading\": false \u21d2 Completely disable block reading, for lagged queue processing \"disable_indexing\": false \u21d2 Disable indexing \"process_deltas\": true \u21d2 Read table deltas \"max_inline\": 20 \u21d2 Max inline actions depth to index 7. Features \u00b6 \"index_deltas\": true \u21d2 Index common table deltas (see delta on definitions/mappings) \"index_transfer_memo\": true \u21d2 Index transfers memo \"index_all_deltas\": true \u21d2 Index all table deltas 7.1 Streaming \u00b6 Enable live streaming \"enable\": false \"traces\": false \"deltas\": false 7.2 Tables \u00b6 Tables to fetch \"proposals\": true \"accounts\": true \"voters\": true \"userres\": false \"delband\": false 8. Prefetch \u00b6 \"read\": 50 \u21d2 Stage 1 prefetch size \"block\": 100 \u21d2 Stage 2 prefetch size \"index\": 500 \u21d2 Stage 3 prefetch size Example \u00b6 Tip For multiple chains, you should have one config file for each chain. Let's suppose that we gonna start Indexing the EOS Mainnet with: Locally exposed API 2 Readers 2 Deserializer Queues Live Streaming Enabled with Traces ABI scan already done The first step is to make a copy of the config file and rename it: chains/example.config.json to chains/eos.config.json . The next step is to edit the file as the following: { \"api\" : { \"chain_name\" : \"eos\" , \"server_addr\" : \"127.0.0.1\" , \"server_port\" : 7000 , \"server_name\" : \"127.0.0.1:7000\" , \"provider_name\" : \"Example Provider\" , \"provider_url\" : \"https://example.com\" , \"chain_logo_url\" : \"\" , \"enable_caching\" : true , \"cache_life\" : 1 , \"limits\" : { \"get_actions\" : 1000 , \"get_voters\" : 100 , \"get_links\" : 1000 , \"get_deltas\" : 1000 }, \"access_log\" : false , \"enable_explorer\" : false }, \"settings\" : { \"preview\" : false , \"chain\" : \"eos\" , \"eosio_alias\" : \"eosio\" , \"parser\" : \"1.8\" , \"auto_stop\" : 300 , \"index_version\" : \"v1\" , \"debug\" : false , \"rate_monitoring\" : true , \"bp_logs\" : false , \"bp_monitoring\" : false , \"ipc_debug_rate\" : 60000 , \"allow_custom_abi\" : false }, \"blacklists\" : { \"actions\" : [], \"deltas\" : [] }, \"whitelists\" : { \"actions\" : [], \"deltas\" : [] }, \"scaling\" : { \"batch_size\" : 10000 , \"queue_limit\" : 50000 , \"readers\" : 2 , \"ds_queues\" : 2 , \"ds_threads\" : 1 , \"ds_pool_size\" : 1 , \"indexing_queues\" : 1 , \"ad_idx_queues\" : 1 , \"max_autoscale\" : 4 , \"auto_scale_trigger\" : 20000 }, \"indexer\" : { \"start_on\" : 1 , \"stop_on\" : 0 , \"rewrite\" : false , \"purge_queues\" : true , \"live_reader\" : false , \"live_only_mode\" : false , \"abi_scan_mode\" : false , \"fetch_block\" : true , \"fetch_traces\" : true , \"disable_reading\" : false , \"disable_indexing\" : false , \"process_deltas\" : true , \"max_inline\" : 20 }, \"features\" : { \"streaming\" : { \"enable\" : true , \"traces\" : true , \"deltas\" : false }, \"tables\" : { \"proposals\" : true , \"accounts\" : true , \"voters\" : true , \"userres\" : false , \"delband\" : false }, \"index_deltas\" : true , \"index_transfer_memo\" : true , \"index_all_deltas\" : true }, \"prefetch\" : { \"read\" : 50 , \"block\" : 100 , \"index\" : 500 } }","title":"Chain Configuration"},{"location":"chain/#detailed-description-of-the-chainsexampleconfigjson","text":"This section is a quick guide of the config.json file. Here you are going to find a brief explanation of each parameter with its default value and an example of real usage.","title":"Detailed description of the chains/example.config.json"},{"location":"chain/#1-api-configuration","text":"\"chain_name\": \"EXAMPLE Chain\" \"server_addr\": \"127.0.0.1\" \"server_port\": 7000 \"server_name\": \"127.0.0.1:7000\" \"provider_name\": \"Example Provider\" \"provider_url\": \"https://example.com\" \"chain_logo_url\": \"\" \"enable_caching\": true \u21d2 Set API cache \"cache_life\": 1 \u21d2 Define the cache life \"limits\" \u21d2 Set API response limits \"get_actions\": 1000 \"get_voters\": 100 \"get_links\": 1000 \"get_deltas\": 1000 \"access_log\": false \u21d2 Enable log API access. \"enable_explorer\": false","title":"1. API Configuration"},{"location":"chain/#2-settings","text":"\"preview\": false \u21d2 Preview mode - prints worker map and exit \"chain\": \"eos\" \u21d2 Chain name (The same used on ecosystem.config.js) \"eosio_alias\": \"eosio\" \"parser\": \"1.8\" \u21d2 Version of the parser to be used \"auto_stop\": 300 \u21d2 Automatically stop Indexer after X seconds if no more blocks are being processed (0=disable) \"index_version\": \"v1\" \u21d2 Set the index version \"debug\": false \u21d2 Enable debug mode \"rate_monitoring\": true \u21d2 Print data rates for each phase every 5s \"bp_logs\": false \u21d2 Enable logs \"bp_monitoring\": false \u21d2 Allow monitoring and logging of missed blocks and rounds \"ipc_debug_rate\": 60000 \u21d2 interval between IPC messaging rate debug messages, setting it to false or 0 will disable the logging \"allow_custom_abi\": false \u21d2 allow using custom ABIs from the custom-abi/ folder, they must match the pattern - - .json. Those ABIs will overwrite on-chain data for the given range.","title":"2. Settings"},{"location":"chain/#3-blacklists","text":"Blacklist for actions and deltas \"actions\": [] \"deltas\": []","title":"3. Blacklists"},{"location":"chain/#4-whitelists","text":"Whitelist for actions and deltas actions\": [] \"deltas\": []","title":"4. Whitelists"},{"location":"chain/#5-scaling-options","text":"\"batch_size\": 10000 \u21d2 Parallel reader batch size in blocks \"queue_limit\": 50000 \u21d2 Queue size limit on rabbitmq \"readers\": 1 \u21d2 Number of readers \"ds_queues\": 1 \u21d2 Number of deserializer queues \"ds_threads\": 1 \u21d2 Number of deserializer threads \"ds_pool_size\": 1 \u21d2 Deserializer pool size \"indexing_queues\": 1 \u21d2 Number of indexing queues \"ad_idx_queues\": 1 \u21d2 Multiplier for action indexing queues \"max_autoscale\": 4 \u21d2 Max number of readers to autoscale \"auto_scale_trigger\": 20000 \u21d2 Number of itens on queue to trigger autoscale \"routing_mode\": \"heatmap\" \u21d2 We recommend using the heatmap routing algorithm since it uses less memory. If you see performance issues on large chains with very unbalanced action distribution (majority of the action on a single contract) you can change it to round_robin to improve throughput. Keep in mind that the round_robin method will use more RAM as it has larger ds_pool_size values","title":"5. Scaling options"},{"location":"chain/#6-indexer-configuration","text":"\"start_on\": 0 \u21d2 Start indexing on block (0=disable) \"stop_on\": 0 \u21d2 Stop indexing on block (0=disable) \"rewrite\": false \u21d2 Force rewrite the target replay range \"purge_queues\": true \u21d2 Clear rabbitmq queues before starting the indexer \"live_reader\": false \u21d2 Enable live reader \"live_only_mode\": false \u21d2 Only reads realtime data serially \"abi_scan_mode\": true \u21d2 Enable abi scan mode ( Indicated on the first run ) \"fetch_block\": true \u21d2 Request full blocks from the state history plugin \"fetch_traces\": true \u21d2 Request traces from the state history plugin \"disable_reading\": false \u21d2 Completely disable block reading, for lagged queue processing \"disable_indexing\": false \u21d2 Disable indexing \"process_deltas\": true \u21d2 Read table deltas \"max_inline\": 20 \u21d2 Max inline actions depth to index","title":"6. Indexer configuration"},{"location":"chain/#7-features","text":"\"index_deltas\": true \u21d2 Index common table deltas (see delta on definitions/mappings) \"index_transfer_memo\": true \u21d2 Index transfers memo \"index_all_deltas\": true \u21d2 Index all table deltas","title":"7. Features"},{"location":"chain/#71-streaming","text":"Enable live streaming \"enable\": false \"traces\": false \"deltas\": false","title":"7.1 Streaming"},{"location":"chain/#72-tables","text":"Tables to fetch \"proposals\": true \"accounts\": true \"voters\": true \"userres\": false \"delband\": false","title":"7.2 Tables"},{"location":"chain/#8-prefetch","text":"\"read\": 50 \u21d2 Stage 1 prefetch size \"block\": 100 \u21d2 Stage 2 prefetch size \"index\": 500 \u21d2 Stage 3 prefetch size","title":"8. Prefetch"},{"location":"chain/#example","text":"Tip For multiple chains, you should have one config file for each chain. Let's suppose that we gonna start Indexing the EOS Mainnet with: Locally exposed API 2 Readers 2 Deserializer Queues Live Streaming Enabled with Traces ABI scan already done The first step is to make a copy of the config file and rename it: chains/example.config.json to chains/eos.config.json . The next step is to edit the file as the following: { \"api\" : { \"chain_name\" : \"eos\" , \"server_addr\" : \"127.0.0.1\" , \"server_port\" : 7000 , \"server_name\" : \"127.0.0.1:7000\" , \"provider_name\" : \"Example Provider\" , \"provider_url\" : \"https://example.com\" , \"chain_logo_url\" : \"\" , \"enable_caching\" : true , \"cache_life\" : 1 , \"limits\" : { \"get_actions\" : 1000 , \"get_voters\" : 100 , \"get_links\" : 1000 , \"get_deltas\" : 1000 }, \"access_log\" : false , \"enable_explorer\" : false }, \"settings\" : { \"preview\" : false , \"chain\" : \"eos\" , \"eosio_alias\" : \"eosio\" , \"parser\" : \"1.8\" , \"auto_stop\" : 300 , \"index_version\" : \"v1\" , \"debug\" : false , \"rate_monitoring\" : true , \"bp_logs\" : false , \"bp_monitoring\" : false , \"ipc_debug_rate\" : 60000 , \"allow_custom_abi\" : false }, \"blacklists\" : { \"actions\" : [], \"deltas\" : [] }, \"whitelists\" : { \"actions\" : [], \"deltas\" : [] }, \"scaling\" : { \"batch_size\" : 10000 , \"queue_limit\" : 50000 , \"readers\" : 2 , \"ds_queues\" : 2 , \"ds_threads\" : 1 , \"ds_pool_size\" : 1 , \"indexing_queues\" : 1 , \"ad_idx_queues\" : 1 , \"max_autoscale\" : 4 , \"auto_scale_trigger\" : 20000 }, \"indexer\" : { \"start_on\" : 1 , \"stop_on\" : 0 , \"rewrite\" : false , \"purge_queues\" : true , \"live_reader\" : false , \"live_only_mode\" : false , \"abi_scan_mode\" : false , \"fetch_block\" : true , \"fetch_traces\" : true , \"disable_reading\" : false , \"disable_indexing\" : false , \"process_deltas\" : true , \"max_inline\" : 20 }, \"features\" : { \"streaming\" : { \"enable\" : true , \"traces\" : true , \"deltas\" : false }, \"tables\" : { \"proposals\" : true , \"accounts\" : true , \"voters\" : true , \"userres\" : false , \"delband\" : false }, \"index_deltas\" : true , \"index_transfer_memo\" : true , \"index_all_deltas\" : true }, \"prefetch\" : { \"read\" : 50 , \"block\" : 100 , \"index\" : 500 } }","title":"Example"},{"location":"connections/","text":"Detailed description of the connections.json \u00b6 RabbitMQ parameters \u00b6 \"host\":\"127.0.0.1:5672\" \"api\":\"127.0.0.1:15672\" \"user\":\"my_user\" \"pass\":\"my_password\" \"vhost\":\"hyperion\" Elasticsearch parameters \u00b6 \"protocol\": \"http\" \u21d2 Protocol used to connect to Elasticsearch (default: http). \"host\":\"127.0.0.1:9200\" \"ingest_nodes\": [ \"127.0.0.1:9200\"] \"user\":\"\" \u21d2 User defined on elasticsearch configuration. \"pass\":\"\" \u21d2 Password defined on elasticsearch configuration. Redis parameters \u00b6 \"host\":\"127.0.0.1\" \"port\":\"6379\" Chain Parameters \u00b6 \"name\":\"EOS Mainnet\" \"chain_id\":\"aca376f206b8fc25a6ed44dbdc66547c36c6c33e3a119ffbeaef943642f0e906\" \"http\":\"http://127.0.0.1:8888\" \"ship\":\"ws://127.0.0.1:8080\" \"WS_ROUTER_HOST\": \"127.0.0.1\" \u21d2 Endpoint used by the Streaming API to connect to the Indexer. This is important when Indexer and API aren't on the same machine/instance. \"WS_ROUTER_PORT\":7001 \u21d2 Port used by the Streaming API to connect to the Indexer. Example \u00b6 In this example we have a connection.json file with: Local RabbitMQ user: admin pass: 123456 Local Elasticsearch no user no password Local Reddis Remote EOS Mainnet state history Remote WAX state history The first step is to make a copy of the config file and rename it: example-connections.json to connections.json . The next step is to edit the file as the follows: { \"amqp\" : { \"host\" : \"127.0.0.1:5672\" , \"api\" : \"127.0.0.1:15672\" , \"user\" : \"admin\" , \"pass\" : \"123456\" , \"vhost\" : \"hyperion\" }, \"elasticsearch\" : { \"host\" : \"127.0.0.1:9200\" , \"ingest_nodes\" : [ \"127.0.0.1:9200\" ], \"user\" : \"\" , \"pass\" : \"\" }, \"redis\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"6379\" }, \"chains\" : { \"eos\" : { \"name\" : \"EOS Mainnet\" , \"chain_id\" : \"aca376f206b8fc25a6ed44dbdc66547c36c6c33e3a119ffbeaef943642f0e906\" , \"http\" : \"http://127.0.0.1:8888\" , \"ship\" : \"ws://127.0.0.1:8080\" , \"WS_ROUTER_PORT\" : 7001 }, \"sample\" : { \"name\" : \"Sample Mainnet\" , \"chain_id\" : \"9473887b3cd1a897ce03ae5b6a865651747e2e152090f99c1d19d4adf73238fas\" , \"http\" : \"https://sample.io\" , \"ship\" : \"ws://192.168.0.1:8080\" , \"WS_ROUTER_HOST\" : \"127.0.0.1\" , \"WS_ROUTER_PORT\" : 8034 } } }","title":"Connections"},{"location":"connections/#detailed-description-of-the-connectionsjson","text":"","title":"Detailed description of the connections.json"},{"location":"connections/#rabbitmq-parameters","text":"\"host\":\"127.0.0.1:5672\" \"api\":\"127.0.0.1:15672\" \"user\":\"my_user\" \"pass\":\"my_password\" \"vhost\":\"hyperion\"","title":"RabbitMQ parameters"},{"location":"connections/#elasticsearch-parameters","text":"\"protocol\": \"http\" \u21d2 Protocol used to connect to Elasticsearch (default: http). \"host\":\"127.0.0.1:9200\" \"ingest_nodes\": [ \"127.0.0.1:9200\"] \"user\":\"\" \u21d2 User defined on elasticsearch configuration. \"pass\":\"\" \u21d2 Password defined on elasticsearch configuration.","title":"Elasticsearch parameters"},{"location":"connections/#redis-parameters","text":"\"host\":\"127.0.0.1\" \"port\":\"6379\"","title":"Redis parameters"},{"location":"connections/#chain-parameters","text":"\"name\":\"EOS Mainnet\" \"chain_id\":\"aca376f206b8fc25a6ed44dbdc66547c36c6c33e3a119ffbeaef943642f0e906\" \"http\":\"http://127.0.0.1:8888\" \"ship\":\"ws://127.0.0.1:8080\" \"WS_ROUTER_HOST\": \"127.0.0.1\" \u21d2 Endpoint used by the Streaming API to connect to the Indexer. This is important when Indexer and API aren't on the same machine/instance. \"WS_ROUTER_PORT\":7001 \u21d2 Port used by the Streaming API to connect to the Indexer.","title":"Chain Parameters"},{"location":"connections/#example","text":"In this example we have a connection.json file with: Local RabbitMQ user: admin pass: 123456 Local Elasticsearch no user no password Local Reddis Remote EOS Mainnet state history Remote WAX state history The first step is to make a copy of the config file and rename it: example-connections.json to connections.json . The next step is to edit the file as the follows: { \"amqp\" : { \"host\" : \"127.0.0.1:5672\" , \"api\" : \"127.0.0.1:15672\" , \"user\" : \"admin\" , \"pass\" : \"123456\" , \"vhost\" : \"hyperion\" }, \"elasticsearch\" : { \"host\" : \"127.0.0.1:9200\" , \"ingest_nodes\" : [ \"127.0.0.1:9200\" ], \"user\" : \"\" , \"pass\" : \"\" }, \"redis\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"6379\" }, \"chains\" : { \"eos\" : { \"name\" : \"EOS Mainnet\" , \"chain_id\" : \"aca376f206b8fc25a6ed44dbdc66547c36c6c33e3a119ffbeaef943642f0e906\" , \"http\" : \"http://127.0.0.1:8888\" , \"ship\" : \"ws://127.0.0.1:8080\" , \"WS_ROUTER_PORT\" : 7001 }, \"sample\" : { \"name\" : \"Sample Mainnet\" , \"chain_id\" : \"9473887b3cd1a897ce03ae5b6a865651747e2e152090f99c1d19d4adf73238fas\" , \"http\" : \"https://sample.io\" , \"ship\" : \"ws://192.168.0.1:8080\" , \"WS_ROUTER_HOST\" : \"127.0.0.1\" , \"WS_ROUTER_PORT\" : 8034 } } }","title":"Example"},{"location":"docker/","text":"Hyperion Docker \u00b6 Hyperion Docker is a multi-container Docker application intended to get Hyperion up and running as fast as possible. It will index data from a development chain where you can set your contracts, push some actions and see what happens when querying the Hyperion API. Warning Using Hyperion Docker is not recommended for production environments, only for testing, debugging and local networks. 1. Dependencies \u00b6 docker and docker-compose 2. RUN \u00b6 Inside the docker folder you will find everything you need to run Hyperion Docker. First of all, you need to generate the Hyperion configuration files. To do that, from the docker folder, run generate-config.sh located inside the scripts folder. You will have to pass an identifier and a name to the chain you will run. Example: ./scripts/generate-config.sh --chain eos --chain-name \"EOS Testnet\" Feel free to change the generated files in hyperion/config folder the way it suits you. For more details, please refer to the Hyperion Setup Section Now you have three options to run it: Option 1: docker-compose up \u00b6 This is the simplest way to run Hyperion. Just run sudo docker-compose up -d and after some time all necessary docker containers will be running. You can start using it as you like. Warning We recommend using the Script to run Hyperion Docker as the order in which containers are started is important. To check logs run: sudo docker-compose logs -f and to bring all containers down run: sudo docker-compose down Option 2: Script \u00b6 We created a script to start every container in a specific order to avoid problems like connection errors. From the docker folder run the start.sh script located inside the scripts folder. Example: ./scripts/start.sh --chain eos With this script, you also have the option to start the chain from a snapshot. Example ./scripts/start.sh --chain eos --snapshot snapshot-file.bin Don't forget to move the snapshot file to the eosio/data/snapshots folder. You can also use the stop.sh script to stop all or a specific service and also to bring all the containers down. Check the script usage for more information. Option 3: Manual \u00b6 If you have some experience with Docker Compose you can probably explore Hyperion Docker a bit more. We recommend starting services in the following order: redis, rabbitmq, elasticsearch, kibana, eosio-node, hyperion-indexer and hyperion-api . Wait until each of them is listening for connections before you start the next. Feel free to change the docker-compose.yml as you like. Bellow, you can find a simple example of how to control hyperion-indexer service: sudo docker-compose up --no-start sudo docker-compose start hyperion-indexer sudo docker-compose stop hyperion-indexer sudo docker-compose down It's also possible to start the chain form a snapshot passing a variable named SNAPSHOT to docker-compose up . 3. Usage \u00b6 After running docker-compose up , you should have a development chain producing blocks on a Docker container (eosio-node) as well as Hyperion Indexer and API on the other two Docker containers (hyperion-indexer and hyperion-api). EOSIO-NODE \u00b6 The port 8888 of this container is exposed so you can use it to interact with the chain. Example cleos -u http://127.0.0.1:8888 get info Hyperion API \u00b6 Perform queries on the endpoint at http://127.0.0.1:7000/ . The complete API reference can be found at API section: v2 Example curl http://127.0.0.1:7000/v2/history/get_actions Kibana \u00b6 Access http://127.0.0.1:5601/ RabbitMQ \u00b6 Access http://127.0.0.1:15672/ 4. Troubleshooting \u00b6 If you're having problems accessing Kibana or using Elasticsearch API, you could disable the xpack security on the docker-compose.yml setting it to false: xpack.security.enabled=false 5. Next steps \u00b6 Feel free to change configurations as you like. All configurations files are located in hyperion/config or eosio/config . For more details, please refer to the Hyperion Setup Section .","title":"Docker"},{"location":"docker/#hyperion-docker","text":"Hyperion Docker is a multi-container Docker application intended to get Hyperion up and running as fast as possible. It will index data from a development chain where you can set your contracts, push some actions and see what happens when querying the Hyperion API. Warning Using Hyperion Docker is not recommended for production environments, only for testing, debugging and local networks.","title":"Hyperion Docker"},{"location":"docker/#1-dependencies","text":"docker and docker-compose","title":"1. Dependencies"},{"location":"docker/#2-run","text":"Inside the docker folder you will find everything you need to run Hyperion Docker. First of all, you need to generate the Hyperion configuration files. To do that, from the docker folder, run generate-config.sh located inside the scripts folder. You will have to pass an identifier and a name to the chain you will run. Example: ./scripts/generate-config.sh --chain eos --chain-name \"EOS Testnet\" Feel free to change the generated files in hyperion/config folder the way it suits you. For more details, please refer to the Hyperion Setup Section Now you have three options to run it:","title":"2. RUN"},{"location":"docker/#option-1-docker-compose-up","text":"This is the simplest way to run Hyperion. Just run sudo docker-compose up -d and after some time all necessary docker containers will be running. You can start using it as you like. Warning We recommend using the Script to run Hyperion Docker as the order in which containers are started is important. To check logs run: sudo docker-compose logs -f and to bring all containers down run: sudo docker-compose down","title":"Option 1: docker-compose up"},{"location":"docker/#option-2-script","text":"We created a script to start every container in a specific order to avoid problems like connection errors. From the docker folder run the start.sh script located inside the scripts folder. Example: ./scripts/start.sh --chain eos With this script, you also have the option to start the chain from a snapshot. Example ./scripts/start.sh --chain eos --snapshot snapshot-file.bin Don't forget to move the snapshot file to the eosio/data/snapshots folder. You can also use the stop.sh script to stop all or a specific service and also to bring all the containers down. Check the script usage for more information.","title":"Option 2: Script"},{"location":"docker/#option-3-manual","text":"If you have some experience with Docker Compose you can probably explore Hyperion Docker a bit more. We recommend starting services in the following order: redis, rabbitmq, elasticsearch, kibana, eosio-node, hyperion-indexer and hyperion-api . Wait until each of them is listening for connections before you start the next. Feel free to change the docker-compose.yml as you like. Bellow, you can find a simple example of how to control hyperion-indexer service: sudo docker-compose up --no-start sudo docker-compose start hyperion-indexer sudo docker-compose stop hyperion-indexer sudo docker-compose down It's also possible to start the chain form a snapshot passing a variable named SNAPSHOT to docker-compose up .","title":"Option 3: Manual"},{"location":"docker/#3-usage","text":"After running docker-compose up , you should have a development chain producing blocks on a Docker container (eosio-node) as well as Hyperion Indexer and API on the other two Docker containers (hyperion-indexer and hyperion-api).","title":"3. Usage"},{"location":"docker/#eosio-node","text":"The port 8888 of this container is exposed so you can use it to interact with the chain. Example cleos -u http://127.0.0.1:8888 get info","title":"EOSIO-NODE"},{"location":"docker/#hyperion-api","text":"Perform queries on the endpoint at http://127.0.0.1:7000/ . The complete API reference can be found at API section: v2 Example curl http://127.0.0.1:7000/v2/history/get_actions","title":"Hyperion API"},{"location":"docker/#kibana","text":"Access http://127.0.0.1:5601/","title":"Kibana"},{"location":"docker/#rabbitmq","text":"Access http://127.0.0.1:15672/","title":"RabbitMQ"},{"location":"docker/#4-troubleshooting","text":"If you're having problems accessing Kibana or using Elasticsearch API, you could disable the xpack security on the docker-compose.yml setting it to false: xpack.security.enabled=false","title":"4. Troubleshooting"},{"location":"docker/#5-next-steps","text":"Feel free to change configurations as you like. All configurations files are located in hyperion/config or eosio/config . For more details, please refer to the Hyperion Setup Section .","title":"5. Next steps"},{"location":"ecosystem/","text":"Detailed description of the ecosystem.config.js \u00b6 const { addApiServer , addIndexer } = require ( \"./definitions/ecosystem_settings\" ); module . exports = { apps : [ addIndexer ( 'eos' ), // Index chain name addApiServer ( 'eos' , 1 ) // API chain name, API threads number ] }; Multiple chains configuration \u00b6 To setup multiple Indexers and/or APIs, just add then inside the apps square brackets: const { addApiServer , addIndexer } = require ( \"./definitions/ecosystem_settings\" ); module . exports = { apps : [ addIndexer ( 'eos' ), addIndexer ( 'wax' ), addIndexer ( 'bos' ), addApiServer ( 'eos' , 1 ), addApiServer ( 'bos' , 1 ) ] };","title":"Ecosystem"},{"location":"ecosystem/#detailed-description-of-the-ecosystemconfigjs","text":"const { addApiServer , addIndexer } = require ( \"./definitions/ecosystem_settings\" ); module . exports = { apps : [ addIndexer ( 'eos' ), // Index chain name addApiServer ( 'eos' , 1 ) // API chain name, API threads number ] };","title":"Detailed description of the ecosystem.config.js"},{"location":"ecosystem/#multiple-chains-configuration","text":"To setup multiple Indexers and/or APIs, just add then inside the apps square brackets: const { addApiServer , addIndexer } = require ( \"./definitions/ecosystem_settings\" ); module . exports = { apps : [ addIndexer ( 'eos' ), addIndexer ( 'wax' ), addIndexer ( 'bos' ), addApiServer ( 'eos' , 1 ), addApiServer ( 'bos' , 1 ) ] };","title":"Multiple chains configuration"},{"location":"endpoint/","text":"Hyperion Open History Endpoint List \u00b6 Tip Check the endpoint status at https://bloks.io/hyperion 1. Mainnets: \u00b6 EOS \u00b6 http://api.eossweden.org/v2/docs https://mainnet.eosn.io/v2/docs BOS \u00b6 http://api.bossweden.org/v2/docs https://bos.eosn.io/v2/docs https://bos.eosusa.news/v2/docs WAX \u00b6 https://api.waxsweden.org/v2/docs https://wax.eosusa.news/v2/docs https://wax.eosphere.io/v2/docs https://wax.pink.gg/v2/docs https://api-wax.maltablock.org/v2/docs http://api.blokcrafters.io/v2/doc https://hyperion.wax.eosdetroit.io/v2/docs TELOS \u00b6 https://mainnet.telosusa.io/v2/docs https://telos.eosphere.io/v2/docs https://api-telos-21zephyr.maltablock.org/v2/docs/ https://telos.caleos.io/v2/docs https://hyperion.telos.eosdetroit.io/v2/docs DAOBet \u00b6 https://daobet.eossweden.org/v2/docs https://daobet.eosusa.news/v2/docs/ INSTAR \u00b6 https://instar.eosusa.news/v2/docs WORBLI \u00b6 https://api.worblisweden.org/v2/docs MEETONE \u00b6 https://api.meetsweden.org/v2/docs Aikon/ORE \u00b6 https://ore.eosusa.news/v2/docs/ Coffe \u00b6 https://hyperion.coffe.io/v2/docs https://coffe.eosusa.news/v2/docs Remme \u00b6 https://rem.eon.llc/v2/docs 2. Testnets: \u00b6 Kylin EOS Testnet \u00b6 https://kylin.eosusa.news/v2/docs https://kylin.eosn.io/v2/docs https://kylin.eossweden.org/v2/docs Jungle EOS Testnet \u00b6 https://jungle.eosusa.news/v2/docs https://junglehistory.cryptolions.io/v2/docs https://jungle.eosn.io/v2/docs https://jungle.eossweden.org/v2/docs BOS Testnet \u00b6 https://tst.bossweden.org/v2/docs Telos Testnet \u00b6 https://testnet.telosusa.io/v2/docs https://testnet.telos.caleos.io/v2/docs WAX Testnet \u00b6 https://testnet.wax.pink.gg/v2/docs http://testnet.blokcrafters.io/v2/doc DAOBet Testnet \u00b6 https://daobet-test.eossweden.org/v2/docs https://test.daobet.eosusa.news/v2/docs/ Lynx Testnet \u00b6 https://tst.lynxsweden.org/v2/docs http://test.lynx.eosusa.news/v2/docs","title":"Endpoint List"},{"location":"endpoint/#hyperion-open-history-endpoint-list","text":"Tip Check the endpoint status at https://bloks.io/hyperion","title":"Hyperion Open History Endpoint List"},{"location":"endpoint/#1-mainnets","text":"","title":"1. Mainnets:"},{"location":"endpoint/#eos","text":"http://api.eossweden.org/v2/docs https://mainnet.eosn.io/v2/docs","title":"EOS"},{"location":"endpoint/#bos","text":"http://api.bossweden.org/v2/docs https://bos.eosn.io/v2/docs https://bos.eosusa.news/v2/docs","title":"BOS"},{"location":"endpoint/#wax","text":"https://api.waxsweden.org/v2/docs https://wax.eosusa.news/v2/docs https://wax.eosphere.io/v2/docs https://wax.pink.gg/v2/docs https://api-wax.maltablock.org/v2/docs http://api.blokcrafters.io/v2/doc https://hyperion.wax.eosdetroit.io/v2/docs","title":"WAX"},{"location":"endpoint/#telos","text":"https://mainnet.telosusa.io/v2/docs https://telos.eosphere.io/v2/docs https://api-telos-21zephyr.maltablock.org/v2/docs/ https://telos.caleos.io/v2/docs https://hyperion.telos.eosdetroit.io/v2/docs","title":"TELOS"},{"location":"endpoint/#daobet","text":"https://daobet.eossweden.org/v2/docs https://daobet.eosusa.news/v2/docs/","title":"DAOBet"},{"location":"endpoint/#instar","text":"https://instar.eosusa.news/v2/docs","title":"INSTAR"},{"location":"endpoint/#worbli","text":"https://api.worblisweden.org/v2/docs","title":"WORBLI"},{"location":"endpoint/#meetone","text":"https://api.meetsweden.org/v2/docs","title":"MEETONE"},{"location":"endpoint/#aikonore","text":"https://ore.eosusa.news/v2/docs/","title":"Aikon/ORE"},{"location":"endpoint/#coffe","text":"https://hyperion.coffe.io/v2/docs https://coffe.eosusa.news/v2/docs","title":"Coffe"},{"location":"endpoint/#remme","text":"https://rem.eon.llc/v2/docs","title":"Remme"},{"location":"endpoint/#2-testnets","text":"","title":"2. Testnets:"},{"location":"endpoint/#kylin-eos-testnet","text":"https://kylin.eosusa.news/v2/docs https://kylin.eosn.io/v2/docs https://kylin.eossweden.org/v2/docs","title":"Kylin EOS Testnet"},{"location":"endpoint/#jungle-eos-testnet","text":"https://jungle.eosusa.news/v2/docs https://junglehistory.cryptolions.io/v2/docs https://jungle.eosn.io/v2/docs https://jungle.eossweden.org/v2/docs","title":"Jungle EOS Testnet"},{"location":"endpoint/#bos-testnet","text":"https://tst.bossweden.org/v2/docs","title":"BOS Testnet"},{"location":"endpoint/#telos-testnet","text":"https://testnet.telosusa.io/v2/docs https://testnet.telos.caleos.io/v2/docs","title":"Telos Testnet"},{"location":"endpoint/#wax-testnet","text":"https://testnet.wax.pink.gg/v2/docs http://testnet.blokcrafters.io/v2/doc","title":"WAX Testnet"},{"location":"endpoint/#daobet-testnet","text":"https://daobet-test.eossweden.org/v2/docs https://test.daobet.eosusa.news/v2/docs/","title":"DAOBet Testnet"},{"location":"endpoint/#lynx-testnet","text":"https://tst.lynxsweden.org/v2/docs http://test.lynx.eosusa.news/v2/docs","title":"Lynx Testnet"},{"location":"faq/","text":"","title":"Faq"},{"location":"howtouse/","text":"How to use \u00b6 1. JavaScript client using https \u00b6 Fetching data from Hyperion using JavaScript is quite simple. To do that we're going to use https library to make the requests. So, the first step is to include the https lib: const https = require ( 'https' ); In this example, we will fetch the WAX chain for the transaction id: 14b36232e919307090c7e9a7a2b17915f40bf0ce72734647c9f8c145c110dda0. This is the query: \"https://wax.hyperion.eosrio.io/v2/history/get_transaction?id=14b36232e919307090c7e9a7a2b17915f40bf0ce72734647c9f8c145c110dda0\" Now, let's create a Promise function that will receive the tx id as parameter and save it into a variable called getTransaction : let getTransaction = function ( args ) { let url = \"https://wax.hyperion.eosrio.io/v2/history/get_transaction?id=\" + args ; return new Promise ( function ( resolve ) { https . get ( url , ( resp ) => { let data = '' ; // A chunk of data has been recieved. resp . on ( 'data' , ( chunk ) => { data += chunk ; }); // The whole response has been received. Print out the result. resp . on ( 'end' , () => { resolve ( JSON . parse ( data )); }); }); }) }; And finally, let's call the function passing the tx id as parameter: ( async () =>{ let id = \"14b36232e919307090c7e9a7a2b17915f40bf0ce72734647c9f8c145c110dda0\" getTransaction ( id ). then ( data => { console . log ( data ); }); })(); Request response: { trx_id: '14b36232e919307090c7e9a7a2b17915f40bf0ce72734647c9f8c145c110dda0', lib: 49925737, actions: [ { action_ordinal: 1, creator_action_ordinal: 0, act: [Object], context_free: false, elapsed: '0', account_ram_deltas: [Array], '@timestamp': '2020-04-08T23:02:02.500', block_num: 49924003, producer: 'zenblockswax', trx_id: '14b36232e919307090c7e9a7a2b17915f40bf0ce72734647c9f8c145c110dda0', global_sequence: 260804226, cpu_usage_us: 1173, net_usage_words: 36, inline_count: 3, inline_filtered: false, receipts: [Array], code_sequence: 7, abi_sequence: 7, notified: [Array], timestamp: '2020-04-08T23:02:02.500' } , { action_ordinal: 2, creator_action_ordinal: 1, act: [Object], context_free: false, elapsed: '0', account_ram_deltas: [Array], '@timestamp': '2020-04-08T23:02:02.500', block_num: 49924003, producer: 'zenblockswax', trx_id: '14b36232e919307090c7e9a7a2b17915f40bf0ce72734647c9f8c145c110dda0', global_sequence: 260804227, receipts: [Array], code_sequence: 6, abi_sequence: 6, notified: [Array], timestamp: '2020-04-08T23:02:02.500' } ], query_time_ms: 45.26 } 2. Third party library \u00b6 There is a third party Javascript library made by EOS Cafe. Refer to their github for further information: https://github.com/eoscafe/hyperion-api Note This is a third party library and is not maintained by EOS Rio","title":"Getting Started"},{"location":"howtouse/#how-to-use","text":"","title":"How to use"},{"location":"howtouse/#1-javascript-client-using-https","text":"Fetching data from Hyperion using JavaScript is quite simple. To do that we're going to use https library to make the requests. So, the first step is to include the https lib: const https = require ( 'https' ); In this example, we will fetch the WAX chain for the transaction id: 14b36232e919307090c7e9a7a2b17915f40bf0ce72734647c9f8c145c110dda0. This is the query: \"https://wax.hyperion.eosrio.io/v2/history/get_transaction?id=14b36232e919307090c7e9a7a2b17915f40bf0ce72734647c9f8c145c110dda0\" Now, let's create a Promise function that will receive the tx id as parameter and save it into a variable called getTransaction : let getTransaction = function ( args ) { let url = \"https://wax.hyperion.eosrio.io/v2/history/get_transaction?id=\" + args ; return new Promise ( function ( resolve ) { https . get ( url , ( resp ) => { let data = '' ; // A chunk of data has been recieved. resp . on ( 'data' , ( chunk ) => { data += chunk ; }); // The whole response has been received. Print out the result. resp . on ( 'end' , () => { resolve ( JSON . parse ( data )); }); }); }) }; And finally, let's call the function passing the tx id as parameter: ( async () =>{ let id = \"14b36232e919307090c7e9a7a2b17915f40bf0ce72734647c9f8c145c110dda0\" getTransaction ( id ). then ( data => { console . log ( data ); }); })(); Request response: { trx_id: '14b36232e919307090c7e9a7a2b17915f40bf0ce72734647c9f8c145c110dda0', lib: 49925737, actions: [ { action_ordinal: 1, creator_action_ordinal: 0, act: [Object], context_free: false, elapsed: '0', account_ram_deltas: [Array], '@timestamp': '2020-04-08T23:02:02.500', block_num: 49924003, producer: 'zenblockswax', trx_id: '14b36232e919307090c7e9a7a2b17915f40bf0ce72734647c9f8c145c110dda0', global_sequence: 260804226, cpu_usage_us: 1173, net_usage_words: 36, inline_count: 3, inline_filtered: false, receipts: [Array], code_sequence: 7, abi_sequence: 7, notified: [Array], timestamp: '2020-04-08T23:02:02.500' } , { action_ordinal: 2, creator_action_ordinal: 1, act: [Object], context_free: false, elapsed: '0', account_ram_deltas: [Array], '@timestamp': '2020-04-08T23:02:02.500', block_num: 49924003, producer: 'zenblockswax', trx_id: '14b36232e919307090c7e9a7a2b17915f40bf0ce72734647c9f8c145c110dda0', global_sequence: 260804227, receipts: [Array], code_sequence: 6, abi_sequence: 6, notified: [Array], timestamp: '2020-04-08T23:02:02.500' } ], query_time_ms: 45.26 }","title":"1. JavaScript client using https"},{"location":"howtouse/#2-third-party-library","text":"There is a third party Javascript library made by EOS Cafe. Refer to their github for further information: https://github.com/eoscafe/hyperion-api Note This is a third party library and is not maintained by EOS Rio","title":"2. Third party library"},{"location":"hyperion/","text":"Hyperion Setup \u00b6 1. Clone & Install packages \u00b6 git clone https://github.com/eosrio/hyperion-history-api.git cd hyperion-history-api npm install Tip if you came from the script section , you can skip this step and go directly to the Edit configs step below. 2. Edit configs \u00b6 cp example-ecosystem.config.js ecosystem.config.js nano ecosystem.config.js # Enter connection details here (chain name must match on the ecosystem file) cp example-connections.json connections.json nano connections.json connections.json Reference \u00b6 { \"amqp\" :{ \"host\" : \"127.0.0.1:5672\" , \"api\" : \"127.0.0.1:15672\" , \"user\" : \"my_user\" , \"pass\" : \"my_password\" , \"vhost\" : \"hyperion\" }, \"elasticsearch\" :{ \"host\" : \"127.0.0.1:9200\" , \"ingest_nodes\" :[ \"127.0.0.1:9200\" ], \"user\" : \"\" , \"pass\" : \"\" }, \"redis\" :{ \"host\" : \"127.0.0.1\" , \"port\" : \"6379\" }, \"chains\" :{ \"eos\" :{ \"name\" : \"EOS Mainnet\" , \"chain_id\" : \"aca376f206b8fc25a6ed44dbdc66547c36c6c33e3a119ffbeaef943642f0e906\" , \"http\" : \"http://127.0.0.1:8888\" , \"ship\" : \"ws://127.0.0.1:8080\" , \"WS_ROUTER_PORT\" : 7001 } } } For more details, refer to the connections section ecosystem.config.js Reference \u00b6 module . exports = { apps : [ addIndexer ( 'eos' ), addApiServer ( 'eos' , 1 ) ] }; For more details, refer to the ecosystem section 3. Setup \u00b6 cp chains/example.config.json chains/CHAIN_NAME.config.json Example: cp chains/example.config.json chains/eos.config.json The default config.json file is ready to run. The parameter abi_scan_mode is true to perform an abi scan on the first run. For more details, refer to the chain section 4. Start and Stop \u00b6 We provide scripts to simplify the process of starting and stopping your Hyperion Indexer or API instance. But, you can also do it manually if you prefer. This section will cover both ways: Option 1: Using run / stop script \u00b6 run script \u00b6 You can use run script to start the Indexer or the API. ./run.sh chain-indexer ./run.sh chain-api Examples Start indexing EOS mainnet: ./run.sh eos-indexer Start EOS API: ./run.sh eos-api Remember that the chain name was previously defined in the setup step . stop script \u00b6 Example Stop the EOS mainnet indexer: ./stop.sh eos-indexer Note The stop script won't stop Hyperion Indexer immediately , it will first flush the queues. This operation could take some time. If you want to stop immediately, you need to run the \"force stop command\" explained below. Option 2: Commands \u00b6 Start indexing \u00b6 pm2 start --only chain-indexer --update-env pm2 logs chain-indexer Stop reading and wait for queues to flush \u00b6 pm2 trigger chain-indexer stop Force stop \u00b6 pm2 stop chain-indexer Starting the API node \u00b6 pm2 start --only chain-api --update-env pm2 logs chain-api 5. Indexer \u00b6 As mentioned before on Setup , the Hyperion Indexer is configured to perform an abi scan (\"abi_scan_mode\": true) as default. So, on your first run, you'll probably see something like this: This an example of an ABI SCAN on the WAX chain. Where: W (Workers): Number of workers. R (Read): Blocks read from state history and pushing into the blocks queue. C (Consumed): Blocks consumed from blocks queue. A (Actions): Actions being read out of processed blocks. D (Deserialized): Deserializations of the actions. I (Indexed): Indexing of all of the docs. 6. API \u00b6 After running the api, you should see a log like this: Now, it's time to play around making some queries. API Reference \u00b6 API Reference: API section: v2 Example: OpenAPI Docs","title":"Hyperion Setup"},{"location":"hyperion/#hyperion-setup","text":"","title":"Hyperion Setup"},{"location":"hyperion/#1-clone-install-packages","text":"git clone https://github.com/eosrio/hyperion-history-api.git cd hyperion-history-api npm install Tip if you came from the script section , you can skip this step and go directly to the Edit configs step below.","title":"1. Clone &amp; Install packages"},{"location":"hyperion/#2-edit-configs","text":"cp example-ecosystem.config.js ecosystem.config.js nano ecosystem.config.js # Enter connection details here (chain name must match on the ecosystem file) cp example-connections.json connections.json nano connections.json","title":"2. Edit configs"},{"location":"hyperion/#connectionsjson-reference","text":"{ \"amqp\" :{ \"host\" : \"127.0.0.1:5672\" , \"api\" : \"127.0.0.1:15672\" , \"user\" : \"my_user\" , \"pass\" : \"my_password\" , \"vhost\" : \"hyperion\" }, \"elasticsearch\" :{ \"host\" : \"127.0.0.1:9200\" , \"ingest_nodes\" :[ \"127.0.0.1:9200\" ], \"user\" : \"\" , \"pass\" : \"\" }, \"redis\" :{ \"host\" : \"127.0.0.1\" , \"port\" : \"6379\" }, \"chains\" :{ \"eos\" :{ \"name\" : \"EOS Mainnet\" , \"chain_id\" : \"aca376f206b8fc25a6ed44dbdc66547c36c6c33e3a119ffbeaef943642f0e906\" , \"http\" : \"http://127.0.0.1:8888\" , \"ship\" : \"ws://127.0.0.1:8080\" , \"WS_ROUTER_PORT\" : 7001 } } } For more details, refer to the connections section","title":"connections.json Reference"},{"location":"hyperion/#ecosystemconfigjs-reference","text":"module . exports = { apps : [ addIndexer ( 'eos' ), addApiServer ( 'eos' , 1 ) ] }; For more details, refer to the ecosystem section","title":"ecosystem.config.js Reference"},{"location":"hyperion/#3-setup","text":"cp chains/example.config.json chains/CHAIN_NAME.config.json Example: cp chains/example.config.json chains/eos.config.json The default config.json file is ready to run. The parameter abi_scan_mode is true to perform an abi scan on the first run. For more details, refer to the chain section","title":"3. Setup"},{"location":"hyperion/#4-start-and-stop","text":"We provide scripts to simplify the process of starting and stopping your Hyperion Indexer or API instance. But, you can also do it manually if you prefer. This section will cover both ways:","title":"4. Start and Stop"},{"location":"hyperion/#option-1-using-run-stop-script","text":"","title":"Option 1: Using run / stop script"},{"location":"hyperion/#run-script","text":"You can use run script to start the Indexer or the API. ./run.sh chain-indexer ./run.sh chain-api Examples Start indexing EOS mainnet: ./run.sh eos-indexer Start EOS API: ./run.sh eos-api Remember that the chain name was previously defined in the setup step .","title":"run script"},{"location":"hyperion/#stop-script","text":"Example Stop the EOS mainnet indexer: ./stop.sh eos-indexer Note The stop script won't stop Hyperion Indexer immediately , it will first flush the queues. This operation could take some time. If you want to stop immediately, you need to run the \"force stop command\" explained below.","title":"stop script"},{"location":"hyperion/#option-2-commands","text":"","title":"Option 2: Commands"},{"location":"hyperion/#start-indexing","text":"pm2 start --only chain-indexer --update-env pm2 logs chain-indexer","title":"Start indexing"},{"location":"hyperion/#stop-reading-and-wait-for-queues-to-flush","text":"pm2 trigger chain-indexer stop","title":"Stop reading and wait for queues to flush"},{"location":"hyperion/#force-stop","text":"pm2 stop chain-indexer","title":"Force stop"},{"location":"hyperion/#starting-the-api-node","text":"pm2 start --only chain-api --update-env pm2 logs chain-api","title":"Starting the API node"},{"location":"hyperion/#5-indexer","text":"As mentioned before on Setup , the Hyperion Indexer is configured to perform an abi scan (\"abi_scan_mode\": true) as default. So, on your first run, you'll probably see something like this: This an example of an ABI SCAN on the WAX chain. Where: W (Workers): Number of workers. R (Read): Blocks read from state history and pushing into the blocks queue. C (Consumed): Blocks consumed from blocks queue. A (Actions): Actions being read out of processed blocks. D (Deserialized): Deserializations of the actions. I (Indexed): Indexing of all of the docs.","title":"5. Indexer"},{"location":"hyperion/#6-api","text":"After running the api, you should see a log like this: Now, it's time to play around making some queries.","title":"6. API"},{"location":"hyperion/#api-reference","text":"API Reference: API section: v2 Example: OpenAPI Docs","title":"API Reference"},{"location":"install/","text":"Manual Installation \u00b6 Dependencies \u00b6 Recommended OS: Ubuntu 18.04 Elasticsearch 7.6.X RabbitMQ Node.js v13 PM2 Nodeos 1.8+ w/ state_history_plugin and chain_api_plugin Redis (only for the API caching layer) Note The indexer requires pm2 and node.js to be on the same machine. The other dependencies (Elasticsearch, RabbitMQ and Nodeos) can be installed on other machines, preferably on a high speed and low latency network. Indexing speed will vary greatly depending on this configuration. Elasticsearch Installation \u00b6 Info Follow the detailed installation instructions on the official elasticsearch documentation 1. Edit /etc/elasticsearch/elasticsearch.yml \u00b6 cluster.name: myCluster bootstrap.memory_lock: true Warning Setting bootstrap.memory_lock: true will make elasticsearch try to use all the RAM configured for JVM on startup (check next step). This could crash if you allocate more RAM than available on the system. Setting mem_lock as false with swap disabled might cause the JVM or shell session to exit if elasticsearch tries to allocate more memory than is available! Testing After starting Elasticsearch, you can see whether this setting was applied successfully by checking the value of mlockall in the output from this request: GET _nodes?filter_path=**.mlockall 2. Edit /etc/elasticsearch/jvm.options \u00b6 Warning Avoid allocating more than 31GB when setting your heap size, even if you have enough RAM. Testing You can test on your system by running the following command with the desired size (change -Xmx32g ): java -Xmx32g -XX:+UseCompressedOops -XX:+PrintFlagsFinal Oops | grep Oops Check if UseCompressedOops is true on the results for a valid optimized heap size. After that, edit the following lines on jvm.options , note that Xms and Xmx must have the same value. -Xms16g -Xmx16g 3. Allow memlock \u00b6 run sudo systemctl edit elasticsearch and add the following lines: [Service] LimitMEMLOCK=infinity 4. Start elasticsearch and check the logs (verify if the memory lock was successful) \u00b6 sudo service elasticsearch start sudo less /var/log/elasticsearch/myCluster.log sudo systemctl enable elasticsearch 5. Test the REST API \u00b6 curl http://localhost:9200 The expeted result should be something like this: { \"name\" : \"ip-172-31-5-121\" , \"cluster_name\" : \"hyperion\" , \"cluster_uuid\" : \"....\" , \"version\" : { \"number\" : \"7.1.0\" , \"build_flavor\" : \"default\" , \"build_type\" : \"deb\" , \"build_hash\" : \"606a173\" , \"build_date\" : \"2019-05-16T00:43:15.323135Z\" , \"build_snapshot\" : false , \"lucene_version\" : \"8.0.0\" , \"minimum_wire_compatibility_version\" : \"6.8.0\" , \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" }, \"tagline\" : \"You Know, for Search\" } 6. Security \u00b6 By default, elasticsearch comes without a password configured. To avoid security problems, we recommend to enable the security pack on elasticsearch. To do that, add xpack.security.enabled: true to the end of /etc/elasticsearch/elasticsearch.yml file. Now it\u2019s time to set the passwords for the cluster: sudo /usr/share/elasticsearch/bin/elasticsearch-setup-passwords auto You can alternatively skip the auto parameter to manually define your passwords using the interactive parameter. Keep track of these passwords, we\u2019ll need them again soon. RabbitMQ Installation \u00b6 Info Follow the detailed installation instructions on the official RabbitMQ documentation 1. Enable the WebUI \u00b6 sudo rabbitmq-plugins enable rabbitmq_management 2. Add vhost \u00b6 sudo rabbitmqctl add_vhost /hyperion 3. Create a user and password \u00b6 sudo rabbitmqctl add_user { my_user } { my_password } 4. Set the user as administrator \u00b6 sudo rabbitmqctl set_user_tags { my_user } administrator 5. Set the user permissions to the vhost \u00b6 sudo rabbitmqctl set_permissions -p /hyperion { my_user } \".*\" \".*\" \".*\" 6. Check access to the WebUI \u00b6 http://localhost:15672 Redis Installation \u00b6 1. Install \u00b6 sudo apt install redis-server 2. Edit /etc/redis/redis.conf \u00b6 Change supervised to systemd Note By default, redis binds to the localhost address. You need to edit bind in the config file if you want to listen to other network. 3. Restart redis \u00b6 sudo systemctl restart redis.service NodeJS \u00b6 1. Add the nodejs source \u00b6 curl -sL https://deb.nodesource.com/setup_13.x | sudo -E bash - 2. Install nodejs \u00b6 sudo apt-get install -y nodejs PM2 \u00b6 1. Install \u00b6 sudo npm install pm2@latest -g 2. Configure for system startup \u00b6 sudo pm2 startup Kibana Installation \u00b6 Info Follow the detailed installation instructions on the official documentation If you have enabled the security pack on elastic, you'll need to setup the password on Kibana. Edit the /etc/kibana/kibana.yml file, find the lines that look like this: #elasticsearch.username: \"user\" #elasticsearch.password: \"pass\" Uncomment the username and password fields by removing the # character at the beginning of the line. Change \"user\" to \"kibana\" and then change \"pass\" to whatever the setup-passwords command tells us the Kibana password is. Save the file, then we can restart Kibana. systemctl restart kibana nodeos config.ini \u00b6 state-history-dir = \"state-history\" trace-history = true chain-state-history = true state-history-endpoint = 127.0.0.1:8080 plugin = eosio::state_history_plugin Tip On EOSIO version higher or equal to 2.0.x, use wasm-runtime = eos-vm-jit to improve performance. If everything runs smoothly, now it's time to install hyperion !","title":"Manual"},{"location":"install/#manual-installation","text":"","title":"Manual Installation"},{"location":"install/#dependencies","text":"Recommended OS: Ubuntu 18.04 Elasticsearch 7.6.X RabbitMQ Node.js v13 PM2 Nodeos 1.8+ w/ state_history_plugin and chain_api_plugin Redis (only for the API caching layer) Note The indexer requires pm2 and node.js to be on the same machine. The other dependencies (Elasticsearch, RabbitMQ and Nodeos) can be installed on other machines, preferably on a high speed and low latency network. Indexing speed will vary greatly depending on this configuration.","title":"Dependencies"},{"location":"install/#elasticsearch-installation","text":"Info Follow the detailed installation instructions on the official elasticsearch documentation","title":"Elasticsearch Installation"},{"location":"install/#1-edit-etcelasticsearchelasticsearchyml","text":"cluster.name: myCluster bootstrap.memory_lock: true Warning Setting bootstrap.memory_lock: true will make elasticsearch try to use all the RAM configured for JVM on startup (check next step). This could crash if you allocate more RAM than available on the system. Setting mem_lock as false with swap disabled might cause the JVM or shell session to exit if elasticsearch tries to allocate more memory than is available! Testing After starting Elasticsearch, you can see whether this setting was applied successfully by checking the value of mlockall in the output from this request: GET _nodes?filter_path=**.mlockall","title":"1. Edit /etc/elasticsearch/elasticsearch.yml"},{"location":"install/#2-edit-etcelasticsearchjvmoptions","text":"Warning Avoid allocating more than 31GB when setting your heap size, even if you have enough RAM. Testing You can test on your system by running the following command with the desired size (change -Xmx32g ): java -Xmx32g -XX:+UseCompressedOops -XX:+PrintFlagsFinal Oops | grep Oops Check if UseCompressedOops is true on the results for a valid optimized heap size. After that, edit the following lines on jvm.options , note that Xms and Xmx must have the same value. -Xms16g -Xmx16g","title":"2. Edit /etc/elasticsearch/jvm.options"},{"location":"install/#3-allow-memlock","text":"run sudo systemctl edit elasticsearch and add the following lines: [Service] LimitMEMLOCK=infinity","title":"3. Allow memlock"},{"location":"install/#4-start-elasticsearch-and-check-the-logs-verify-if-the-memory-lock-was-successful","text":"sudo service elasticsearch start sudo less /var/log/elasticsearch/myCluster.log sudo systemctl enable elasticsearch","title":"4. Start elasticsearch and check the logs (verify if the memory lock was successful)"},{"location":"install/#5-test-the-rest-api","text":"curl http://localhost:9200 The expeted result should be something like this: { \"name\" : \"ip-172-31-5-121\" , \"cluster_name\" : \"hyperion\" , \"cluster_uuid\" : \"....\" , \"version\" : { \"number\" : \"7.1.0\" , \"build_flavor\" : \"default\" , \"build_type\" : \"deb\" , \"build_hash\" : \"606a173\" , \"build_date\" : \"2019-05-16T00:43:15.323135Z\" , \"build_snapshot\" : false , \"lucene_version\" : \"8.0.0\" , \"minimum_wire_compatibility_version\" : \"6.8.0\" , \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" }, \"tagline\" : \"You Know, for Search\" }","title":"5. Test the REST API"},{"location":"install/#6-security","text":"By default, elasticsearch comes without a password configured. To avoid security problems, we recommend to enable the security pack on elasticsearch. To do that, add xpack.security.enabled: true to the end of /etc/elasticsearch/elasticsearch.yml file. Now it\u2019s time to set the passwords for the cluster: sudo /usr/share/elasticsearch/bin/elasticsearch-setup-passwords auto You can alternatively skip the auto parameter to manually define your passwords using the interactive parameter. Keep track of these passwords, we\u2019ll need them again soon.","title":"6. Security"},{"location":"install/#rabbitmq-installation","text":"Info Follow the detailed installation instructions on the official RabbitMQ documentation","title":"RabbitMQ Installation"},{"location":"install/#1-enable-the-webui","text":"sudo rabbitmq-plugins enable rabbitmq_management","title":"1. Enable the WebUI"},{"location":"install/#2-add-vhost","text":"sudo rabbitmqctl add_vhost /hyperion","title":"2. Add vhost"},{"location":"install/#3-create-a-user-and-password","text":"sudo rabbitmqctl add_user { my_user } { my_password }","title":"3. Create a user and password"},{"location":"install/#4-set-the-user-as-administrator","text":"sudo rabbitmqctl set_user_tags { my_user } administrator","title":"4. Set the user as administrator"},{"location":"install/#5-set-the-user-permissions-to-the-vhost","text":"sudo rabbitmqctl set_permissions -p /hyperion { my_user } \".*\" \".*\" \".*\"","title":"5. Set the user permissions to the vhost"},{"location":"install/#6-check-access-to-the-webui","text":"http://localhost:15672","title":"6. Check access to the WebUI"},{"location":"install/#redis-installation","text":"","title":"Redis Installation"},{"location":"install/#1-install","text":"sudo apt install redis-server","title":"1. Install"},{"location":"install/#2-edit-etcredisredisconf","text":"Change supervised to systemd Note By default, redis binds to the localhost address. You need to edit bind in the config file if you want to listen to other network.","title":"2. Edit /etc/redis/redis.conf"},{"location":"install/#3-restart-redis","text":"sudo systemctl restart redis.service","title":"3. Restart redis"},{"location":"install/#nodejs","text":"","title":"NodeJS"},{"location":"install/#1-add-the-nodejs-source","text":"curl -sL https://deb.nodesource.com/setup_13.x | sudo -E bash -","title":"1. Add the nodejs source"},{"location":"install/#2-install-nodejs","text":"sudo apt-get install -y nodejs","title":"2. Install nodejs"},{"location":"install/#pm2","text":"","title":"PM2"},{"location":"install/#1-install_1","text":"sudo npm install pm2@latest -g","title":"1. Install"},{"location":"install/#2-configure-for-system-startup","text":"sudo pm2 startup","title":"2. Configure for system startup"},{"location":"install/#kibana-installation","text":"Info Follow the detailed installation instructions on the official documentation If you have enabled the security pack on elastic, you'll need to setup the password on Kibana. Edit the /etc/kibana/kibana.yml file, find the lines that look like this: #elasticsearch.username: \"user\" #elasticsearch.password: \"pass\" Uncomment the username and password fields by removing the # character at the beginning of the line. Change \"user\" to \"kibana\" and then change \"pass\" to whatever the setup-passwords command tells us the Kibana password is. Save the file, then we can restart Kibana. systemctl restart kibana","title":"Kibana Installation"},{"location":"install/#nodeos-configini","text":"state-history-dir = \"state-history\" trace-history = true chain-state-history = true state-history-endpoint = 127.0.0.1:8080 plugin = eosio::state_history_plugin Tip On EOSIO version higher or equal to 2.0.x, use wasm-runtime = eos-vm-jit to improve performance. If everything runs smoothly, now it's time to install hyperion !","title":"nodeos config.ini"},{"location":"kibana/","text":"The purpose here is to guide you through some basic steps using and configuring the Kibana. For more detailed information, please, refer to the official documentation . Running Kibana with systemd \u00b6 To configure Kibana to start automatically when the system boots up, run the following commands: sudo /bin/systemctl daemon-reload sudo /bin/systemctl enable kibana.service Kibana can be started and stopped as follows: sudo systemctl start kibana.service sudo systemctl stop kibana.service These commands provide no feedback as to whether Kibana was started successfully or not. Log information can be accessed via journalctl -u kibana.service. Opening Kibana \u00b6 Open http://localhost:5601 and check if you can access Kibana. If Kibana asks for credentials, the default user and password is: user: elastic password: changeme If you installed Kibana through the install_infra.sh script, check your elastic_pass.txt file. If you can't access, check your credentials on your config file. Creating index pattern \u00b6 To create a new index pattern, go to Management Click on Index Patterns at the left menu Click on Create index pattern Enter desired index pattern and click on > Next step Tip Index Pattern List: eos-abi-* eos-action-* eos-block-* eos-logs-* eos-delta-* Where eos is the name of the chain. Select a time filter, if there are any, and click on Create index pattern","title":"Kibana"},{"location":"kibana/#running-kibana-with-systemd","text":"To configure Kibana to start automatically when the system boots up, run the following commands: sudo /bin/systemctl daemon-reload sudo /bin/systemctl enable kibana.service Kibana can be started and stopped as follows: sudo systemctl start kibana.service sudo systemctl stop kibana.service These commands provide no feedback as to whether Kibana was started successfully or not. Log information can be accessed via journalctl -u kibana.service.","title":"Running Kibana with systemd"},{"location":"kibana/#opening-kibana","text":"Open http://localhost:5601 and check if you can access Kibana. If Kibana asks for credentials, the default user and password is: user: elastic password: changeme If you installed Kibana through the install_infra.sh script, check your elastic_pass.txt file. If you can't access, check your credentials on your config file.","title":"Opening Kibana"},{"location":"kibana/#creating-index-pattern","text":"To create a new index pattern, go to Management Click on Index Patterns at the left menu Click on Create index pattern Enter desired index pattern and click on > Next step Tip Index Pattern List: eos-abi-* eos-action-* eos-block-* eos-logs-* eos-delta-* Where eos is the name of the chain. Select a time filter, if there are any, and click on Create index pattern","title":"Creating index pattern"},{"location":"quickstart/","text":"Tip The usage of this installation script is highly recommended for fresh installs. If you already have dependencies installed, update them manually before running the script. Note For Windows installation using Multipass, refer to this guide We provide an automated shell script that installs all dependencies and then configure Hyperion. The first step is to clone the Hyperion repository: git clone https://github.com/eosrio/hyperion-history-api.git cd hyperion-history-api Then, you just need to run the script: ./install_env.sh The script will ask you for some information: Enter rabbitmq user [hyperion]: Enter the desired rabbitmq user and hit enter. If you leave it blank, the default user hyperion will be set. Then, do the same for the rabbitmq password: Enter rabbitmq password [123456]: And finally, it will ask if you want to create the npm global folder: Do you want to create a directory for npm global installations [Y/n] : This is recommended. If you choose n , global npm packages will require root permissions. Now, the script will do the work, this can take a while. Get a cup of coffee and relax. Info The installation script may ask you for the admin password. This is needed to install the dependencies, please, provide it. If everything runs smoothly, the script will automatically generate elasticsearch passwords and save them on the elastic_pass.txt file. To login into Kibana, use the elastic user credentials. Now it's time to install hyperion !","title":"Script"},{"location":"rabbit/","text":"RabbitMQ \u00b6 How to Delete all the queues from RabbitMQ? \u00b6 1. Using Policies - Management Console \u00b6 Go to Management Console Click on Admin tab Policies tab (on the right side) Add Policy Fill Fields Virtual Host: Select (Default is /hyperion) Name: Expire All Policies(Delete Later) Pattern: .* Apply to: Queues Definition: expires with value 1 (change type from String to Number) Click on Add / update policy Checkout Queues tab again, all queues must be deleted. Warning You must remove this policy after this operation. 2. Using command line \u00b6 First, list your queues: rabbitmqadmin list queues name Then from the list, you'll need to manually delete them one by one: rabbitmqadmin delete queue name = 'queuename' Because of the output format, doesn't appear you can grep the response from list queues . Warning Be sure you really want to do this! This gonna reset all your rabbitMQ configuration, and return it to the default state.","title":"RabbitMQ"},{"location":"rabbit/#rabbitmq","text":"","title":"RabbitMQ"},{"location":"rabbit/#how-to-delete-all-the-queues-from-rabbitmq","text":"","title":"How to Delete all the queues from RabbitMQ?"},{"location":"rabbit/#1-using-policies-management-console","text":"Go to Management Console Click on Admin tab Policies tab (on the right side) Add Policy Fill Fields Virtual Host: Select (Default is /hyperion) Name: Expire All Policies(Delete Later) Pattern: .* Apply to: Queues Definition: expires with value 1 (change type from String to Number) Click on Add / update policy Checkout Queues tab again, all queues must be deleted. Warning You must remove this policy after this operation.","title":"1. Using Policies - Management Console"},{"location":"rabbit/#2-using-command-line","text":"First, list your queues: rabbitmqadmin list queues name Then from the list, you'll need to manually delete them one by one: rabbitmqadmin delete queue name = 'queuename' Because of the output format, doesn't appear you can grep the response from list queues . Warning Be sure you really want to do this! This gonna reset all your rabbitMQ configuration, and return it to the default state.","title":"2. Using command line"},{"location":"stream_client/","text":"Hyperion Stream Client \u00b6 Streaming Client for Hyperion History API (v3+) 1. Usage \u00b6 We currently provide libraries for nodejs and prebuilt browser bundle npm package \u00b6 npm install @eosrio/hyperion-stream-client --save Import the client const HyperionSocketClient = require('@eosrio/hyperion-stream-client').default; Browser library \u00b6 <script src=\"https://<ENDPOINT>/stream-client.js\"></script> Where <ENDPOINT> is the Hyperion API (e.g. https://wax.hyperion.eosrio.io ) For other usages the bundle is also available at dist/bundle.js 2. Connection \u00b6 Setup the endpoint that you want to fetch data from and the flow control mode: const client = new HyperionSocketClient ( ENDPOINT , { async : false }); Example const client = new HyperionSocketClient ( 'https://example.com' , { async : false }); https://example.com is the host, from where https://example.com/v2/history/... is served. 2.1 Flow control mode \u00b6 Async: true - The transmission will be asynchronous and you need an acknowledge function. Incoming data will be held in a local queue until ack is called. Async: false - The transmission will be synchronous. The acknowledge function is not needed. 3. Requests \u00b6 client.streamActions(request: StreamActionsRequest): void -- Request action stream client.streamDeltas(request: StreamDeltasRequest): void -- Request delta stream (contract rows) to ensure the client is connected, requests should be defined on the client.onConnect property, refer to examples below; 3.1 Action Stream - client.streamActions \u00b6 contract - contract account action - action name account - notified account name start_from - start reading on block or on a specific date: (0=disabled) read_until - stop reading on block (0=disable) or on a specific date (0=disabled) filters - actions filter (more details below) Notes Block number can be either positive or negative - E.g.: -15000, 700 In case of negative block number, it will be subtracted from the HEAD Date format (ISO 8601) - e.g. 2020-01-01T00:00:00.000Z const HyperionSocketClient = require ( '@eosrio/hyperion-stream-client' ). default ; const client = new HyperionSocketClient ( 'http://localhost:7000' , { async : true }); client . onConnect = () => { client . streamActions ({ contract : 'eosio' , action : 'voteproducer' , account : '' , start_from : '2020-03-15T00:00:00.000Z' , read_until : 0 , filters : [], }); } // see 3 for handling data client . onData = async ( data , ack ) => { console . log ( data ); // process incoming data, replace with your code ack (); // ACK when done } client . connect (() => { console . log ( 'connected!' ); }); 3.1.1 Act Data Filters \u00b6 You can set up filters to refine your stream. Filters should use fields following the Hyperion Action Data Structure, such as: act.data.producers (on eosio::voteproducer) @transfer.to (here the @ prefix is required since transfers have special mappings) please refer to the mapping definitions to know which data fields are available Example Filter the stream for every transfer made to the eosio.ramfee account: client . streamActions ({ contract : 'eosio.token' , action : 'transfer' , account : 'eosio' , start_from : 0 , read_until : 0 , filters : [ { field : '@transfer.to' , value : 'eosio.ramfee' } ], }); To refine even more your stream, you could add more filters. Attention Remember that adding more filters will result in AND operations. Currently, it's not possible to make OR operations with filters. 3.2 Delta Stream - client.streamDeltas \u00b6 code - contract account table - table name scope - table scope payer - ram payer start_from - start reading on block or on a specific date: (0=disabled) read_until - stop reading on block (0=disable) or on a specific date (0=disabled) Example Referring to the same pattern as the action stream example above, one could also include a delta stream request client . streamDeltas ({ code : 'eosio.token' , table : '*' , scope : '' , payer : '' , start_from : 0 , read_until : 0 , }); Note Delta filters are planned to be implemented soon. 4. Handling Data \u00b6 Incoming data is handled via the client.onData callback data object is structured as follows: type - action | delta mode - live | history content - Hyperion Data Structure (see action index template) client . onData = async ( data , ack ) => { console . log ( data ); // process incoming data, replace with your code ack (); // ACK when done } On every LIB update, client.onLIB is called client . onLIB = async ( data ) => { console . log ( data ); } You can also look for forks, using the client.onFork callback client . onFork = async ( data ) => { console . log ( data ); // process when a there is a fork } If you set async: false on the connection step, the ack must not be used: client . onData = async ( data ) => { //code here }","title":"Stream Client"},{"location":"stream_client/#hyperion-stream-client","text":"Streaming Client for Hyperion History API (v3+)","title":"Hyperion Stream Client"},{"location":"stream_client/#1-usage","text":"We currently provide libraries for nodejs and prebuilt browser bundle","title":"1. Usage"},{"location":"stream_client/#npm-package","text":"npm install @eosrio/hyperion-stream-client --save Import the client const HyperionSocketClient = require('@eosrio/hyperion-stream-client').default;","title":"npm package"},{"location":"stream_client/#browser-library","text":"<script src=\"https://<ENDPOINT>/stream-client.js\"></script> Where <ENDPOINT> is the Hyperion API (e.g. https://wax.hyperion.eosrio.io ) For other usages the bundle is also available at dist/bundle.js","title":"Browser library"},{"location":"stream_client/#2-connection","text":"Setup the endpoint that you want to fetch data from and the flow control mode: const client = new HyperionSocketClient ( ENDPOINT , { async : false }); Example const client = new HyperionSocketClient ( 'https://example.com' , { async : false }); https://example.com is the host, from where https://example.com/v2/history/... is served.","title":"2. Connection"},{"location":"stream_client/#21-flow-control-mode","text":"Async: true - The transmission will be asynchronous and you need an acknowledge function. Incoming data will be held in a local queue until ack is called. Async: false - The transmission will be synchronous. The acknowledge function is not needed.","title":"2.1 Flow control mode"},{"location":"stream_client/#3-requests","text":"client.streamActions(request: StreamActionsRequest): void -- Request action stream client.streamDeltas(request: StreamDeltasRequest): void -- Request delta stream (contract rows) to ensure the client is connected, requests should be defined on the client.onConnect property, refer to examples below;","title":"3. Requests"},{"location":"stream_client/#31-action-stream-clientstreamactions","text":"contract - contract account action - action name account - notified account name start_from - start reading on block or on a specific date: (0=disabled) read_until - stop reading on block (0=disable) or on a specific date (0=disabled) filters - actions filter (more details below) Notes Block number can be either positive or negative - E.g.: -15000, 700 In case of negative block number, it will be subtracted from the HEAD Date format (ISO 8601) - e.g. 2020-01-01T00:00:00.000Z const HyperionSocketClient = require ( '@eosrio/hyperion-stream-client' ). default ; const client = new HyperionSocketClient ( 'http://localhost:7000' , { async : true }); client . onConnect = () => { client . streamActions ({ contract : 'eosio' , action : 'voteproducer' , account : '' , start_from : '2020-03-15T00:00:00.000Z' , read_until : 0 , filters : [], }); } // see 3 for handling data client . onData = async ( data , ack ) => { console . log ( data ); // process incoming data, replace with your code ack (); // ACK when done } client . connect (() => { console . log ( 'connected!' ); });","title":"3.1 Action Stream - client.streamActions"},{"location":"stream_client/#311-act-data-filters","text":"You can set up filters to refine your stream. Filters should use fields following the Hyperion Action Data Structure, such as: act.data.producers (on eosio::voteproducer) @transfer.to (here the @ prefix is required since transfers have special mappings) please refer to the mapping definitions to know which data fields are available Example Filter the stream for every transfer made to the eosio.ramfee account: client . streamActions ({ contract : 'eosio.token' , action : 'transfer' , account : 'eosio' , start_from : 0 , read_until : 0 , filters : [ { field : '@transfer.to' , value : 'eosio.ramfee' } ], }); To refine even more your stream, you could add more filters. Attention Remember that adding more filters will result in AND operations. Currently, it's not possible to make OR operations with filters.","title":"3.1.1 Act Data Filters"},{"location":"stream_client/#32-delta-stream-clientstreamdeltas","text":"code - contract account table - table name scope - table scope payer - ram payer start_from - start reading on block or on a specific date: (0=disabled) read_until - stop reading on block (0=disable) or on a specific date (0=disabled) Example Referring to the same pattern as the action stream example above, one could also include a delta stream request client . streamDeltas ({ code : 'eosio.token' , table : '*' , scope : '' , payer : '' , start_from : 0 , read_until : 0 , }); Note Delta filters are planned to be implemented soon.","title":"3.2 Delta Stream - client.streamDeltas"},{"location":"stream_client/#4-handling-data","text":"Incoming data is handled via the client.onData callback data object is structured as follows: type - action | delta mode - live | history content - Hyperion Data Structure (see action index template) client . onData = async ( data , ack ) => { console . log ( data ); // process incoming data, replace with your code ack (); // ACK when done } On every LIB update, client.onLIB is called client . onLIB = async ( data ) => { console . log ( data ); } You can also look for forks, using the client.onFork callback client . onFork = async ( data ) => { console . log ( data ); // process when a there is a fork } If you set async: false on the connection step, the ack must not be used: client . onData = async ( data ) => { //code here }","title":"4. Handling Data"},{"location":"v1/","text":"API Reference: v1 \u00b6 /v1/history/get_actions \u00b6 POST \u00b6 Summary \u00b6 get actions Description \u00b6 legacy get actions query Request Body \u00b6 { \"account_name\": \"string\", \"pos\": 0, \"offset\": 0, \"filter\": \"string\", \"sort\": \"desc\", \"after\": \"2020-01-17T19:51:03.618Z\", \"before\": \"2020-01-17T19:51:03.618Z\", \"parent\": 0 } Schema \u00b6 variable type description account_name string minLength: 1 maxLength: 12 notified account pos integer action position (pagination) offset integer limit of [n] actions per page filter string minLength: 3 code:name filter sort string sort direction Enum: [ desc, asc, 1, -1 ] after string($date-time) filter after specified date (ISO8601) before string($date-time) filter before specified date (ISO8601) parent integer minimum: 0 filter by parent global sequence Responses \u00b6 Code Description 200 Example \u00b6 curl -X POST \"https://eos.hyperion.eosrio.io/v1/history/get_actions\" /v1/history/get_controlled_accounts \u00b6 POST \u00b6 Summary \u00b6 get controlled accounts by controlling accounts Description \u00b6 get controlled accounts by controlling accounts Request Body Required \u00b6 { \"controlling_account\": \"string\" } Schema \u00b6 variable type description controlling_account string controlling account Responses \u00b6 Code Description 200 Example \u00b6 curl -X POST \"https://eos.hyperion.eosrio.io/v1/history/get_controlled_accounts\" -d '{\"controlling_account\":\"eosio\"}' /v1/history/get_key_accounts \u00b6 POST \u00b6 Summary \u00b6 get accounts by public key Description \u00b6 get accounts by public key Request Body Required \u00b6 { \"public_key\": \"string\" } Schema \u00b6 variable type description public_key public key public key Responses \u00b6 Code Description 200 Default Response Example \u00b6 curl -X POST \"https://eos.hyperion.eosrio.io/v1/history/get_key_accounts\" -d '{\"public_key\":\"EOS8fDDZm7ommT5XBf9MPYkRioXX6GeCUeSNkTpimdwKon5bNAVm7\"}' Code Description 200 /v1/history/get_transaction \u00b6 POST \u00b6 Summary \u00b6 get transaction by id Description \u00b6 get all actions belonging to the same transaction Request Body Required \u00b6 { \"id\": \"string\" } Schema \u00b6 variable type description id string transaction id Responses \u00b6 Code Description 200 Default Response Example \u00b6 curl -X POST \"https://eos.hyperion.eosrio.io/v1/history/get_transaction\" /v1/chain/get_block \u00b6 POST \u00b6 Summary \u00b6 Returns an object containing various details about a specific block on the blockchain. Description \u00b6 Returns an object containing various details about a specific block on the blockchain. Request Body \u00b6 { \"block_num_or_id\": \"string\" } Schema \u00b6 block_num_or_id* -> string -> Provide a block number or a block id Responses \u00b6 Code Description 200 Default Response Example \u00b6 curl -X POST \"https://eos.hyperion.eosrio.io/v1/chain/get_block\" -d '{\"block_num_or_id\": \"1000\"}' /v1/trace_api/get_block \u00b6 POST \u00b6 Summary \u00b6 Get block traces. Description \u00b6 Get block traces. Request Body \u00b6 { \"block_num\": 0, \"block_id\": \"string\" } Schema \u00b6 block_num -> integer -> block number block_id -> string -> block id Responses \u00b6 Code Description 200 Default Response Examples \u00b6 Get block 10000: curl -X POST --url https://wax.hyperion.eosrio.io/v1/trace_api/get_block --data '{\"block_num\": 10000}' --header 'content-type: application/json' Get latest block: curl -X POST --url https://wax.hyperion.eosrio.io/v1/trace_api/get_block --data '{\"block_num\": -1}' --header 'content-type: application/json' By block_id: curl -X POST --url https://wax.hyperion.eosrio.io/v1/trace_api/get_block --data '{\"block_id\": \"0307e42d3b2328805606929438411aa78dffb1756b2a0bcb2a9f7ce8a7035990\"}' --header 'content-type: application/json'","title":"v1 compatible"},{"location":"v1/#api-reference-v1","text":"","title":"API Reference: v1"},{"location":"v1/#v1historyget_actions","text":"","title":"/v1/history/get_actions"},{"location":"v1/#post","text":"","title":"POST"},{"location":"v1/#summary","text":"get actions","title":"Summary"},{"location":"v1/#description","text":"legacy get actions query","title":"Description"},{"location":"v1/#request-body","text":"{ \"account_name\": \"string\", \"pos\": 0, \"offset\": 0, \"filter\": \"string\", \"sort\": \"desc\", \"after\": \"2020-01-17T19:51:03.618Z\", \"before\": \"2020-01-17T19:51:03.618Z\", \"parent\": 0 }","title":"Request Body"},{"location":"v1/#schema","text":"variable type description account_name string minLength: 1 maxLength: 12 notified account pos integer action position (pagination) offset integer limit of [n] actions per page filter string minLength: 3 code:name filter sort string sort direction Enum: [ desc, asc, 1, -1 ] after string($date-time) filter after specified date (ISO8601) before string($date-time) filter before specified date (ISO8601) parent integer minimum: 0 filter by parent global sequence","title":"Schema"},{"location":"v1/#responses","text":"Code Description 200","title":"Responses"},{"location":"v1/#example","text":"curl -X POST \"https://eos.hyperion.eosrio.io/v1/history/get_actions\"","title":"Example"},{"location":"v1/#v1historyget_controlled_accounts","text":"","title":"/v1/history/get_controlled_accounts"},{"location":"v1/#post_1","text":"","title":"POST"},{"location":"v1/#summary_1","text":"get controlled accounts by controlling accounts","title":"Summary"},{"location":"v1/#description_1","text":"get controlled accounts by controlling accounts","title":"Description"},{"location":"v1/#request-body-required","text":"{ \"controlling_account\": \"string\" }","title":"Request Body Required"},{"location":"v1/#schema_1","text":"variable type description controlling_account string controlling account","title":"Schema"},{"location":"v1/#responses_1","text":"Code Description 200","title":"Responses"},{"location":"v1/#example_1","text":"curl -X POST \"https://eos.hyperion.eosrio.io/v1/history/get_controlled_accounts\" -d '{\"controlling_account\":\"eosio\"}'","title":"Example"},{"location":"v1/#v1historyget_key_accounts","text":"","title":"/v1/history/get_key_accounts"},{"location":"v1/#post_2","text":"","title":"POST"},{"location":"v1/#summary_2","text":"get accounts by public key","title":"Summary"},{"location":"v1/#description_2","text":"get accounts by public key","title":"Description"},{"location":"v1/#request-body-required_1","text":"{ \"public_key\": \"string\" }","title":"Request Body Required"},{"location":"v1/#schema_2","text":"variable type description public_key public key public key","title":"Schema"},{"location":"v1/#responses_2","text":"Code Description 200 Default Response","title":"Responses"},{"location":"v1/#example_2","text":"curl -X POST \"https://eos.hyperion.eosrio.io/v1/history/get_key_accounts\" -d '{\"public_key\":\"EOS8fDDZm7ommT5XBf9MPYkRioXX6GeCUeSNkTpimdwKon5bNAVm7\"}' Code Description 200","title":"Example"},{"location":"v1/#v1historyget_transaction","text":"","title":"/v1/history/get_transaction"},{"location":"v1/#post_3","text":"","title":"POST"},{"location":"v1/#summary_3","text":"get transaction by id","title":"Summary"},{"location":"v1/#description_3","text":"get all actions belonging to the same transaction","title":"Description"},{"location":"v1/#request-body-required_2","text":"{ \"id\": \"string\" }","title":"Request Body Required"},{"location":"v1/#schema_3","text":"variable type description id string transaction id","title":"Schema"},{"location":"v1/#responses_3","text":"Code Description 200 Default Response","title":"Responses"},{"location":"v1/#example_3","text":"curl -X POST \"https://eos.hyperion.eosrio.io/v1/history/get_transaction\"","title":"Example"},{"location":"v1/#v1chainget_block","text":"","title":"/v1/chain/get_block"},{"location":"v1/#post_4","text":"","title":"POST"},{"location":"v1/#summary_4","text":"Returns an object containing various details about a specific block on the blockchain.","title":"Summary"},{"location":"v1/#description_4","text":"Returns an object containing various details about a specific block on the blockchain.","title":"Description"},{"location":"v1/#request-body_1","text":"{ \"block_num_or_id\": \"string\" }","title":"Request Body"},{"location":"v1/#schema_4","text":"block_num_or_id* -> string -> Provide a block number or a block id","title":"Schema"},{"location":"v1/#responses_4","text":"Code Description 200 Default Response","title":"Responses"},{"location":"v1/#example_4","text":"curl -X POST \"https://eos.hyperion.eosrio.io/v1/chain/get_block\" -d '{\"block_num_or_id\": \"1000\"}'","title":"Example"},{"location":"v1/#v1trace_apiget_block","text":"","title":"/v1/trace_api/get_block"},{"location":"v1/#post_5","text":"","title":"POST"},{"location":"v1/#summary_5","text":"Get block traces.","title":"Summary"},{"location":"v1/#description_5","text":"Get block traces.","title":"Description"},{"location":"v1/#request-body_2","text":"{ \"block_num\": 0, \"block_id\": \"string\" }","title":"Request Body"},{"location":"v1/#schema_5","text":"block_num -> integer -> block number block_id -> string -> block id","title":"Schema"},{"location":"v1/#responses_5","text":"Code Description 200 Default Response","title":"Responses"},{"location":"v1/#examples","text":"Get block 10000: curl -X POST --url https://wax.hyperion.eosrio.io/v1/trace_api/get_block --data '{\"block_num\": 10000}' --header 'content-type: application/json' Get latest block: curl -X POST --url https://wax.hyperion.eosrio.io/v1/trace_api/get_block --data '{\"block_num\": -1}' --header 'content-type: application/json' By block_id: curl -X POST --url https://wax.hyperion.eosrio.io/v1/trace_api/get_block --data '{\"block_id\": \"0307e42d3b2328805606929438411aa78dffb1756b2a0bcb2a9f7ce8a7035990\"}' --header 'content-type: application/json'","title":"Examples"},{"location":"v2/","text":"API Reference: v2 \u00b6 Tip For more details and live testing, check out our WAX swagger /v2/history/get_abi_snapshot \u00b6 GET \u00b6 Summary \u00b6 Fetch contract ABI Description \u00b6 Fetch contract ABI at a specific block if specified. Parameters \u00b6 Name Located in Description Required Schema contract query Contract account Yes string block query Target block No integer fetch query Fetch the ABI No boolean Responses \u00b6 Code Description 200 Default Response Examples \u00b6 /v2/history/get_abi_snapshot?contract=eosio.token&fetch=true /v2/history/get_actions \u00b6 GET \u00b6 Summary \u00b6 Get actions Description \u00b6 Get past actions based on notified account. Parameters \u00b6 Name Located in Description Required Schema account query Notified account No string filter query Filter by code:name No string track query Total results to track (count) No string skip query Skip [n] actions (pagination) No integer limit query Limit of [n] actions per page No integer sort query Sort direction ('desc', 'asc', '1' or '-1') No string after query Filter actions after specified date (ISO8601) No string before query Filter actions before specified date (ISO8601) No string simple query Simplified output mode No boolean noBinary query exclude large binary data No boolean checkLib query perform reversibility check No boolean Note Besides the parameters listed above, this endpoint also accepts generic parameters based on indexed fields (e.g. act.authorization.actor=eosio or act.name=delegatebw ). If included they will be combined with an AND operator. Tip You can check action mappings on index-templates.ts file for the list of all possible parameters. Responses \u00b6 Code Description 200 Default Response Examples \u00b6 /v2/history/get_actions?account=eosio&act.name=delegatebw noBinary - Will suppress large hex payloads to reduce the transmission latency /v2/history/get_actions?noBinary=true checkLib - Performs lib test. The default is false, meaning the query will be faster since it won't wait for the get_info to happen. If you wish to see the lib value on the results you must request explicitly. /v2/history/get_actions?checkLib=true The simple mode will add the irreversible field ( true/false ) on each action /v2/history/get_actions?checkLib=true&simple=true /v2/history/get_created_accounts \u00b6 GET \u00b6 Summary \u00b6 Get created accounts Description \u00b6 Get all accounts created by one creator. Parameters \u00b6 Name Located in Description Required Schema account query Account creator Yes string Responses \u00b6 Code Description 200 Default Response Examples \u00b6 /v2/history/get_created_accounts?account=eosio /v2/history/get_creator \u00b6 GET \u00b6 Summary \u00b6 Get account creator Description \u00b6 Get the creator of a specific account. Parameters \u00b6 Name Located in Description Required Schema account query Created account Yes string Responses \u00b6 Code Description 200 Default Response Examples \u00b6 /v2/history/get_creator?account=eosriobrazil /v2/history/get_deltas \u00b6 GET \u00b6 Summary \u00b6 Get state deltas Description \u00b6 Get state deltas of any table. Parameters \u00b6 Name Located in Description Required Schema code query Contract account No string scope query Table scope No string table query Table name No string payer query Payer account No string Note Besides the parameters listed above, this endpoint also accepts generic parameters based on indexed fields (e.g. block_num=10000000 ). If included they will be combined with an AND operator. Tip You can check delta mappings on index-templates.ts file for the list of all possible parameters. Responses \u00b6 Code Description 200 Default Response Examples \u00b6 Get all deltas from eosio.token contract on a specific block /v2/history/get_deltas?code=eosio.token&block_num=100000 Get all deltas from the table accounts of the eosio.token contract /v2/history/get_deltas?code=eosio.token&table=accounts /v2/history/get_transacted_accounts \u00b6 GET \u00b6 Summary \u00b6 Get account interactions based on transfers Description \u00b6 Get all accounts that interacted with the source account provided. Interactions can be from or to (direction) source account. Parameters \u00b6 Name Located in Description Required Schema account query Source account Yes string direction query Interaction direction ('in', 'out' or 'both') Yes string symbol query Token symbol No string contract query Token contract No string min query Transfer minimum value No number max query Transfer maximum value No number limit query Limit of [n] interactions No number after query Filter actions after specified date (ISO8601) or block number No string before query Filter actions before specified date (ISO8601) or block number No string Responses \u00b6 Code Description 200 Default Response Examples \u00b6 /v2/history/get_transacted_accounts?account=eosriobrazil&direction=out /v2/history/get_transaction \u00b6 GET \u00b6 Summary \u00b6 Get transaction by id Description \u00b6 Get a transaction with all its actions. Parameters \u00b6 Name Located in Description Required Schema id query Transaction ID Yes string Responses \u00b6 Code Description 200 Default Response Examples \u00b6 v2/history/get_transaction?id=eec44c2ab2c2330e88fdacd3cd4c63838adb60da679ff12f299a4341fd036658 /v2/history/get_transfers \u00b6 GET \u00b6 Summary \u00b6 Get token transfers Description \u00b6 Get token transfers from contracts using the eosio.token standard. Parameters \u00b6 Name Located in Description Required Schema from query Source account No string to query Destination account No string symbol query Token symbol No string contract query Token contract No string skip query Skip [n] actions (pagination) No integer limit query Limit of [n] actions per page No integer after query Filter actions after specified date (ISO8601) No string before query Filter actions before specified date (ISO8601) No string Responses \u00b6 Code Description 200 Default Response Examples \u00b6 Get all transfers from eosriobrazil account /v2/history/get_transfers?from=eosriobrazil Get all transfers from eosriobrazil account to eosio.ramfee account /v2/history/get_transfers?from=eosriobrazil&to=eosio.ramfee\" Get all transfers from eosriobrazil account to eosio.ramfee account after November 2019 /v2/history/get_transfers?from=eosriobrazil&to=eosio.ramfee&after=2019-11-01T00:00:00.000Z /v2/state/get_account \u00b6 GET \u00b6 Summary \u00b6 Get account summary Description \u00b6 Get all data related to an account. Parameters \u00b6 Name Located in Description Required Schema account query Account name No string Responses \u00b6 Code Description 200 Default Response Examples \u00b6 /v2/state/get_account?account=eosio /v2/state/get_key_accounts \u00b6 GET \u00b6 Summary \u00b6 Get accounts by public key Description \u00b6 Get all accounts with specified public key. Parameters \u00b6 Name Located in Description Required Schema public_key query Public key Yes string Responses \u00b6 Code Description 200 Default Response Examples \u00b6 /v2/state/get_key_accounts?public_key=EOS8fDDZm7ommT5XBf9MPYkRioXX6GeCUeSNkTpimdwKon5bNAVm7\" /v2/state/get_key_accounts \u00b6 POST \u00b6 Summary \u00b6 Get accounts by public key Description \u00b6 Get all accounts with specified public key. Parameters \u00b6 Name Located in Description Required Schema public_key body Public key Yes string Responses \u00b6 Code Description 200 Default Response Examples \u00b6 /v2/state/get_key_accounts - body: {\"public_key\":\"EOS8fDDZm7ommT5XBf9MPYkRioXX6GeCUeSNkTpimdwKon5bNAVm7\"} /v2/state/get_links \u00b6 GET \u00b6 Summary \u00b6 Get permission links Description \u00b6 Get permission links from an account. Parameters \u00b6 Name Located in Description Required Schema account name (query) Account name Yes string code (query) Contract name No string action (query) Method name No string permission (query) Permission name No string Responses \u00b6 Code Description 200 Default Response Examples \u00b6 /v2/state/get_links?account=eosriobrazil /v2/state/get_proposals \u00b6 GET \u00b6 Summary \u00b6 Get proposals Description \u00b6 Get all available proposals. Parameters \u00b6 Name Located in Description Required Schema proposer query Filter by proposer No string proposal query Filter by proposal name No string account query Filter by either requested or provided account No string requested query Filter by requested account No string provided query Filter by provided account No string executed query Filter by execution status No boolean track query Total results to track (count) No string skip query Skip [n] actions (pagination) No integer limit query Limit of [n] actions per page No integer Responses \u00b6 Code Description 200 Default Response Examples \u00b6 /v2/state/get_proposals?proposer=eosriobrazil /v2/state/get_tokens \u00b6 GET \u00b6 Summary \u00b6 Get tokens from account Description \u00b6 Retrieve all tokens holded by an account. Parameters \u00b6 Name Located in Description Required Schema account query Account name Yes string Responses \u00b6 Code Description 200 Default Response Examples \u00b6 /v2/state/get_tokens?account=eosriobrazil /v2/state/get_voters \u00b6 GET \u00b6 Summary \u00b6 Get voters Description \u00b6 Get all voting accounts. Parameters \u00b6 Name Located in Description Required Schema producer query Filter by voted producer (comma separated) No string skip query Skip [n] actions (pagination) No integer limit query Limit of [n] actions per page No integer Responses \u00b6 Code Description 200 Default Response Examples \u00b6 Get all eosriobrazil voters /v2/state/get_voters?producer=eosriobrazil Get only the first 3 responses /v2/state/get_voters?producer=eosriobrazil&limit=3 /v2/health \u00b6 GET \u00b6 Summary: \u00b6 API Service Health Report Responses \u00b6 Code Description 200 Default Response","title":"v2"},{"location":"v2/#api-reference-v2","text":"Tip For more details and live testing, check out our WAX swagger","title":"API Reference: v2"},{"location":"v2/#v2historyget_abi_snapshot","text":"","title":"/v2/history/get_abi_snapshot"},{"location":"v2/#get","text":"","title":"GET"},{"location":"v2/#summary","text":"Fetch contract ABI","title":"Summary"},{"location":"v2/#description","text":"Fetch contract ABI at a specific block if specified.","title":"Description"},{"location":"v2/#parameters","text":"Name Located in Description Required Schema contract query Contract account Yes string block query Target block No integer fetch query Fetch the ABI No boolean","title":"Parameters"},{"location":"v2/#responses","text":"Code Description 200 Default Response","title":"Responses"},{"location":"v2/#examples","text":"/v2/history/get_abi_snapshot?contract=eosio.token&fetch=true","title":"Examples"},{"location":"v2/#v2historyget_actions","text":"","title":"/v2/history/get_actions"},{"location":"v2/#get_1","text":"","title":"GET"},{"location":"v2/#summary_1","text":"Get actions","title":"Summary"},{"location":"v2/#description_1","text":"Get past actions based on notified account.","title":"Description"},{"location":"v2/#parameters_1","text":"Name Located in Description Required Schema account query Notified account No string filter query Filter by code:name No string track query Total results to track (count) No string skip query Skip [n] actions (pagination) No integer limit query Limit of [n] actions per page No integer sort query Sort direction ('desc', 'asc', '1' or '-1') No string after query Filter actions after specified date (ISO8601) No string before query Filter actions before specified date (ISO8601) No string simple query Simplified output mode No boolean noBinary query exclude large binary data No boolean checkLib query perform reversibility check No boolean Note Besides the parameters listed above, this endpoint also accepts generic parameters based on indexed fields (e.g. act.authorization.actor=eosio or act.name=delegatebw ). If included they will be combined with an AND operator. Tip You can check action mappings on index-templates.ts file for the list of all possible parameters.","title":"Parameters"},{"location":"v2/#responses_1","text":"Code Description 200 Default Response","title":"Responses"},{"location":"v2/#examples_1","text":"/v2/history/get_actions?account=eosio&act.name=delegatebw noBinary - Will suppress large hex payloads to reduce the transmission latency /v2/history/get_actions?noBinary=true checkLib - Performs lib test. The default is false, meaning the query will be faster since it won't wait for the get_info to happen. If you wish to see the lib value on the results you must request explicitly. /v2/history/get_actions?checkLib=true The simple mode will add the irreversible field ( true/false ) on each action /v2/history/get_actions?checkLib=true&simple=true","title":"Examples"},{"location":"v2/#v2historyget_created_accounts","text":"","title":"/v2/history/get_created_accounts"},{"location":"v2/#get_2","text":"","title":"GET"},{"location":"v2/#summary_2","text":"Get created accounts","title":"Summary"},{"location":"v2/#description_2","text":"Get all accounts created by one creator.","title":"Description"},{"location":"v2/#parameters_2","text":"Name Located in Description Required Schema account query Account creator Yes string","title":"Parameters"},{"location":"v2/#responses_2","text":"Code Description 200 Default Response","title":"Responses"},{"location":"v2/#examples_2","text":"/v2/history/get_created_accounts?account=eosio","title":"Examples"},{"location":"v2/#v2historyget_creator","text":"","title":"/v2/history/get_creator"},{"location":"v2/#get_3","text":"","title":"GET"},{"location":"v2/#summary_3","text":"Get account creator","title":"Summary"},{"location":"v2/#description_3","text":"Get the creator of a specific account.","title":"Description"},{"location":"v2/#parameters_3","text":"Name Located in Description Required Schema account query Created account Yes string","title":"Parameters"},{"location":"v2/#responses_3","text":"Code Description 200 Default Response","title":"Responses"},{"location":"v2/#examples_3","text":"/v2/history/get_creator?account=eosriobrazil","title":"Examples"},{"location":"v2/#v2historyget_deltas","text":"","title":"/v2/history/get_deltas"},{"location":"v2/#get_4","text":"","title":"GET"},{"location":"v2/#summary_4","text":"Get state deltas","title":"Summary"},{"location":"v2/#description_4","text":"Get state deltas of any table.","title":"Description"},{"location":"v2/#parameters_4","text":"Name Located in Description Required Schema code query Contract account No string scope query Table scope No string table query Table name No string payer query Payer account No string Note Besides the parameters listed above, this endpoint also accepts generic parameters based on indexed fields (e.g. block_num=10000000 ). If included they will be combined with an AND operator. Tip You can check delta mappings on index-templates.ts file for the list of all possible parameters.","title":"Parameters"},{"location":"v2/#responses_4","text":"Code Description 200 Default Response","title":"Responses"},{"location":"v2/#examples_4","text":"Get all deltas from eosio.token contract on a specific block /v2/history/get_deltas?code=eosio.token&block_num=100000 Get all deltas from the table accounts of the eosio.token contract /v2/history/get_deltas?code=eosio.token&table=accounts","title":"Examples"},{"location":"v2/#v2historyget_transacted_accounts","text":"","title":"/v2/history/get_transacted_accounts"},{"location":"v2/#get_5","text":"","title":"GET"},{"location":"v2/#summary_5","text":"Get account interactions based on transfers","title":"Summary"},{"location":"v2/#description_5","text":"Get all accounts that interacted with the source account provided. Interactions can be from or to (direction) source account.","title":"Description"},{"location":"v2/#parameters_5","text":"Name Located in Description Required Schema account query Source account Yes string direction query Interaction direction ('in', 'out' or 'both') Yes string symbol query Token symbol No string contract query Token contract No string min query Transfer minimum value No number max query Transfer maximum value No number limit query Limit of [n] interactions No number after query Filter actions after specified date (ISO8601) or block number No string before query Filter actions before specified date (ISO8601) or block number No string","title":"Parameters"},{"location":"v2/#responses_5","text":"Code Description 200 Default Response","title":"Responses"},{"location":"v2/#examples_5","text":"/v2/history/get_transacted_accounts?account=eosriobrazil&direction=out","title":"Examples"},{"location":"v2/#v2historyget_transaction","text":"","title":"/v2/history/get_transaction"},{"location":"v2/#get_6","text":"","title":"GET"},{"location":"v2/#summary_6","text":"Get transaction by id","title":"Summary"},{"location":"v2/#description_6","text":"Get a transaction with all its actions.","title":"Description"},{"location":"v2/#parameters_6","text":"Name Located in Description Required Schema id query Transaction ID Yes string","title":"Parameters"},{"location":"v2/#responses_6","text":"Code Description 200 Default Response","title":"Responses"},{"location":"v2/#examples_6","text":"v2/history/get_transaction?id=eec44c2ab2c2330e88fdacd3cd4c63838adb60da679ff12f299a4341fd036658","title":"Examples"},{"location":"v2/#v2historyget_transfers","text":"","title":"/v2/history/get_transfers"},{"location":"v2/#get_7","text":"","title":"GET"},{"location":"v2/#summary_7","text":"Get token transfers","title":"Summary"},{"location":"v2/#description_7","text":"Get token transfers from contracts using the eosio.token standard.","title":"Description"},{"location":"v2/#parameters_7","text":"Name Located in Description Required Schema from query Source account No string to query Destination account No string symbol query Token symbol No string contract query Token contract No string skip query Skip [n] actions (pagination) No integer limit query Limit of [n] actions per page No integer after query Filter actions after specified date (ISO8601) No string before query Filter actions before specified date (ISO8601) No string","title":"Parameters"},{"location":"v2/#responses_7","text":"Code Description 200 Default Response","title":"Responses"},{"location":"v2/#examples_7","text":"Get all transfers from eosriobrazil account /v2/history/get_transfers?from=eosriobrazil Get all transfers from eosriobrazil account to eosio.ramfee account /v2/history/get_transfers?from=eosriobrazil&to=eosio.ramfee\" Get all transfers from eosriobrazil account to eosio.ramfee account after November 2019 /v2/history/get_transfers?from=eosriobrazil&to=eosio.ramfee&after=2019-11-01T00:00:00.000Z","title":"Examples"},{"location":"v2/#v2stateget_account","text":"","title":"/v2/state/get_account"},{"location":"v2/#get_8","text":"","title":"GET"},{"location":"v2/#summary_8","text":"Get account summary","title":"Summary"},{"location":"v2/#description_8","text":"Get all data related to an account.","title":"Description"},{"location":"v2/#parameters_8","text":"Name Located in Description Required Schema account query Account name No string","title":"Parameters"},{"location":"v2/#responses_8","text":"Code Description 200 Default Response","title":"Responses"},{"location":"v2/#examples_8","text":"/v2/state/get_account?account=eosio","title":"Examples"},{"location":"v2/#v2stateget_key_accounts","text":"","title":"/v2/state/get_key_accounts"},{"location":"v2/#get_9","text":"","title":"GET"},{"location":"v2/#summary_9","text":"Get accounts by public key","title":"Summary"},{"location":"v2/#description_9","text":"Get all accounts with specified public key.","title":"Description"},{"location":"v2/#parameters_9","text":"Name Located in Description Required Schema public_key query Public key Yes string","title":"Parameters"},{"location":"v2/#responses_9","text":"Code Description 200 Default Response","title":"Responses"},{"location":"v2/#examples_9","text":"/v2/state/get_key_accounts?public_key=EOS8fDDZm7ommT5XBf9MPYkRioXX6GeCUeSNkTpimdwKon5bNAVm7\"","title":"Examples"},{"location":"v2/#v2stateget_key_accounts_1","text":"","title":"/v2/state/get_key_accounts"},{"location":"v2/#post","text":"","title":"POST"},{"location":"v2/#summary_10","text":"Get accounts by public key","title":"Summary"},{"location":"v2/#description_10","text":"Get all accounts with specified public key.","title":"Description"},{"location":"v2/#parameters_10","text":"Name Located in Description Required Schema public_key body Public key Yes string","title":"Parameters"},{"location":"v2/#responses_10","text":"Code Description 200 Default Response","title":"Responses"},{"location":"v2/#examples_10","text":"/v2/state/get_key_accounts - body: {\"public_key\":\"EOS8fDDZm7ommT5XBf9MPYkRioXX6GeCUeSNkTpimdwKon5bNAVm7\"}","title":"Examples"},{"location":"v2/#v2stateget_links","text":"","title":"/v2/state/get_links"},{"location":"v2/#get_10","text":"","title":"GET"},{"location":"v2/#summary_11","text":"Get permission links","title":"Summary"},{"location":"v2/#description_11","text":"Get permission links from an account.","title":"Description"},{"location":"v2/#parameters_11","text":"Name Located in Description Required Schema account name (query) Account name Yes string code (query) Contract name No string action (query) Method name No string permission (query) Permission name No string","title":"Parameters"},{"location":"v2/#responses_11","text":"Code Description 200 Default Response","title":"Responses"},{"location":"v2/#examples_11","text":"/v2/state/get_links?account=eosriobrazil","title":"Examples"},{"location":"v2/#v2stateget_proposals","text":"","title":"/v2/state/get_proposals"},{"location":"v2/#get_11","text":"","title":"GET"},{"location":"v2/#summary_12","text":"Get proposals","title":"Summary"},{"location":"v2/#description_12","text":"Get all available proposals.","title":"Description"},{"location":"v2/#parameters_12","text":"Name Located in Description Required Schema proposer query Filter by proposer No string proposal query Filter by proposal name No string account query Filter by either requested or provided account No string requested query Filter by requested account No string provided query Filter by provided account No string executed query Filter by execution status No boolean track query Total results to track (count) No string skip query Skip [n] actions (pagination) No integer limit query Limit of [n] actions per page No integer","title":"Parameters"},{"location":"v2/#responses_12","text":"Code Description 200 Default Response","title":"Responses"},{"location":"v2/#examples_12","text":"/v2/state/get_proposals?proposer=eosriobrazil","title":"Examples"},{"location":"v2/#v2stateget_tokens","text":"","title":"/v2/state/get_tokens"},{"location":"v2/#get_12","text":"","title":"GET"},{"location":"v2/#summary_13","text":"Get tokens from account","title":"Summary"},{"location":"v2/#description_13","text":"Retrieve all tokens holded by an account.","title":"Description"},{"location":"v2/#parameters_13","text":"Name Located in Description Required Schema account query Account name Yes string","title":"Parameters"},{"location":"v2/#responses_13","text":"Code Description 200 Default Response","title":"Responses"},{"location":"v2/#examples_13","text":"/v2/state/get_tokens?account=eosriobrazil","title":"Examples"},{"location":"v2/#v2stateget_voters","text":"","title":"/v2/state/get_voters"},{"location":"v2/#get_13","text":"","title":"GET"},{"location":"v2/#summary_14","text":"Get voters","title":"Summary"},{"location":"v2/#description_14","text":"Get all voting accounts.","title":"Description"},{"location":"v2/#parameters_14","text":"Name Located in Description Required Schema producer query Filter by voted producer (comma separated) No string skip query Skip [n] actions (pagination) No integer limit query Limit of [n] actions per page No integer","title":"Parameters"},{"location":"v2/#responses_14","text":"Code Description 200 Default Response","title":"Responses"},{"location":"v2/#examples_14","text":"Get all eosriobrazil voters /v2/state/get_voters?producer=eosriobrazil Get only the first 3 responses /v2/state/get_voters?producer=eosriobrazil&limit=3","title":"Examples"},{"location":"v2/#v2health","text":"","title":"/v2/health"},{"location":"v2/#get_14","text":"","title":"GET"},{"location":"v2/#summary_15","text":"API Service Health Report","title":"Summary:"},{"location":"v2/#responses_15","text":"Code Description 200 Default Response","title":"Responses"},{"location":"windows/","text":"Windows \u00b6 Installation instructions for Multipass on Windows 10 with Hyper-V \u00b6 1. Create VM: \u00b6 multipass launch -m 16G -c 4 -d 16G -n hyperion *adjust -m/-d/-c according to your needs 2. Stop the VM: \u00b6 multipass stop hyperion 3. Disable Dynamic Memory \u00b6 Open the Hyper-V Manager right click the \"hyperion\" VM go on \"Settings...\" -> \"Memory\" -> Uncheck \"Enable Dynamic Memory\" 4. Start the VM: \u00b6 multipass start hyperion 5. Proceed with the automated install: \u00b6 git clone https://github.com/eosrio/hyperion-history-api.git cd hyperion-history-api ./install_env.sh 6. Check if elasticsearch memlock was successful: \u00b6 systemctl status elasticsearch.service Note \"Dynamic Memory\" can be enabled after installation if desired, just stop the vm, update the settings and restart.","title":"Windows"},{"location":"windows/#windows","text":"","title":"Windows"},{"location":"windows/#installation-instructions-for-multipass-on-windows-10-with-hyper-v","text":"","title":"Installation instructions for Multipass on Windows 10 with Hyper-V"},{"location":"windows/#1-create-vm","text":"multipass launch -m 16G -c 4 -d 16G -n hyperion *adjust -m/-d/-c according to your needs","title":"1. Create VM:"},{"location":"windows/#2-stop-the-vm","text":"multipass stop hyperion","title":"2. Stop the VM:"},{"location":"windows/#3-disable-dynamic-memory","text":"Open the Hyper-V Manager right click the \"hyperion\" VM go on \"Settings...\" -> \"Memory\" -> Uncheck \"Enable Dynamic Memory\"","title":"3. Disable Dynamic Memory"},{"location":"windows/#4-start-the-vm","text":"multipass start hyperion","title":"4. Start the VM:"},{"location":"windows/#5-proceed-with-the-automated-install","text":"git clone https://github.com/eosrio/hyperion-history-api.git cd hyperion-history-api ./install_env.sh","title":"5. Proceed with the automated install:"},{"location":"windows/#6-check-if-elasticsearch-memlock-was-successful","text":"systemctl status elasticsearch.service Note \"Dynamic Memory\" can be enabled after installation if desired, just stop the vm, update the settings and restart.","title":"6. Check if elasticsearch memlock was successful:"}]}