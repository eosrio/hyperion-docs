{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#hyperion-history-api","title":"Hyperion History API","text":"<p>Scalable Full History API Solution for Antelope (former EOSIO) based blockchains. Made with \u2665 by Rio Blocks / EOS Rio</p>"},{"location":"#official-documentation","title":"Official documentation","text":""},{"location":"#for-providers","title":"For Providers","text":"<p>Getting Started Configuring Updating Repairing</p>"},{"location":"#for-developers","title":"For Developers","text":"<p>Getting Started</p> <p>Stream Client Endpoints List</p>"},{"location":"#api-reference","title":"API Reference","text":"<p>V2 V1 Compatible</p>"},{"location":"#official-plugins","title":"Official plugins:","text":"<ul> <li>Hyperion Lightweight Explorer</li> </ul>"},{"location":"#1-overview","title":"1. Overview","text":"<p>Hyperion is a full history solution for indexing, storing and retrieving Antelope blockchain's historical data. Antelope protocol is highly scalable reaching up to tens of thousands of transactions per second demanding high performance indexing and optimized storage and querying solutions. Hyperion is developed to tackle those challenges providing open source software to be operated by block producers, infrastructure providers and dApp developers.</p> <p>Focused on delivering faster search times, lower bandwidth overhead and easier usability for UI/UX developers, Hyperion implements an improved data structure. Actions are stored in a flattened format, transaction ids are added to all inline actions, allowing to group by transaction without storing a full transaction index. Besides that if the inline action data is identical to the parent, it is considered a notification and thus removed from the database. No full block or transaction data is stored, all information can be reconstructed from actions and deltas, only a block header index is stored.</p>"},{"location":"#2-architecture","title":"2. Architecture","text":"<p>The following components are required in order to have a fully functional Hyperion API deployment. </p> <ul> <li>For small use cases, it is absolutely fine to run all components on a single machine.  </li> <li>For larger chains and production environments, we recommend setting them up into different servers under a high-speed local network.  </li> </ul>"},{"location":"#21-elasticsearch-cluster","title":"2.1 Elasticsearch Cluster","text":"<p>The ES cluster is responsible for storing all indexed data. Direct access to the Hyperion API and Indexer must be provided. We recommend nodes in the cluster to have at least 32 GB of RAM and 8 cpu cores. SSD/NVME drives are recommended for maximum indexing throughput, although HDDs can be used for cold storage nodes. For production environments, a multi-node cluster is highly recommended.</p>"},{"location":"#22-hyperion-indexer","title":"2.2 Hyperion Indexer","text":"<p>The Indexer is a Node.js based app that process data from the state history plugin and allows it to be indexed. The PM2 process manager is used to launch and operate the indexer. The configuration flexibility is very extensive, so system recommendations will depend on the use case and data load. It will require access to at least one ES node, RabbitMQ and the state history node.</p>"},{"location":"#23-hyperion-api","title":"2.3 Hyperion API","text":"<p>Parallelizable API server that provides the V2 and V1 (legacy history plugin) endpoints. It is launched by PM2 and can also operate in cluster mode. It requires direct access to at least one ES node for the queries and all other services for full healthcheck</p>"},{"location":"#24-rabbitmq","title":"2.4 RabbitMQ","text":"<p>Used as messaging queue and data transport between the indexer stages and for real-time data streaming</p>"},{"location":"#25-redis","title":"2.5 Redis","text":"<p>Used for transient data storage across processes and for the preemptive transaction caching used on the <code>v2/history/get_transaction</code> and <code>v2/history/check_transaction</code> endpoints</p>"},{"location":"#26-leap-state-history","title":"2.6 Leap State History","text":"<p>Leap / Nodeos plugin used to collect action traces and state deltas. Provides data via websocket to the indexer</p>"},{"location":"#27-hyperion-stream-client-optional","title":"2.7 Hyperion Stream Client (optional)","text":"<p>Web and Node.js client for real-time streaming on enabled hyperion providers. Documentation</p>"},{"location":"#28-hyperion-plugins-optional","title":"2.8 Hyperion Plugins (optional)","text":"<p>Hyperion includes a flexible plugin architecture to allow further customization. Plugins are managed by the <code>hpm</code> (hyperion plugin manager) command line tool.</p> <p></p>"},{"location":"api/v1/","title":"API Reference: v1","text":""},{"location":"api/v1/#v1historyget_actions","title":"/v1/history/get_actions","text":""},{"location":"api/v1/#post","title":"POST","text":""},{"location":"api/v1/#summary","title":"Summary","text":"<p>get actions</p>"},{"location":"api/v1/#description","title":"Description","text":"<p>legacy get actions query</p>"},{"location":"api/v1/#request-body","title":"Request Body","text":"<pre><code>{\n  \"account_name\": \"string\",\n  \"pos\": 0,\n  \"offset\": 0,\n  \"filter\": \"string\",\n  \"sort\": \"desc\",\n  \"after\": \"2020-01-17T19:51:03.618Z\",\n  \"before\": \"2020-01-17T19:51:03.618Z\",\n  \"parent\": 0\n}\n</code></pre>"},{"location":"api/v1/#schema","title":"Schema","text":"variable type description account_name string  minLength: 1 maxLength: 12 notified account pos integer action position (pagination) offset integer limit of [n] actions per page filter string minLength: 3 code:name filter sort string sort direction Enum: [ desc, asc, 1, -1 ] after string($date-time) filter after specified date (ISO8601) before string($date-time) filter before specified date (ISO8601) parent integer minimum: 0 filter by parent global sequence"},{"location":"api/v1/#responses","title":"Responses","text":"Code Description 200"},{"location":"api/v1/#example","title":"Example","text":"<pre><code>curl -X POST \"https://eos.hyperion.eosrio.io/v1/history/get_actions\"\n</code></pre>"},{"location":"api/v1/#v1historyget_controlled_accounts","title":"/v1/history/get_controlled_accounts","text":""},{"location":"api/v1/#post_1","title":"POST","text":""},{"location":"api/v1/#summary_1","title":"Summary","text":"<p>get controlled accounts by controlling accounts</p>"},{"location":"api/v1/#description_1","title":"Description","text":"<p>get controlled accounts by controlling accounts</p>"},{"location":"api/v1/#request-body-required","title":"Request Body Required","text":"<pre><code>{\n  \"controlling_account\": \"string\"\n}\n</code></pre>"},{"location":"api/v1/#schema_1","title":"Schema","text":"variable type description controlling_account string controlling account"},{"location":"api/v1/#responses_1","title":"Responses","text":"Code Description 200"},{"location":"api/v1/#example_1","title":"Example","text":"<pre><code>curl -X POST \"https://eos.hyperion.eosrio.io/v1/history/get_controlled_accounts\" -d '{\"controlling_account\":\"eosio\"}'\n</code></pre>"},{"location":"api/v1/#v1historyget_key_accounts","title":"/v1/history/get_key_accounts","text":""},{"location":"api/v1/#post_2","title":"POST","text":""},{"location":"api/v1/#summary_2","title":"Summary","text":"<p>get accounts by public key</p>"},{"location":"api/v1/#description_2","title":"Description","text":"<p>get accounts by public key</p>"},{"location":"api/v1/#request-body-required_1","title":"Request Body Required","text":"<pre><code>{\n  \"public_key\": \"string\"\n}\n</code></pre>"},{"location":"api/v1/#schema_2","title":"Schema","text":"variable type description public_key public key public key"},{"location":"api/v1/#responses_2","title":"Responses","text":"Code Description 200 Default Response"},{"location":"api/v1/#example_2","title":"Example","text":"<pre><code>curl -X POST \"https://eos.hyperion.eosrio.io/v1/history/get_key_accounts\" -d '{\"public_key\":\"EOS8fDDZm7ommT5XBf9MPYkRioXX6GeCUeSNkTpimdwKon5bNAVm7\"}'\n</code></pre> Code Description 200"},{"location":"api/v1/#v1historyget_transaction","title":"/v1/history/get_transaction","text":""},{"location":"api/v1/#post_3","title":"POST","text":""},{"location":"api/v1/#summary_3","title":"Summary","text":"<p>get transaction by id</p>"},{"location":"api/v1/#description_3","title":"Description","text":"<p>get all actions belonging to the same transaction</p>"},{"location":"api/v1/#request-body-required_2","title":"Request Body Required","text":"<pre><code>{\n  \"id\": \"string\"\n}\n</code></pre>"},{"location":"api/v1/#schema_3","title":"Schema","text":"variable type description id string transaction id"},{"location":"api/v1/#responses_3","title":"Responses","text":"Code Description 200 Default Response"},{"location":"api/v1/#example_3","title":"Example","text":"<pre><code>curl -X POST \"https://eos.hyperion.eosrio.io/v1/history/get_transaction\"\n</code></pre>"},{"location":"api/v1/#v1chainget_block","title":"/v1/chain/get_block","text":""},{"location":"api/v1/#post_4","title":"POST","text":""},{"location":"api/v1/#summary_4","title":"Summary","text":"<p>Returns an object containing various details about a specific block on the blockchain.</p>"},{"location":"api/v1/#description_4","title":"Description","text":"<p>Returns an object containing various details about a specific block on the blockchain.</p>"},{"location":"api/v1/#request-body_1","title":"Request Body","text":"<pre><code>{\n  \"block_num_or_id\": \"string\"\n}\n</code></pre>"},{"location":"api/v1/#schema_4","title":"Schema","text":"<p>block_num_or_id* -&gt; string -&gt; Provide a block number or a block id</p>"},{"location":"api/v1/#responses_4","title":"Responses","text":"Code Description 200 Default Response"},{"location":"api/v1/#example_4","title":"Example","text":"<pre><code>curl -X POST \"https://eos.hyperion.eosrio.io/v1/chain/get_block\" -d '{\"block_num_or_id\": \"1000\"}'\n</code></pre>"},{"location":"api/v1/#v1trace_apiget_block","title":"/v1/trace_api/get_block","text":""},{"location":"api/v1/#post_5","title":"POST","text":""},{"location":"api/v1/#summary_5","title":"Summary","text":"<p>Get block traces.</p>"},{"location":"api/v1/#description_5","title":"Description","text":"<p>Get block traces.</p>"},{"location":"api/v1/#request-body_2","title":"Request Body","text":"<pre><code>{\n  \"block_num\": 0,\n  \"block_id\": \"string\"\n}\n</code></pre>"},{"location":"api/v1/#schema_5","title":"Schema","text":"<p>block_num -&gt; integer -&gt; block number block_id -&gt; string -&gt; block id</p>"},{"location":"api/v1/#responses_5","title":"Responses","text":"Code Description 200 Default Response"},{"location":"api/v1/#examples","title":"Examples","text":"<p>Get block 10000: <pre><code>curl -X POST --url https://wax.hyperion.eosrio.io/v1/trace_api/get_block --data '{\"block_num\": 10000}' --header 'content-type: application/json'\n</code></pre></p> <p>Get latest block: <pre><code>curl -X POST --url https://wax.hyperion.eosrio.io/v1/trace_api/get_block --data '{\"block_num\": -1}' --header 'content-type: application/json'\n</code></pre></p> <p>By block_id: <pre><code>curl -X POST --url https://wax.hyperion.eosrio.io/v1/trace_api/get_block --data '{\"block_id\": \"0307e42d3b2328805606929438411aa78dffb1756b2a0bcb2a9f7ce8a7035990\"}' --header 'content-type: application/json'\n</code></pre></p>"},{"location":"api/v2/","title":"API Reference: v2","text":"<p>Tip</p> <p>For more details and live testing, check out our WAX swagger</p>"},{"location":"api/v2/#v2historyget_abi_snapshot","title":"/v2/history/get_abi_snapshot","text":""},{"location":"api/v2/#get","title":"GET","text":""},{"location":"api/v2/#summary","title":"Summary","text":"<p>Fetch contract ABI</p>"},{"location":"api/v2/#description","title":"Description","text":"<p>Fetch contract ABI at a specific block if specified.</p>"},{"location":"api/v2/#parameters","title":"Parameters","text":"Name Located in Description Required Schema contract query Contract account Yes string block query Target block No integer fetch query Fetch the ABI No boolean"},{"location":"api/v2/#responses","title":"Responses","text":"Code Description 200 Default Response"},{"location":"api/v2/#examples","title":"Examples","text":"<pre><code>/v2/history/get_abi_snapshot?contract=eosio.token&amp;fetch=true\n</code></pre>"},{"location":"api/v2/#v2historyget_actions","title":"/v2/history/get_actions","text":""},{"location":"api/v2/#get_1","title":"GET","text":""},{"location":"api/v2/#summary_1","title":"Summary","text":"<p>Get actions</p>"},{"location":"api/v2/#description_1","title":"Description","text":"<p>Get past actions based on notified account.</p>"},{"location":"api/v2/#parameters_1","title":"Parameters","text":"Name Located in Description Required Schema account query Notified account No string filter query Filter by code:name No string track query Total results to track (count) No string skip query Skip [n] actions (pagination) No integer limit query Limit of [n] actions per page No integer sort query Sort direction ('desc', 'asc', '1' or '-1') No string block_num query Filter actions by block_num range [from]-[to] No string global_sequence query Filter actions by global_sequence range [from]-[to] No string after query Filter actions after specified date (ISO8601) No string before query Filter actions before specified date (ISO8601) No string simple query Simplified output mode No boolean noBinary query exclude large binary data No boolean checkLib query perform reversibility check No boolean <p>Note</p> <p>Besides the parameters listed above, this endpoint also accepts generic parameters based on indexed fields (e.g. <code>act.authorization.actor=eosio</code> or <code>act.name=delegatebw</code>). </p> <p>If included they will be combined with an AND operator.</p> <p>Tip</p> <p>You can check action mappings on index-templates.ts file for the list of all possible parameters.</p>"},{"location":"api/v2/#responses_1","title":"Responses","text":"Code Description 200 Default Response"},{"location":"api/v2/#examples_1","title":"Examples","text":"<pre><code>/v2/history/get_actions?account=eosio&amp;act.name=delegatebw\n</code></pre> <p><code>noBinary</code> - Will suppress large hex payloads to reduce the transmission latency</p> <pre><code>/v2/history/get_actions?noBinary=true\n</code></pre> <p><code>checkLib</code> - Performs lib test. The default is false, meaning the query will be faster since it won't wait for the get_info to happen. If you wish to see the lib value on the results you must request explicitly.</p> <pre><code>/v2/history/get_actions?checkLib=true\n</code></pre> <p>The simple mode will add the irreversible field (<code>true/false</code>) on each action</p> <pre><code>/v2/history/get_actions?checkLib=true&amp;simple=true\n</code></pre> <p></p>"},{"location":"api/v2/#v2historyget_created_accounts","title":"/v2/history/get_created_accounts","text":""},{"location":"api/v2/#get_2","title":"GET","text":""},{"location":"api/v2/#summary_2","title":"Summary","text":"<p>Get created accounts</p>"},{"location":"api/v2/#description_2","title":"Description","text":"<p>Get all accounts created by one creator.</p>"},{"location":"api/v2/#parameters_2","title":"Parameters","text":"Name Located in Description Required Schema account query Account creator Yes string"},{"location":"api/v2/#responses_2","title":"Responses","text":"Code Description 200 Default Response"},{"location":"api/v2/#examples_2","title":"Examples","text":"<pre><code>/v2/history/get_created_accounts?account=eosio\n</code></pre>"},{"location":"api/v2/#v2historyget_creator","title":"/v2/history/get_creator","text":""},{"location":"api/v2/#get_3","title":"GET","text":""},{"location":"api/v2/#summary_3","title":"Summary","text":"<p>Get account creator</p>"},{"location":"api/v2/#description_3","title":"Description","text":"<p>Get the creator of a specific account.</p>"},{"location":"api/v2/#parameters_3","title":"Parameters","text":"Name Located in Description Required Schema account query Created account Yes string"},{"location":"api/v2/#responses_3","title":"Responses","text":"Code Description 200 Default Response"},{"location":"api/v2/#examples_3","title":"Examples","text":"<pre><code>/v2/history/get_creator?account=eosriobrazil\n</code></pre>"},{"location":"api/v2/#v2historyget_deltas","title":"/v2/history/get_deltas","text":""},{"location":"api/v2/#get_4","title":"GET","text":""},{"location":"api/v2/#summary_4","title":"Summary","text":"<p>Get state deltas</p>"},{"location":"api/v2/#description_4","title":"Description","text":"<p>Get state deltas of any table.</p>"},{"location":"api/v2/#parameters_4","title":"Parameters","text":"Name Located in Description Required Schema code query Contract account No string scope query Table scope No string table query Table name No string payer query Payer account No string <p>Note</p> <p>Besides the parameters listed above, this endpoint also accepts generic parameters based on indexed fields (e.g. <code>block_num=10000000</code>). </p> <p>If included they will be combined with an AND operator.</p> <p>Tip</p> <p>You can check delta mappings on index-templates.ts file for the list of all possible parameters.</p>"},{"location":"api/v2/#responses_4","title":"Responses","text":"Code Description 200 Default Response"},{"location":"api/v2/#examples_4","title":"Examples","text":"<p>Get all deltas from <code>eosio.token</code> contract on a specific block <pre><code>/v2/history/get_deltas?code=eosio.token&amp;block_num=100000\n</code></pre></p> <p>Get all deltas from the table <code>accounts</code> of the <code>eosio.token</code> contract  <pre><code>/v2/history/get_deltas?code=eosio.token&amp;table=accounts\n</code></pre></p> <p></p>"},{"location":"api/v2/#v2historyget_transacted_accounts","title":"/v2/history/get_transacted_accounts","text":""},{"location":"api/v2/#get_5","title":"GET","text":""},{"location":"api/v2/#summary_5","title":"Summary","text":"<p>Get account interactions based on transfers</p>"},{"location":"api/v2/#description_5","title":"Description","text":"<p>Get all accounts that interacted with the source account provided. Interactions can be from or to (direction) source account.</p>"},{"location":"api/v2/#parameters_5","title":"Parameters","text":"Name Located in Description Required Schema account query Source account Yes string direction query Interaction direction ('in', 'out' or 'both') Yes string symbol query Token symbol No string contract query Token contract No string min query Transfer minimum value No number max query Transfer maximum value No number limit query Limit of [n] interactions No number after query Filter actions after specified date (ISO8601) or block number No string before query Filter actions before specified date (ISO8601) or block number No string"},{"location":"api/v2/#responses_5","title":"Responses","text":"Code Description 200 Default Response"},{"location":"api/v2/#examples_5","title":"Examples","text":"<pre><code>/v2/history/get_transacted_accounts?account=eosriobrazil&amp;direction=out\n</code></pre>"},{"location":"api/v2/#v2historyget_transaction","title":"/v2/history/get_transaction","text":""},{"location":"api/v2/#get_6","title":"GET","text":""},{"location":"api/v2/#summary_6","title":"Summary","text":"<p>Get transaction by id</p>"},{"location":"api/v2/#description_6","title":"Description","text":"<p>Get a transaction with all its actions.</p>"},{"location":"api/v2/#parameters_6","title":"Parameters","text":"Name Located in Description Required Schema id query Transaction ID Yes string"},{"location":"api/v2/#responses_6","title":"Responses","text":"Code Description 200 Default Response"},{"location":"api/v2/#examples_6","title":"Examples","text":"<pre><code>v2/history/get_transaction?id=eec44c2ab2c2330e88fdacd3cd4c63838adb60da679ff12f299a4341fd036658\n</code></pre>"},{"location":"api/v2/#v2historyget_transfers","title":"/v2/history/get_transfers","text":""},{"location":"api/v2/#get_7","title":"GET","text":""},{"location":"api/v2/#summary_7","title":"Summary","text":"<p>Get token transfers</p>"},{"location":"api/v2/#description_7","title":"Description","text":"<p>Get token transfers from contracts using the eosio.token standard.</p>"},{"location":"api/v2/#parameters_7","title":"Parameters","text":"Name Located in Description Required Schema from query Source account No string to query Destination account No string symbol query Token symbol No string contract query Token contract No string skip query Skip [n] actions (pagination) No integer limit query Limit of [n] actions per page No integer after query Filter actions after specified date (ISO8601) No string before query Filter actions before specified date (ISO8601) No string"},{"location":"api/v2/#responses_7","title":"Responses","text":"Code Description 200 Default Response"},{"location":"api/v2/#examples_7","title":"Examples","text":"<p>Get all transfers from <code>eosriobrazil</code> account <pre><code>/v2/history/get_transfers?from=eosriobrazil\n</code></pre></p> <p>Get all transfers from <code>eosriobrazil</code> account to <code>eosio.ramfee</code> account <pre><code>/v2/history/get_transfers?from=eosriobrazil&amp;to=eosio.ramfee\"\n</code></pre></p> <p>Get all transfers from <code>eosriobrazil</code> account to <code>eosio.ramfee</code> account after November 2019 <pre><code>/v2/history/get_transfers?from=eosriobrazil&amp;to=eosio.ramfee&amp;after=2019-11-01T00:00:00.000Z\n</code></pre></p> <p></p>"},{"location":"api/v2/#v2stateget_account","title":"/v2/state/get_account","text":""},{"location":"api/v2/#get_8","title":"GET","text":""},{"location":"api/v2/#summary_8","title":"Summary","text":"<p>Get account summary</p>"},{"location":"api/v2/#description_8","title":"Description","text":"<p>Get all data related to an account.</p>"},{"location":"api/v2/#parameters_8","title":"Parameters","text":"Name Located in Description Required Schema account query Account name No string"},{"location":"api/v2/#responses_8","title":"Responses","text":"Code Description 200 Default Response"},{"location":"api/v2/#examples_8","title":"Examples","text":"<pre><code>/v2/state/get_account?account=eosio\n</code></pre>"},{"location":"api/v2/#v2stateget_key_accounts","title":"/v2/state/get_key_accounts","text":""},{"location":"api/v2/#get_9","title":"GET","text":""},{"location":"api/v2/#summary_9","title":"Summary","text":"<p>Get accounts by public key</p>"},{"location":"api/v2/#description_9","title":"Description","text":"<p>Get all accounts with specified public key.</p>"},{"location":"api/v2/#parameters_9","title":"Parameters","text":"Name Located in Description Required Schema public_key query Public key Yes string"},{"location":"api/v2/#responses_9","title":"Responses","text":"Code Description 200 Default Response"},{"location":"api/v2/#examples_9","title":"Examples","text":"<pre><code>/v2/state/get_key_accounts?public_key=EOS8fDDZm7ommT5XBf9MPYkRioXX6GeCUeSNkTpimdwKon5bNAVm7\"\n</code></pre>"},{"location":"api/v2/#v2stateget_key_accounts_1","title":"/v2/state/get_key_accounts","text":""},{"location":"api/v2/#post","title":"POST","text":""},{"location":"api/v2/#summary_10","title":"Summary","text":"<p>Get accounts by public key</p>"},{"location":"api/v2/#description_10","title":"Description","text":"<p>Get all accounts with specified public key.</p>"},{"location":"api/v2/#parameters_10","title":"Parameters","text":"Name Located in Description Required Schema public_key body Public key Yes string"},{"location":"api/v2/#responses_10","title":"Responses","text":"Code Description 200 Default Response"},{"location":"api/v2/#examples_10","title":"Examples","text":"<pre><code>/v2/state/get_key_accounts - body: {\"public_key\":\"EOS8fDDZm7ommT5XBf9MPYkRioXX6GeCUeSNkTpimdwKon5bNAVm7\"}\n</code></pre>"},{"location":"api/v2/#v2stateget_links","title":"/v2/state/get_links","text":""},{"location":"api/v2/#get_10","title":"GET","text":""},{"location":"api/v2/#summary_11","title":"Summary","text":"<p>Get permission links</p>"},{"location":"api/v2/#description_11","title":"Description","text":"<p>Get permission links from an account.</p>"},{"location":"api/v2/#parameters_11","title":"Parameters","text":"Name Located in Description Required Schema account name (query) Account name Yes string code (query) Contract name No string action (query) Method name No string permission (query) Permission name No string"},{"location":"api/v2/#responses_11","title":"Responses","text":"Code Description 200 Default Response"},{"location":"api/v2/#examples_11","title":"Examples","text":"<pre><code>/v2/state/get_links?account=eosriobrazil\n</code></pre>"},{"location":"api/v2/#v2stateget_proposals","title":"/v2/state/get_proposals","text":""},{"location":"api/v2/#get_11","title":"GET","text":""},{"location":"api/v2/#summary_12","title":"Summary","text":"<p>Get proposals</p>"},{"location":"api/v2/#description_12","title":"Description","text":"<p>Get all available proposals.</p>"},{"location":"api/v2/#parameters_12","title":"Parameters","text":"Name Located in Description Required Schema proposer query Filter by proposer No string proposal query Filter by proposal name No string account query Filter by either requested or provided account No string requested query Filter by requested account No string provided query Filter by provided account No string executed query Filter by execution status No boolean track query Total results to track (count) No string skip query Skip [n] actions (pagination) No integer limit query Limit of [n] actions per page No integer"},{"location":"api/v2/#responses_12","title":"Responses","text":"Code Description 200 Default Response"},{"location":"api/v2/#examples_12","title":"Examples","text":"<pre><code>/v2/state/get_proposals?proposer=eosriobrazil\n</code></pre>"},{"location":"api/v2/#v2stateget_tokens","title":"/v2/state/get_tokens","text":""},{"location":"api/v2/#get_12","title":"GET","text":""},{"location":"api/v2/#summary_13","title":"Summary","text":"<p>Get tokens from account</p>"},{"location":"api/v2/#description_13","title":"Description","text":"<p>Retrieve all tokens holded by an account.</p>"},{"location":"api/v2/#parameters_13","title":"Parameters","text":"Name Located in Description Required Schema account query Account name Yes string"},{"location":"api/v2/#responses_13","title":"Responses","text":"Code Description 200 Default Response"},{"location":"api/v2/#examples_13","title":"Examples","text":"<pre><code>/v2/state/get_tokens?account=eosriobrazil\n</code></pre>"},{"location":"api/v2/#v2stateget_voters","title":"/v2/state/get_voters","text":""},{"location":"api/v2/#get_13","title":"GET","text":""},{"location":"api/v2/#summary_14","title":"Summary","text":"<p>Get voters</p>"},{"location":"api/v2/#description_14","title":"Description","text":"<p>Get all voting accounts.</p>"},{"location":"api/v2/#parameters_14","title":"Parameters","text":"Name Located in Description Required Schema producer query Filter by voted producer (comma separated) No string skip query Skip [n] actions (pagination) No integer limit query Limit of [n] actions per page No integer"},{"location":"api/v2/#responses_14","title":"Responses","text":"Code Description 200 Default Response"},{"location":"api/v2/#examples_14","title":"Examples","text":"<p>Get all <code>eosriobrazil</code> voters <pre><code>/v2/state/get_voters?producer=eosriobrazil\n</code></pre></p> <p>Get only the first 3 responses <pre><code>/v2/state/get_voters?producer=eosriobrazil&amp;limit=3\n</code></pre></p> <p></p>"},{"location":"api/v2/#v2health","title":"/v2/health","text":""},{"location":"api/v2/#get_14","title":"GET","text":""},{"location":"api/v2/#summary_15","title":"Summary:","text":"<p>API Service Health Report</p>"},{"location":"api/v2/#responses_15","title":"Responses","text":"Code Description 200 Default Response"},{"location":"dev/endpoint/","title":"Hyperion Open History Endpoint List","text":"<p>Tip</p> <p>Check the endpoint status at https://bloks.io/hyperion</p>"},{"location":"dev/endpoint/#1-mainnets","title":"1. Mainnets:","text":""},{"location":"dev/endpoint/#eos","title":"EOS","text":"url docs explorer https://api.eossweden.org/v2 docs https://eos.eosusa.io/v2 docs"},{"location":"dev/endpoint/#wax","title":"WAX","text":"url docs explorer https://wax.eosrio.io/v2 docs explorer https://api.waxsweden.org/v2 docs https://wax.eosusa.io/v2 docs https://wax.eosphere.io/v2 docs https://wax.pink.gg/v2 docs https://wax.blokcrafters.io/v2 docs https://hyperion.wax.eosdetroit.io/v2 docs https://wax.cryptolions.io/v2 docs explorer"},{"location":"dev/endpoint/#telos","title":"TELOS","text":"url docs explorer https://mainnet.telos.net/v2 docs explorer https://telos.eosrio.io/v2 docs explorer https://telos.eosusa.io/v2 docs https://telos.eosphere.io/v2 docs https://telos.caleos.io/v2 docs explorer https://hyperion.telos.eosdetroit.io/v2 docs"},{"location":"dev/endpoint/#uos","title":"UOS","text":"url docs explorer https://ultra.eosusa.io/v2 docs"},{"location":"dev/endpoint/#instar","title":"INSTAR","text":"url docs explorer https://instar.eosusa.io/v2 docs"},{"location":"dev/endpoint/#proton","title":"Proton","text":"url docs explorer https://proton.eosusa.io/v2 docs https://proton.cryptolions.io/v2 docs explorer"},{"location":"dev/endpoint/#fio","title":"FIO","text":"url docs explorer https://fio.cryptolions.io/v2 docs explorer https://fio.eosusa.io/v2 docs"},{"location":"dev/endpoint/#libre","title":"LIBRE","text":"url docs explorer https://libre.eosusa.io/v2 docs"},{"location":"dev/endpoint/#aikonore","title":"Aikon/ORE","text":"url docs explorer https://ore.eosusa.io/v2 docs"},{"location":"dev/endpoint/#coffe","title":"Coffe","text":"url docs explorer https://hyperion.coffe.io/v2 docs"},{"location":"dev/endpoint/#ux","title":"UX","text":"url docs explorer https://ux.eosusa.io/v2 docs"},{"location":"dev/endpoint/#2-testnets","title":"2. Testnets:","text":""},{"location":"dev/endpoint/#kylin-eos-testnet","title":"Kylin EOS Testnet","text":"url docs explorer https://kylin.eossweden.org/v2 docs https://kylin.eosusa.io/v2 docs"},{"location":"dev/endpoint/#jungle-4-eos-testnet","title":"Jungle 4 EOS Testnet","text":"url docs explorer https://jungle.eosusa.io/v2 docs"},{"location":"dev/endpoint/#wax-testnet","title":"WAX Testnet","text":"url docs explorer https://testnet.wax.pink.gg/v2 docs https://wax-test.blokcrafters.io/v2 docs https://test.wax.eosusa.io/v2 docs"},{"location":"dev/endpoint/#telos-testnet","title":"Telos Testnet","text":"url docs explorer https://testnet.telos.net/v2 docs explorer https://test.telos.eosusa.io/v2 docs https://testnet.telos.caleos.io/v2 docs explorer"},{"location":"dev/endpoint/#uos-testnet","title":"UOS Testnet","text":"url docs explorer https://test.ultra.eosusa.io/v2 docs"},{"location":"dev/endpoint/#proton-testnet","title":"Proton Testnet","text":"url docs explorer https://test.proton.eosusa.io/v2 docs https://testnet.protonchain.com/v2 docs explorer"},{"location":"dev/endpoint/#fio-testnet","title":"FIO Testnet","text":"url docs explorer https://test.fio.eosusa.io/v2 docs"},{"location":"dev/endpoint/#libre-testnet","title":"LIBRE Testnet","text":"url docs explorer https://test.libre.eosusa.io/v2 docs"},{"location":"dev/endpoint/#ux-testnet","title":"UX Testnet","text":"url docs explorer https://test.ux.eosusa.io/v2 docs"},{"location":"dev/howtouse/","title":"How to use","text":""},{"location":"dev/howtouse/#1-javascript-client-using-https","title":"1. JavaScript client using https","text":"<p>Fetching data from Hyperion using JavaScript is quite simple. </p> <p>To do that we're going to use https library to make the requests.</p> <p>So, the first step is to include the https lib: <pre><code>const https = require('https');\n</code></pre></p> <p>In this example, we will fetch the WAX chain for the transaction id:  14b36232e919307090c7e9a7a2b17915f40bf0ce72734647c9f8c145c110dda0.</p> <p>This is the query: <pre><code>\"https://wax.hyperion.eosrio.io/v2/history/get_transaction?id=14b36232e919307090c7e9a7a2b17915f40bf0ce72734647c9f8c145c110dda0\"\n</code></pre></p> <p>Now, let's create a Promise function that will receive the tx id as parameter and save it into a variable called <code>getTransaction</code>: <pre><code>let getTransaction = function (args) {\nlet url = \"https://wax.hyperion.eosrio.io/v2/history/get_transaction?id=\" + args;\nreturn new Promise(function (resolve) {\nhttps.get(url, (resp) =&gt; {\nlet data = '';\n\n// A chunk of data has been recieved.\nresp.on('data', (chunk) =&gt; {\ndata += chunk;\n});\n\n// The whole response has been received. Print out the result.\nresp.on('end', () =&gt; {\nresolve(JSON.parse(data));\n});\n});\n\n})\n};\n</code></pre></p> <p>And finally, let's call the function passing the tx id as parameter: <pre><code>(async() =&gt;{\nlet id = \"14b36232e919307090c7e9a7a2b17915f40bf0ce72734647c9f8c145c110dda0\"\ngetTransaction(id).then(data =&gt; {\nconsole.log(data);\n});\n})();\n</code></pre></p> <p>Request response:</p> <pre><code>{\ntrx_id: '14b36232e919307090c7e9a7a2b17915f40bf0ce72734647c9f8c145c110dda0',\nlib: 49925737,\nactions: [\n{\naction_ordinal: 1,\ncreator_action_ordinal: 0,\nact: [Object],\ncontext_free: false,\nelapsed: '0',\naccount_ram_deltas: [Array],\n'@timestamp': '2020-04-08T23:02:02.500',\nblock_num: 49924003,\nproducer: 'zenblockswax',\ntrx_id: '14b36232e919307090c7e9a7a2b17915f40bf0ce72734647c9f8c145c110dda0',\nglobal_sequence: 260804226,\ncpu_usage_us: 1173,\nnet_usage_words: 36,\ninline_count: 3,\ninline_filtered: false,\nreceipts: [Array],\ncode_sequence: 7,\nabi_sequence: 7,\nnotified: [Array],\ntimestamp: '2020-04-08T23:02:02.500'\n},\n{\naction_ordinal: 2,\ncreator_action_ordinal: 1,\nact: [Object],\ncontext_free: false,\nelapsed: '0',\naccount_ram_deltas: [Array],\n'@timestamp': '2020-04-08T23:02:02.500',\nblock_num: 49924003,\nproducer: 'zenblockswax',\ntrx_id: '14b36232e919307090c7e9a7a2b17915f40bf0ce72734647c9f8c145c110dda0',\nglobal_sequence: 260804227,\nreceipts: [Array],\ncode_sequence: 6,\nabi_sequence: 6,\nnotified: [Array],\ntimestamp: '2020-04-08T23:02:02.500'\n}\n],\nquery_time_ms: 45.26\n}\n</code></pre> <p></p>"},{"location":"dev/howtouse/#2-third-party-library","title":"2. Third party library","text":"<p>There is a third party Javascript library made by EOS Cafe.  Refer to their github for further information: https://github.com/eoscafe/hyperion-api</p> <p>Note</p> <p>This is a third party library and is not maintained by EOS Rio</p>"},{"location":"dev/stream_client/","title":"Hyperion Stream Client","text":""},{"location":"dev/stream_client/#streaming-client-for-hyperion-history-api-v3","title":"Streaming Client for Hyperion History API (v3+)","text":""},{"location":"dev/stream_client/#usage","title":"Usage","text":""},{"location":"dev/stream_client/#supported-environments","title":"Supported Environments","text":"<ul> <li>Node.js v16 and up<ul> <li>ES Module</li> <li>CommonJS</li> </ul> </li> <li>Browsers<ul> <li>ES Module - Angular, React and other frameworks</li> <li>UMD</li> </ul> </li> </ul>"},{"location":"dev/stream_client/#quick-start","title":"Quick Start","text":""},{"location":"dev/stream_client/#installing-via-npm-package","title":"Installing via npm package","text":"<pre><code>npm install @eosrio/hyperion-stream-client --save\n</code></pre>"},{"location":"dev/stream_client/#importing-the-client","title":"Importing the client","text":"<p>ESM (Node and Browser): <pre><code>import {HyperionStreamClient} from \"@eosrio/hyperion-stream-client\";\n</code></pre></p> <p>CommonJs (Node): <pre><code>const {HyperionStreamClient} = require('@eosrio/hyperion-stream-client');\n</code></pre></p>"},{"location":"dev/stream_client/#browser-library-served-from-public-hyperion-apis","title":"Browser library (served from public Hyperion APIs)","text":"<p>Without installing via npm, you can also load the webpack bundle directly:</p> <pre><code>&lt;script src=\"https://&lt;ENDPOINT&gt;/stream-client.js\"&gt;&lt;/script&gt;\n</code></pre> <p>Where <code>&lt;ENDPOINT&gt;</code> is the Hyperion API (e.g. <code>https://eos.hyperion.eosrio.io</code>)</p> <p>For other usages, the bundle is also available at <code>dist/hyperion-stream-client.js</code></p>"},{"location":"dev/stream_client/#1-connection","title":"1. Connection","text":"<p>Set up the endpoint that you want to fetch data from:</p> <pre><code>const client = new HyperionStreamClient({\nendpoint: 'https://example.com',\ndebug: true,\nlibStream: false\n});\n</code></pre> <p><code>https://example.com</code> is the host, from where <code>https://example.com/v2/history/...</code> is served.</p> <ul> <li>set <code>libStream</code> to <code>true</code> if you want to enable a stream of only irreversible data.   Don't forget to attach the handler using the method: <code>setAsyncLibDataHandler(handler: AsyncHandlerFunction)</code></li> <li>set <code>debug</code>  to <code>true</code> to print debugging messages</li> </ul>"},{"location":"dev/stream_client/#2-making-requests","title":"2. Making requests","text":"<p>to ensure the client is connected, requests should be made only after calling the <code>client.connect()</code> method, refer to examples below;</p>"},{"location":"dev/stream_client/#21-action-stream-clientstreamactions","title":"2.1 Action Stream - client.streamActions","text":"<p><code>client.streamActions(request: StreamActionsRequest): void</code></p> <ul> <li><code>contract</code> - contract account</li> <li><code>action</code> - action name</li> <li><code>account</code> - notified account name</li> <li><code>start_from</code> - start reading on block or on a specific date. 0=disabled means it will read starting from HEAD block.</li> <li><code>read_until</code> - stop reading on block  (0=disable) or on a specific date (0=disabled)</li> <li><code>filters</code> - actions filter (more details below)  </li> </ul> <p>Notes</p> <ul> <li>Block number can be either positive or negative - E.g.: 700 (start from block 700)</li> <li>In case of negative block number, it will be subtracted from the HEAD - E.g.: -150 (since 150 blocks ago)</li> <li>Date format (ISO 8601) - e.g. 2020-01-01T00:00:00.000Z</li> </ul> <pre><code>import {HyperionStreamClient, StreamClientEvents} from \"@eosrio/hyperion-stream-client\";\nconst client = new HyperionStreamClient({\nendpoint: \"https://sidechain.node.tibs.app\",\ndebug: true,\nlibStream: false\n});\nclient.on(StreamClientEvents.LIBUPDATE, (data: EventData) =&gt; {\nconsole.log(data);\n});\nclient.on('connect', () =&gt; {\nconsole.log('connected!');\n});\nclient.setAsyncDataHandler(async (data) =&gt; {\nconsole.log(data);\n// process incoming data, replace with your code\n// await processSomethingHere();\n})\nawait client.connect();\nclient.streamActions({\ncontract: 'eosio',\naction: 'voteproducer',\naccount: '',\nstart_from: '2020-03-15T00:00:00.000Z',\nread_until: 0,\nfilters: [],\n});\n</code></pre>"},{"location":"dev/stream_client/#211-act-data-filters","title":"2.1.1 Act Data Filters","text":"<p>You can set up filters to refine your stream. Filters should use fields following the Hyperion Action Data Structure, such as:</p> <ul> <li><code>act.data.producers</code> (on eosio::voteproducer)</li> <li><code>@transfer.to</code> (here the @ prefix is required since transfers have special mappings)</li> </ul> <p>Please refer to the mapping definitions to know which data fields are available</p> <p>For example, to filter the stream for every transfer made to the <code>eosio.ramfee</code> account:</p> <pre><code>client.streamActions({\ncontract: 'eosio.token',\naction: 'transfer',\naccount: 'eosio',\nstart_from: 0,\nread_until: 0,\nfilters: [\n{field: '@transfer.to', value: 'eosio.ramfee'}\n],\n});\n</code></pre> <p>To refine even more your stream, you could add more filters. Remember that adding more filters will result in AND operations. For OR operations setup another request.</p>"},{"location":"dev/stream_client/#22-delta-stream-contract-rows-clientstreamdeltas","title":"2.2 Delta Stream (contract rows) - client.streamDeltas","text":"<p><code>client.streamDeltas(request: StreamDeltasRequest): void</code></p> <ul> <li><code>code</code> - contract account</li> <li><code>table</code> - table name</li> <li><code>scope</code> - table scope</li> <li><code>payer</code> - ram payer</li> <li><code>start_from</code> - start reading on block or on a specific date. 0=disabled means it will read starting from HEAD block.</li> <li><code>read_until</code> - stop reading on block  (0=disable) or on a specific date (0=disabled)</li> </ul> <p>Example:</p> <p>Referring to the same pattern as the action stream example above, one could also include a delta stream request <pre><code>client.streamDeltas({\ncode: 'eosio.token',\ntable: '*',\nscope: '',\npayer: '',\nstart_from: 0,\nread_until: 0,\n});\n</code></pre></p> <p>Note: Delta filters are planned to be implemented soon.</p>"},{"location":"dev/stream_client/#3-handling-data","title":"3. Handling Data","text":"<p>Incoming data handler is defined via the <code>client.setAsyncDataHandler(async (data)=&gt; void)</code> method</p> <p>if you set <code>libStream</code> to <code>true</code> another stream of only irreversible data will be available. Don't forget to attach the handler using the method: <code>setAsyncLibDataHandler(handler: AsyncHandlerFunction)</code></p> <p>data object is structured as follows:</p> <ul> <li><code>type</code> - action | delta</li> <li><code>mode</code> - live | history</li> <li><code>content</code> - Hyperion Data Structure (see action index   and delta index   templates)</li> </ul> <pre><code>client.setAsyncDataHandler(async (data) =&gt; {\nconsole.log(data);\n// process incoming data, replace with your code\n// await processSomethingHere();\n})\n// irreversible data stream only for when libStream: true on client connection setup\nclient.setAsyncLibDataHandler(async (data) =&gt; {\nconsole.log(data);\n// process incoming data, replace with your code\n// await processSomethingHere();\n})\n</code></pre> <p>Tip</p> <p>In this link you can find some useful information about load-balancing multiple Socket.IO servers.</p>"},{"location":"providers/get-started/","title":"Providers - Getting Started","text":"<p>Hyperion runs exclusively in Linux, we recommend Ubuntu 22.04. The infrastructure requires <code>systemd</code> to be enabled, if you are in WSL2 please follow this guide before the installation.</p> <p>To install Hyperion as a provider you can choose one of the following routes:</p> <p>Considering a development environment, we recommend performing the installation process with the Automated Installation Script.</p> <p>Automated Installation Script</p> <p>For production/advanced installations, use the Manual Installation steps</p> <p>Manual Installation</p> <p>Testing/Development</p> <p>For docker based deployments, use the Docker Guide</p> <p>Docker Installation</p> <p>For LXD pre-configured Hyperion instance, use the LXD Guide</p> <p>LXD Installation</p>"},{"location":"providers/repair/","title":"Repairing Indexed Data","text":""},{"location":"providers/repair/#forks-and-missed-blocks","title":"Forks and Missed Blocks","text":"<p>After version 3.3.9-5, Hyperion includes a tool to repair indexed data. This tool can be used to fix any unlinked block (forked) or missed blocks in the indexer. Usually forks are handled by the indexer itself, but there was an issue with the state-history plugin in the past, that caused fork events to be omitted during live indexing. For cases like that and others, this tool can be used to fix the data and check integrity.</p>"},{"location":"providers/repair/#1-test-the-connection","title":"1. Test the connection","text":"<p>Use the following command to test the connection to the indexer:</p> <pre><code>./hyp-repair connect --host \"ws://127.0.0.1:7002\"\n</code></pre> <p>7002 is the default port for the indexer control websocket, which can be configured on the connections.json file under <code>chains -&gt; YOUR_CHAIN -&gt; control_port</code>.</p> <p>Connection successful</p> <p>If the connection is successful, you should see the following output:</p> <pre><code>\u2705  Hyperion Indexer Online - ws://127.0.0.1:7002\n</code></pre>"},{"location":"providers/repair/#2-scan","title":"2. Scan","text":"<p>Scan for forks or missing blocks</p> <pre><code>./hyp-repair scan local\n# Specify a range\n./hyp-repair scan local --first 1000000 --last 2000000\n# Specify the output pathname\n./hyp-repair scan local -o ./local\n</code></pre> <p>If you don't specify a range, the scan will start in reverse, from the last indexed block until the first block in Elasticsearch. And if no output path is specified, the scan will be saved in the <code>.repair</code> folder.</p>"},{"location":"providers/repair/#3-verify-the-saved-scan-file","title":"3. Verify the saved scan file","text":"<pre><code># Example, your file name may be different\n./hyp-repair view .repair/local-4-25334-missing-blocks.json\n</code></pre>"},{"location":"providers/repair/#4-request","title":"4. Request","text":""},{"location":"providers/repair/#41-missing-blocks","title":"4.1. Missing blocks","text":"<p>Request the indexer to fill the missing blocks:</p> <pre><code># Example, your file name may be different\n./hyp-repair fill-missing local .repair/local-4-25334-missing-blocks.json\n</code></pre> <p></p>"},{"location":"providers/repair/#42-forked-blocks","title":"4.2. Forked blocks","text":"<p>In the case of forked blocks (blocks that were indexed but are not linked to the previous block), you can use:</p> <pre><code># Dry-run first to check the proposed removals\n./hyp-repair repair local .repair/local-4-25334-forked-blocks.json --dry\n\n# If everything looks good, run the repair\n./hyp-repair repair local .repair/local-4-25334-forked-blocks.json\n</code></pre> <p><code>hyp-repair repair</code> will first remove the forked blocks and the corresponding actions, deltas and state tables. Then it will request the indexer to fill the missing blocks.</p> <p>Verify the results</p> <p>Once the repair is completed, you can run the scan again to verify that there are no more missing blocks or forks.</p> <p></p>"},{"location":"providers/update/","title":"Updating","text":""},{"location":"providers/update/#update-and-build","title":"Update and Build","text":"<p>Checkout the main branch or the specific tag you wish to update to, fetch new changes with <code>git pull</code> and run <code>npm install</code> (it will update all packages and automatically build afterwards) <pre><code>git checkout main\ngit pull\nnpm install\n</code></pre></p> <p>After the rebuild, you need to restart the indexer and api. <pre><code>cd ~/hyperion\n./run.sh [chain name]-indexer\n./run.sh [chain name]-api\n</code></pre></p>"},{"location":"providers/help/kibana/","title":"Kibana","text":"<p>The purpose here is to guide you through some basic steps using and configuring the Kibana.  For more detailed information, please, refer to the official documentation.</p>"},{"location":"providers/help/kibana/#running-kibana-with-systemd","title":"Running Kibana with <code>systemd</code>","text":"<p>To configure Kibana to start automatically when the system boots up, run the following commands: <pre><code>sudo /bin/systemctl daemon-reload\nsudo /bin/systemctl enable kibana.service\n</code></pre> Kibana can be started and stopped as follows: <pre><code>sudo systemctl start kibana.service\nsudo systemctl stop kibana.service\n</code></pre> These commands provide no feedback as to whether Kibana was started successfully or not.  Log information can be accessed via <code>journalctl -u kibana.service.</code></p>"},{"location":"providers/help/kibana/#opening-kibana","title":"Opening Kibana","text":"<p>Open http://localhost:5601 and check if you can access Kibana.</p> <p>If Kibana asks for credentials, the default user and password is:</p> <pre><code>user: elastic\npassword: changeme\n</code></pre> <p>If you installed Kibana through the <code>install_infra.sh</code> script, check your elastic_pass.txt file.</p> <p>If you can't access, check your credentials on your config file.</p>"},{"location":"providers/help/kibana/#creating-index-pattern","title":"Creating index pattern","text":"<ol> <li> <p>To create a new index pattern, go to <code>Management</code></p> <p></p> </li> <li> <p>Click on <code>Index Patterns</code> at the left menu</p> <p></p> </li> <li> <p>Click on <code>Create index pattern</code></p> <p></p> </li> <li> <p>Enter desired index pattern and click on <code>&gt; Next step</code></p> <p></p> <p>Tip</p> <p>Index Pattern List:  <code>eos-abi-*</code> <code>eos-action-*</code> <code>eos-block-*</code> <code>eos-logs-*</code> <code>eos-delta-*</code>   Where <code>eos</code> is the name of the chain.</p> </li> <li> <p>Select a time filter, if there are any, and click on <code>Create index pattern</code></p> <p></p> <p></p> </li> </ol>"},{"location":"providers/help/rabbit/","title":"RabbitMQ","text":""},{"location":"providers/help/rabbit/#how-to-delete-all-the-queues-from-rabbitmq","title":"How to Delete all the queues from RabbitMQ?","text":"<p>Warning</p> <p>Be sure you really want to do this! This gonna reset all your rabbitMQ configuration, and return it to the default state.</p>"},{"location":"providers/help/rabbit/#1-using-policies-management-console","title":"1. Using Policies - Management Console","text":"<ul> <li>Go to Management Console at http://localhost:15672</li> <li>Click on Admin tab</li> <li>Policies tab (on the right side)</li> <li>Add Policy</li> <li>Fill Fields<ul> <li>Virtual Host: Select (Default is <code>/hyperion</code>)</li> <li>Name: Expire All Policies(Delete Later)</li> <li>Pattern: .*</li> <li>Apply to: Queues</li> <li>Definition: expires with value 1 (change type from String to Number)</li> </ul> </li> <li>Click on <code>Add / update policy</code></li> <li>Checkout Queues tab again, all queues must be deleted.</li> </ul> <p>Warning</p> <p>You must remove this policy after this operation.</p>"},{"location":"providers/help/rabbit/#2-using-command-line","title":"2. Using command line","text":"<p>First, list your queues: <pre><code>rabbitmqadmin list queues name </code></pre> Then from the list, you'll need to manually delete them one by one: <pre><code>rabbitmqadmin delete queue name='queuename' </code></pre> Because of the output format, doesn't appear you can grep the response from <code>list queues</code>. </p> <p>Alternatively, if you're just looking for a way to clear everything, use: <pre><code>rabbitmqctl stop_app\nrabbitmqctl reset\nrabbitmqctl start_app\n</code></pre></p> <p></p>"},{"location":"providers/install/auto_install/","title":"Automated Installation Script","text":"<p>This installation script is maintained here</p> <p>Tip</p> <p>Learning about the software components of the Hyperion architecture is recommended. This automatic setup will use defaults that are not suitable for all scenarios.</p> <p>Already have some dependencies installed?</p> <p>The usage of this script is recommended only for fresh installations.  If you already have some dependencies installed, please proceed with the manual setup</p> <p>WSL2</p> <p>For Windows installation using WSL2, refer to this guide</p>"},{"location":"providers/install/auto_install/#1-create-a-directory-for-the-installer-files","title":"1. Create a directory for the installer files","text":"<pre><code>mkdir -p ~/.hyperion-installer &amp;&amp; cd ~/.hyperion-installer\n</code></pre>"},{"location":"providers/install/auto_install/#2-unpack-the-latest-installer","title":"2. Unpack the latest installer","text":"<pre><code>wget -qO- https://github.com/eosrio/hyperion-auto-setup/raw/main/install.tar.gz | tar -xvz\n</code></pre>"},{"location":"providers/install/auto_install/#3-install-by-running","title":"3. Install by running","text":"<p>Info</p> <p>The installation script may ask you for the sudo password.</p> <pre><code>./install.sh\n</code></pre>"},{"location":"providers/install/auto_install/#4-verify-installation","title":"4. Verify installation","text":"<ul> <li>Elasticsearch will be listening on port 9200/9300</li> <li>Redis will be listening on port 6379</li> <li>RabbitMQ will be listening on port 5672/15672/25672</li> </ul> <p>Elasticsearch password</p> <p>The elastic account password will be saved on the <code>~/.hyperion-installer/elastic.pass</code> file, please save this on a safe location, as you might need it later on. If you need to reset this password you can do it with the following command:   <pre><code>sudo /usr/share/elasticsearch/bin/elasticsearch-reset-password -u elastic -a -s -b\n</code></pre></p> <ul> <li>RabbitMQ Management UI will be available on http://localhost:15672<ul> <li>user: <code>hyperion_user</code></li> <li>password: <code>hyperion_password</code></li> <li>vhost: <code>hyperion</code></li> <li>changing your credentials is recommended, specially if opening access to the management interface is planned</li> </ul> </li> </ul> <p>To check if Hyperion was successfully built after the auto install script, verify if <code>launcher.js</code> was generated <pre><code>cd ~/hyperion\nstat launcher.js\n</code></pre></p>"},{"location":"providers/install/auto_install/#5-proceed-with-the-configuration","title":"5. Proceed with the configuration","text":"<p>Hyperion Configuration </p>"},{"location":"providers/install/docker/","title":"Hyperion Docker","text":"<ul> <li>Hyperion Docker Repository</li> </ul> <p>Warning</p> <p>Hyperion Docker is not recommended for production environments, only for testing, debugging and local networks.</p> <p>Hyperion Docker is a multi-container Docker application intended to get Hyperion up and running as fast as possible. It will index data from a development chain where you can set your contracts, push some actions and see what happens when querying the Hyperion API.</p> <p>Recommend OS</p> <p>Ubuntu 22.04</p>"},{"location":"providers/install/docker/#architecture","title":"Architecture","text":""},{"location":"providers/install/docker/#layers","title":"Layers","text":"<p>To simplify things, we divided the microservices involved with Hyperion into layers.</p> <ol> <li>Blockchain (Leap with state-history plugin)</li> <li>Hyperion</li> <li>Infra (Elasticsearch, Redis, RabbitMQ)</li> </ol> <p>The first layer would be the Chain itself - <code>Node Service</code>. For Hyperion to work, we need a chain to consume data from. In this layer, we will have a single microservice:</p> <p>Leap Node</p> <p>local chain for data consumption</p> <p>The second layer would be Hyperion itself, which is divided into 2 microservices:</p> <p>Hyperion API</p> <p>This service allows interaction with the indexed data.</p> <p>Hyperion Indexer</p> <p>As its name suggests, this service connects to the Chain to fetch and index data.</p> <p>And finally, in the third layer, which we can understand as <code>Infra Services</code>, there are 3 microservices:</p> <ol> <li>Elasticsearch</li> <li>Redis</li> <li>RabbitMQ</li> </ol> <p>* We added 2 extra microservices to facilitate debugging. These microservices need additional commands to be executed; see running extra tools section.</p> <ol> <li>Kibana</li> <li>RedisCommander</li> </ol>"},{"location":"providers/install/docker/#image-infrastructure","title":"Image Infrastructure","text":"Infra Image <p>Considering this structure, the Project Repository has 3 folders representing each mentioned layer.</p> <ul> <li>../hyperion-docker<ul> <li>hyperion</li> <li>infra</li> <li>nodeos</li> </ul> </li> </ul> <p>In each directory mentioned above, we added a <code>docker-compose.yaml</code> file that will be responsible for starting its respective microservices.</p> <p>For those who have never used <code>docker-compose</code>, it allows the creation of several containers simultaneously. These containers are declared as services.</p> Example <code>docker-compose.yaml</code> <p></p> <p>All services (containers) declared within a <code>docker-compose.yaml</code> share the same network by default. Still, as we will separate the containers into different files, we will need to create a network in Docker that will be shared. The procedure will be detailed below in the configuration process.</p>"},{"location":"providers/install/docker/#getting-started","title":"Getting Started","text":""},{"location":"providers/install/docker/#prerequisites","title":"Prerequisites","text":"<p>Before starting with the containers, we need to make some Linux configurations to ensure we are fine with Elasticsearch. Edit the file <code>/etc/sysctl.conf</code></p> <pre><code>sudo nano /etc/sysctl.conf\n</code></pre> <p>Add the following properties:</p> <pre><code>vm.overcommit_memory=1\nvm.max_map_count=262144\n</code></pre> <p>To ensure the settings are working without restarting the machine, just run:</p> <pre><code>sudo sysctl -w vm.overcommit_memory=1\nsudo sysctl -w vm.max_map_count=262144\n</code></pre> <p>Now let's get started!</p>"},{"location":"providers/install/docker/#infrastructure-layer","title":"Infrastructure Layer","text":""},{"location":"providers/install/docker/#1-clone-the-repository","title":"1. Clone the repository","text":"<p>Clone the repository to your local machine:</p> <pre><code>git clone https://github.com/eosrio/hyperion-docker.git\ncd hyperion-docker\n</code></pre>"},{"location":"providers/install/docker/#2-verify-docker-is-running","title":"2. Verify Docker is running","text":"<p>Make sure Docker is running by executing the following command in the terminal: <pre><code>docker ps\n</code></pre></p> <p>Expected result</p> <p></p>"},{"location":"providers/install/docker/#3-create-shared-network","title":"3. Create shared network","text":"<p>Create a network that will be shared between the containers by running the command: <pre><code>docker network create hyperion\n</code></pre></p> <p>Expected result</p> <p></p>"},{"location":"providers/install/docker/#4-create-the-microservices","title":"4. Create the microservices","text":"<p>Now, let's start creating the microservices of the infrastructure layer. </p> <p>Navigate to the infra directory of the repository and run the following command:</p> <pre><code>cd infra\ndocker compose up -d\n</code></pre> <p><code>-d</code> flag</p> <p>Note that we use the <code>-d</code> flag to run in detached mode, allowing us to continue using the session's command line.</p> <p>This command will create the 3 microservices (Elasticsearch, Redis, RabbitMQ).</p> <p>The first time you run the command, it may take some time for everything to be set up. You can follow the execution log using the command: <pre><code>docker compose logs -f\n</code></pre></p> <p>Press Ctrl+C to end the log reading process.</p> <p></p>"},{"location":"providers/install/docker/#5-running-extra-tools","title":"5. Running extra tools","text":"<p>Assuming that the 3 microservices are up and running, we added 2 extra microservices (Kibana and RedisCommander) to the docker-compose that can be executed with the following command:</p> <pre><code>docker compose --profile tools up -d\n</code></pre> <p>These two microservices are responsible for the graphical interfaces interacting with Redis and Elasticsearch.</p> <p>RedisCommander is a tool that allows you to view the data stored in Redis. It is useful for debugging and checking if the data is being stored correctly.</p> <p>Kibana is a tool used to monitor your Elasticsearch cluster and view the data stored. It is useful for creating dashboards and graphs to visualize the data.</p>"},{"location":"providers/install/docker/#6-check-services","title":"6. Check services","text":"<p>Check if the services are up and running:</p> <ul> <li> <p>RabbitMQ - http://localhost:15672/</p> </li> <li> <p>Kibana - http://localhost:5601/</p> </li> <li> <p>RedisCommander - http://localhost:8089/</p> </li> </ul> <p>Once we have completed the Infrastructure Layer configuration, we can move on to the Leap (nodeos) Layer.</p>"},{"location":"providers/install/docker/#leap-nodeos-layer","title":"Leap (nodeos) Layer","text":"<p>Navigate to the nodeos directory in the repository and run:</p> <p><pre><code>cd ../nodeos\ndocker compose up -d\n</code></pre> This layer was added to the repository assuming that you don't have a configured chain from which  the Hyperion Indexer will consume the data.</p> <p>Once the infrastructure and the blockchain node are configured, we can finally start Hyperion.</p>"},{"location":"providers/install/docker/#hyperion-layer","title":"Hyperion Layer","text":"<p>This layer has 2 microservices, Hyperion API and Hyperion Indexer.</p> <p>To start them, navigate to the hyperion directory and run the following command:</p> <pre><code>cd ../hyperion\ndocker compose up -d\n</code></pre>"},{"location":"providers/install/docker/#troubleshooting","title":"Troubleshooting","text":"<p>If you're having problems accessing Kibana or using Elasticsearch API, you could disable the xpack security on the docker-compose.yml setting it to false:</p> <pre><code>xpack.security.enabled=false\n</code></pre>"},{"location":"providers/install/docker/#next-steps","title":"Next steps","text":"<p>Feel free to change configurations as you like. All configurations files are located in <code>hyperion/config</code> or <code>nodeos/leap/config</code>. </p> <p>For more details, please refer to the Hyperion Configuration Section.</p> <p></p>"},{"location":"providers/install/lxd/","title":"Configuring Hyperion with LXD","text":"<p>We host a LXD image containing a pre-configured Hyperion instance. Please follow the instructions below to get started.</p> <p>For those unfamiliar with LXD, it is a native Linux containerization tool developed by Canonical, the company behind Ubuntu. Thus, it is very well-supported in recent Ubuntu distributions.</p> <p>WSL2</p> <p>For Windows installation using WSL2, refer to this guide before proceeding to make sure <code>systemd</code> is enabled.</p>"},{"location":"providers/install/lxd/#1-install-lxd","title":"1. Install LXD","text":"<p>LXD is pre-installed on Ubuntu Server cloud images, but if it's not available, you can install it using Snap with the following command:</p> <p>Linux Terminal</p> <pre><code>sudo snap install lxd\n</code></pre> <p>non-Snap installations</p> <p>For other Linux distributions or non-Snap installations, please refer to the documentation.</p>"},{"location":"providers/install/lxd/#2-initialize-lxd","title":"2. Initialize LXD","text":"<p>After installation, you must initialize LXD, which involves configuring the network interface, storage, and other things. For this process, run the command below. The prompt will ask some questions, it's fine to use the default values, just hit Enter to proceed.</p> <p>When asked about the default pool size, you can use the default value or set it to a higher value if you have enough disk space. Later on, it's easy to expand to add more storage, but keep in mind you can't shrink an existing pool.</p> <p>Linux Terminal</p> <pre><code>sudo lxd init\n</code></pre>"},{"location":"providers/install/lxd/#3-download-hyperion-image-and-configuration","title":"3. Download Hyperion image and configuration","text":"<p>Now that LXD is running, you need to download the Hyperion image from our repository and then launch the image. You also need to download the device configuration file, which will be used to configure the ports exposed by the container.</p> <p>Linux Terminal</p> <pre><code>wget https://images.eosrio.io/hyperion_3.3.9-5.tar.zst\nwget https://raw.githubusercontent.com/eosrio/hyperion-lxd/main/hyperion-devices.yaml\n</code></pre> <p>Tip</p> <p>This process may take a few minutes, the size of the image is approximately 2.0GB</p>"},{"location":"providers/install/lxd/#4-import-the-hyperion-image","title":"4. Import the Hyperion image","text":"<p>LXC</p> <p>The next command is LXC (Client), which is part of LXD (Daemon). This command is often used to manage resources, and you can learn more about it by typing: <pre><code>lxc --help\n</code></pre></p> <p>Linux Terminal</p> <pre><code>lxc image import hyperion_3.3.9-5.tar.zst --alias hyperion-starter\n</code></pre> <p>When the image import is complete, you can check if it's present by running the command:</p> <pre><code>lxc image ls\n</code></pre> <p>Tip</p> <p>Feel free to delete the downloaded file <code>hyperion_3.3.9-5.tar.zst</code> after importing the image.</p>"},{"location":"providers/install/lxd/#5-create-the-hyperion-container","title":"5. Create the Hyperion container","text":"<p>Now let's create the container with the image. We provided the configuration file <code>hyperion-devices.yaml</code> to configure the ports exposed by the container. You can pass it to the launch command below to streamline the configuration. Feel free to modify the <code>listen</code> port values in the file if you need to. Just keep the <code>connect</code> ports as they are.</p> <p>Linux Terminal</p> <pre><code>lxc launch hyperion-starter hyperion-1 &lt; hyperion-devices.yaml\n</code></pre> <p>Tip</p> <p>If you want to change any device configuration after the container has been started you can use the <code>lxc config device ...</code> command</p> <p>We can verify our created instance with the command <code>lxc ls</code></p> <p>You can also test if everything is working properly by accessing http://localhost:7000/v2/health to get a response from the Hyperion API.</p>"},{"location":"providers/install/lxd/#6-accessing-the-container","title":"6. Accessing the container","text":"<p>At this point the container should be running and you are ready to use Hyperion, you can open a shell inside it with:</p> <p>Linux Terminal</p> <pre><code>lxc exec hyperion-1 -- bash -c 'sudo su - ubuntu'\n</code></pre> <p>You can use the <code>pm2 ls</code> command in the terminal to see the status of the two Hyperion microservices (API and Indexer) and whether they are online or offline.</p> <p>Check PM2 logs to see if the indexer is running: <pre><code>pm2 logs\n</code></pre> Check your elastic password: <pre><code>cat ~/elastic.pass\n</code></pre></p> <p>Tip</p> <p>Using Fish, you can create an alias like the example below: <pre><code>alias hyperion-shell=\"lxc exec hyperion-1 -- bash -c 'sudo su - ubuntu'\"\nfuncsave hyperion-shell\n</code></pre> This way, you just need to type <code>hyperion-shell</code> and you're inside the container with everything you need configured.</p> What's inside the container? <p>inside the container you will find:</p> <ul> <li>ElasticSearch</li> <li>RabbitMq</li> <li>Redis</li> <li>Hyperion-API</li> <li>Hyperion-Indexer</li> <li>Nodeos(Leap)</li> </ul>"},{"location":"providers/install/lxd/#7-accessing-the-services","title":"7. Accessing the services","text":"<p>Our default device configuration includes proxies for Kibana, RabbitMQ Management and the Hyperion API. They can be accessed at:</p> <ul> <li>Kibana: http://localhost:5601<ul> <li>Username: elastic</li> <li>Password: </li> </ul> </li> <li>RabbitMQ Management: http://localhost:15672<ul> <li>Username: guest</li> <li>Password: guest</li> </ul> </li> <li>Hyperion API: http://localhost:7000</li> </ul>"},{"location":"providers/install/lxd/#next-steps","title":"Next steps","text":"<p>Feel free to change configurations as you like. All configurations files are located in <code>~/hyperion</code> or <code>~/nodeos</code></p> <p>For more details, please refer to the Hyperion Configuration Section.</p>"},{"location":"providers/install/manual_install/","title":"Manual Installation","text":"<p>This section describes how to manually install Hyperion and its environment. If you want more control of your installation, this is the way to go.</p> <p>Attention</p> <p>Recommended OS: Ubuntu 22.04</p>"},{"location":"providers/install/manual_install/#dependencies","title":"Dependencies","text":"<p>Below you can find the list of all Hyperion's dependencies:</p> <ul> <li>Elasticsearch 8.X</li> <li>Kibana 8.X</li> <li>RabbitMQ</li> <li>Redis</li> <li>Node.js v18</li> <li>PM2</li> <li>LEAP/NODEOS 3.2.1</li> </ul> <p>On the next steps you will install and configure each one of them.</p> <p>Note</p> <p>The Hyperion Indexer requires Node.js and pm2 to be on the same machine. All other dependencies (Elasticsearch, RabbitMQ, Redis and EOSIO) can be installed on different machines, preferably on a high speed and low latency network. Keep in mind that indexing speed will vary greatly depending on this configuration.</p>"},{"location":"providers/install/manual_install/#elasticsearch","title":"Elasticsearch","text":"<p>Follow the detailed installation instructions on the official Elasticsearch documentation and return to this guide before running it.</p> <p>Info</p> <p>Elasticsearch is not started automatically after installation. We recommend running it with systemd.</p> <p>Note</p> <p>It is very important to know the Elasticsearch directory layout and to understand how the configuration works.</p>"},{"location":"providers/install/manual_install/#configuration","title":"Configuration","text":""},{"location":"providers/install/manual_install/#1-elasticsearch-configuration","title":"1. Elasticsearch configuration","text":"<p>Edit the following lines on <code>/etc/elasticsearch/elasticsearch.yml</code>:</p> <pre><code>cluster.name: CLUSTER_NAME\nbootstrap.memory_lock: true\n</code></pre> <p>The memory lock option will prevent any Elasticsearch heap memory from being swapped out.</p> <p>Warning</p> <p>Setting <code>bootstrap.memory_lock: true</code> will make Elasticsearch try to use all the RAM configured for JVM on startup ( check next step). This can cause the application to crash if you allocate more RAM than available.</p> <p>Note</p> <p>A different approach is to disable swapping on your system.</p> <p>Testing</p> <p>After starting Elasticsearch, you can see whether this setting was applied successfully by checking the value of <code>mlockall</code> in the output from this request:</p> <pre><code>curl -X GET \"localhost:9200/_nodes?filter_path=**.mlockall&amp;pretty\"\n</code></pre>"},{"location":"providers/install/manual_install/#2-heap-size-configuration","title":"2. Heap size configuration","text":"<p>For a optimized heap size, check how much RAM can be allocated by the JVM on your system. Run the following command:</p> <pre><code>java -Xms16g -Xmx16g -XX:+UseCompressedOops -XX:+PrintFlagsFinal Oops | grep Oops\n</code></pre> <p>Check if <code>UseCompressedOops</code> is true on the results and change <code>-Xms</code> and <code>-Xmx</code> to the desired value.</p> <p>Note</p> <p>Elasticsearch includes a bundled version of OpenJDK from the JDK maintainers. You can find it on <code>/usr/share/elasticsearch/jdk</code>.</p> <p>After that, change the heap size by editting the following lines on <code>/etc/elasticsearch/jvm.options</code>:</p> <pre><code>-Xms16g\n-Xmx16g\n</code></pre> <p>Note</p> <p>Xms and Xmx must have the same value.</p> <p>Warning</p> <p>Avoid allocating more than 31GB when setting your heap size, even if you have enough RAM.</p>"},{"location":"providers/install/manual_install/#3-allow-memory-lock","title":"3. Allow memory lock","text":"<p>Override systemd configuration by running <code>sudo systemctl edit elasticsearch</code> and add the following lines:</p> <pre><code>[Service]\nLimitMEMLOCK=infinity\n</code></pre> <p>Run the following command to reload units:</p> <pre><code>sudo systemctl daemon-reload\n</code></pre>"},{"location":"providers/install/manual_install/#4-start-elasticsearch","title":"4. Start Elasticsearch","text":"<p>Start Elasticsearch and check the logs:</p> <pre><code>sudo systemctl start elasticsearch.service\nsudo less /var/log/elasticsearch/CLUSTE_NAME.log\n</code></pre> <p>Enable it to run at startup:</p> <pre><code>sudo systemctl enable elasticsearch.service\n</code></pre> <p>And finally, test the REST API:</p> <pre><code>curl -X GET \"localhost:9200/?pretty\"\n</code></pre> <p>Note</p> <p>Don't forget to check if memory lock worked.</p> <p>The expected result should be something like this:</p> <pre><code>{\n\"name\": \"ip-172-31-5-121\",\n\"cluster_name\": \"CLUSTER_NAME\",\n\"cluster_uuid\": \"FFl8DNcOQV-dVk3p1JDNMA\",\n\"version\": {\n\"number\": \"7.14.1\",\n\"build_flavor\": \"default\",\n\"build_type\": \"deb\",\n\"build_hash\": \"606a173\",\n\"build_date\": \"2021-08-26T00:43:15.323135Z\",\n\"build_snapshot\": false,\n\"lucene_version\": \"8.9.0\",\n\"minimum_wire_compatibility_version\": \"6.8.0\",\n\"minimum_index_compatibility_version\": \"6.0.0-beta1\"\n},\n\"tagline\": \"You Know, for Search\"\n}\n</code></pre>"},{"location":"providers/install/manual_install/#5-set-up-minimal-security","title":"5. Set up minimal security","text":"<p>The Elasticsearch security features are disabled by default. To avoid security problems, we recommend enabling the security pack.</p> <p>To do that, add the following line to the end of the <code>/etc/elasticsearch/elasticsearch.yml</code> file:</p> <pre><code>xpack.security.enabled: true\n</code></pre> <p>Restart Elasticsearch and set the passwords for the cluster:</p> <pre><code>sudo systemctl restart elasticsearch.service\nsudo /usr/share/elasticsearch/bin/elasticsearch-setup-passwords auto\n</code></pre> <p>Keep track of these passwords, we\u2019ll need them again soon.</p> <p>Note</p> <p>You can alternatively use the <code>interactive</code> parameter to manually define your passwords.</p> <p>Attention</p> <p>The minimal security scenario is not sufficient for production mode clusters. Check the documentation for more information.</p>"},{"location":"providers/install/manual_install/#kibana","title":"Kibana","text":"<p>Follow the detailed installation instructions on the official Kibana documentation. Return to this documentation before running it.</p> <p>Info</p> <p>Kibana is not started automatically after installation. We recomend running it with systemd.</p> <p>Note</p> <p>Like on Elasticsearch, it is very important to know the Kibana directory layout and to understand how the configuration works.</p>"},{"location":"providers/install/manual_install/#configuration_1","title":"Configuration","text":""},{"location":"providers/install/manual_install/#1-elasticsearch-security","title":"1. Elasticsearch security","text":"<p>If you have enabled the security pack on Elasticsearch, you need to set up the password on Kibana. Edit the folowing lines on the <code>/etc/kibana/kibana.yml</code> file:</p> <pre><code>elasticsearch.username: \"kibana_system\"\nelasticsearch.password: \"password\"\n</code></pre>"},{"location":"providers/install/manual_install/#2-start-kibana","title":"2. Start Kibana","text":"<p>Start Kibana and check the logs:</p> <pre><code>sudo systemctl start kibana.service\nsudo less /var/log/kibana/kibana.log\n</code></pre> <p>Enable it to run at startup:</p> <pre><code>sudo systemctl enable kibana.service\n</code></pre>"},{"location":"providers/install/manual_install/#rabbitmq","title":"RabbitMQ","text":"<p>Follow the detailed installation instructions on the official RabbitMQ documentation.</p> <p>RabbitMQ should automatically start after installation. Check the documentation for more details on how to manage its service.</p>"},{"location":"providers/install/manual_install/#configuration_2","title":"Configuration","text":""},{"location":"providers/install/manual_install/#1-enable-the-webui","title":"1. Enable the WebUI","text":"<pre><code>sudo rabbitmq-plugins enable rabbitmq_management\n</code></pre>"},{"location":"providers/install/manual_install/#2-add-vhost","title":"2. Add vhost","text":"<pre><code>sudo rabbitmqctl add_vhost hyperion\n</code></pre>"},{"location":"providers/install/manual_install/#3-create-a-user-and-password","title":"3. Create a user and password","text":"<pre><code>sudo rabbitmqctl add_user USER PASSWORD\n</code></pre>"},{"location":"providers/install/manual_install/#4-set-the-user-as-administrator","title":"4. Set the user as administrator","text":"<pre><code>sudo rabbitmqctl set_user_tags USER administrator\n</code></pre>"},{"location":"providers/install/manual_install/#5-set-the-user-permissions-to-the-vhost","title":"5. Set the user permissions to the vhost","text":"<pre><code>sudo rabbitmqctl set_permissions -p hyperion USER \".*\" \".*\" \".*\"\n</code></pre>"},{"location":"providers/install/manual_install/#6-check-access-to-the-webui","title":"6. Check access to the WebUI","text":"<p>Try to access RabbitMQ WebUI at http://localhost:15672 with the user and password you just created.</p>"},{"location":"providers/install/manual_install/#redis","title":"Redis","text":"<pre><code>sudo apt install redis-server\n</code></pre> <p>Redis will also start automatically after installation.</p>"},{"location":"providers/install/manual_install/#configuration_3","title":"Configuration","text":""},{"location":"providers/install/manual_install/#1-update-redis-supervision-method","title":"1. Update Redis supervision method","text":"<p>Change the <code>supervised</code> configuration from <code>supervised no</code> to  <code>supervised systemd</code> on <code>/etc/redis/redis.conf</code>.</p> <p>Note</p> <p>By default, Redis binds to the localhost address. You need to edit <code>bind</code> in the config file if you want to listen to other network.</p>"},{"location":"providers/install/manual_install/#2-restart-redis","title":"2. Restart Redis","text":"<pre><code>sudo systemctl restart redis.service\n</code></pre>"},{"location":"providers/install/manual_install/#nodejs","title":"NodeJS","text":"<pre><code>curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -\nsudo apt-get install -y nodejs\n</code></pre> <p>Attention</p> <p>Make sure to configure npm not to use sudo when installing global packages.</p>"},{"location":"providers/install/manual_install/#pm2","title":"PM2","text":"<pre><code>npm install pm2@latest -g\n</code></pre>"},{"location":"providers/install/manual_install/#configuration_4","title":"Configuration","text":""},{"location":"providers/install/manual_install/#1-configure-for-system-startup","title":"1. Configure for system startup","text":"<pre><code>pm2 startup\n</code></pre>"},{"location":"providers/install/manual_install/#eosio","title":"EOSIO","text":"<pre><code>wget https://github.com/AntelopeIO/leap/releases/download/v3.2.1/leap_3.2.1-ubuntu22.04_amd64.deb\nsudo apt install ./leap_3.2.1-ubuntu22.04_amd64.deb\n</code></pre>"},{"location":"providers/install/manual_install/#configuration_5","title":"Configuration","text":"<p>Add the following configuration to the <code>config.ini</code> file:</p> <pre><code>state-history-dir = \"state-history\"\ntrace-history = true\nchain-state-history = true\nstate-history-endpoint = 127.0.0.1:8080\nplugin = eosio::chain_api_plugin\nplugin = eosio::state_history_plugin\n</code></pre>"},{"location":"providers/install/manual_install/#hyperion","title":"Hyperion","text":"<p>If everything runs smoothly, it's time to install Hyperion! </p> <p>To do that, simply run the following commands:</p> <pre><code>git clone https://github.com/eosrio/hyperion-history-api.git\ncd hyperion-history-api\nnpm install\n</code></pre>"},{"location":"providers/install/manual_install/#proceed-with-hyperion-configuration","title":"Proceed with Hyperion Configuration","text":"<p>Hyperion Setup </p>"},{"location":"providers/install/wsl2/","title":"WSL2 + systemd","text":"<p>Guide on enabling systemd if you are using WSL2 on Windows.</p> <p>Note</p> <p>Optimizing for performance is beyond the scope of this guide, this is intended for learning and development on Hyperion. systemd is now available in WSL2 and required for this guide. Read more</p>"},{"location":"providers/install/wsl2/#1-make-sure-wsl2-is-updated","title":"1. Make sure WSL2 is updated","text":"<p>Windows Terminal</p> <pre><code>wsl --update\n</code></pre>"},{"location":"providers/install/wsl2/#2-install-and-launch-an-ubuntu-2204-instance","title":"2. Install and launch an Ubuntu 22.04 instance","text":"<p>Ubuntu 22.04.1 LTS on Microsoft Store</p>"},{"location":"providers/install/wsl2/#3-enable-systemd","title":"3. Enable systemd","text":"<p>The infrastructure requires <code>systemd</code> to be enabled</p> <p>Linux Terminal</p> <p>As root edit the file /etc/wsl.conf <pre><code>sudo nano /etc/wsl.conf\n</code></pre> Add the following lines <pre><code>[boot]\nsystemd=true\n</code></pre></p>"},{"location":"providers/install/wsl2/#4-restart-the-wsl2-engine","title":"4. Restart the WSL2 engine","text":"<p>Windows Terminal</p> <p>Shutdown all instances <pre><code>wsl --shutdown\n</code></pre> Start your instance <pre><code>wsl -d Ubuntu-22.04\n</code></pre></p>"},{"location":"providers/install/wsl2/#5-proceed-with-the-installation","title":"5. Proceed with the installation","text":"<p>Automatic Installation Script </p> <p>Manual Installation </p>"},{"location":"providers/setup/chain/","title":"Chain Config Reference","text":"<p> Hyperion Configuration</p>"},{"location":"providers/setup/chain/#chain-configuration-reference","title":"Chain Configuration Reference","text":"<p>This section is a quick guide of the config.json file. Here you are going to find a brief explanation of each parameter with its default value and an example of real usage.</p>"},{"location":"providers/setup/chain/#1-api-configuration","title":"1. API Configuration","text":"<ul> <li><code>\"chain_name\": \"EXAMPLE Chain\"</code></li> <li><code>\"server_addr\": \"127.0.0.1\"</code></li> <li><code>\"server_port\": 7000</code></li> <li><code>\"server_name\": \"127.0.0.1:7000\"</code></li> <li><code>\"provider_name\": \"Example Provider\"</code></li> <li><code>\"provider_url\": \"https://example.com\"</code></li> <li><code>\"chain_logo_url\": \"\"</code></li> <li><code>\"enable_caching\": true</code> \u21d2 Set API cache</li> <li><code>\"cache_life\": 1</code> \u21d2 Define the cache life</li> <li><code>\"limits\"</code> \u21d2 Set API response limits</li> <li><code>\"get_actions\": 1000</code></li> <li><code>\"get_voters\": 100</code></li> <li><code>\"get_links\": 1000</code></li> <li><code>\"get_deltas\": 1000</code></li> <li><code>\"access_log\": false</code> \u21d2 Enable log API access.</li> <li><code>\"enable_explorer\": false</code></li> </ul>"},{"location":"providers/setup/chain/#2-settings","title":"2. Settings","text":"<ul> <li><code>\"preview\": false</code> \u21d2 Preview mode - prints worker map and exit</li> <li><code>\"chain\": \"eos\"</code> \u21d2 Chain name (The same used on ecosystem.config.js)</li> <li><code>\"eosio_alias\": \"eosio\"</code></li> <li><code>\"parser\": \"1.8\"</code> \u21d2 Version of the parser to be used</li> <li><code>\"auto_stop\": 300</code> \u21d2 Automatically stop Indexer after X seconds if no more blocks are being processed (0=disable)</li> <li><code>\"index_version\": \"v1\"</code> \u21d2 Set the index version</li> <li><code>\"debug\": false</code> \u21d2 Enable debug mode</li> <li><code>\"rate_monitoring\": true</code> \u21d2 Print data rates for each phase every 5s</li> <li><code>\"bp_logs\": false</code> \u21d2 Enable logs</li> <li><code>\"bp_monitoring\": false</code> \u21d2 Allow monitoring and logging of missed blocks and rounds</li> <li><code>\"ipc_debug_rate\": 60000</code> \u21d2 interval between IPC messaging rate debug messages, setting it to false or 0 will disable   the logging</li> <li><code>\"allow_custom_abi\": false</code> \u21d2 allow using custom ABIs from the custom-abi/ folder, they must match the   pattern --.json. Those ABIs will overwrite on-chain data for the given range."},{"location":"providers/setup/chain/#3-blacklists","title":"3. Blacklists","text":"<p>Blacklist for actions and deltas</p> <ul> <li>\"actions\": []</li> <li>\"deltas\": []</li> </ul>"},{"location":"providers/setup/chain/#4-whitelists","title":"4. Whitelists","text":"<p>Whitelist for actions and deltas</p> <ul> <li>actions\": []</li> <li>\"deltas\": []</li> </ul>"},{"location":"providers/setup/chain/#5-scaling-options","title":"5. Scaling options","text":"<ul> <li><code>\"batch_size\": 10000</code> \u21d2 Parallel reader batch size in blocks</li> <li><code>\"queue_limit\": 50000</code> \u21d2 Queue size limit on rabbitmq</li> <li><code>\"readers\": 1</code> \u21d2 Number of readers</li> <li><code>\"ds_queues\": 1</code> \u21d2 Number of deserializer queues</li> <li><code>\"ds_threads\": 1</code> \u21d2 Number of deserializer threads</li> <li><code>\"ds_pool_size\": 1</code> \u21d2 Deserializer pool size</li> <li><code>\"indexing_queues\": 1</code> \u21d2 Number of indexing queues</li> <li><code>\"ad_idx_queues\": 1</code> \u21d2 Multiplier for action indexing queues</li> <li><code>\"max_autoscale\": 4</code> \u21d2 Max number of readers to autoscale</li> <li><code>\"auto_scale_trigger\": 20000</code> \u21d2 Number of itens on queue to trigger autoscale</li> <li><code>\"routing_mode\": \"heatmap\"</code> \u21d2 We recommend using the <code>heatmap</code> routing algorithm since it uses less memory. If you see   performance issues on large chains with very unbalanced action distribution (majority of the action on a single   contract) you can change it to <code>round_robin</code> to improve throughput. Keep in mind that the <code>round_robin</code> method will   use more RAM as it has larger <code>ds_pool_size</code> values</li> </ul>"},{"location":"providers/setup/chain/#6-indexer-configuration","title":"6. Indexer configuration","text":"<ul> <li><code>\"start_on\": 0</code> \u21d2 Start indexing on block (0=disable)</li> <li><code>\"stop_on\": 0</code> \u21d2 Stop indexing on block  (0=disable)</li> <li><code>\"rewrite\": false</code> \u21d2 Force rewrite the target replay range</li> <li><code>\"purge_queues\": true</code> \u21d2 Clear rabbitmq queues before starting the indexer</li> <li><code>\"live_reader\": false</code> \u21d2 Enable live reader</li> <li><code>\"live_only_mode\": false</code> \u21d2 Only reads realtime data serially</li> <li><code>\"abi_scan_mode\": true</code> \u21d2 Enable abi scan mode (Indicated on the first run)</li> <li><code>\"fetch_block\": true</code> \u21d2 Request full blocks from the state history plugin</li> <li><code>\"fetch_traces\": true</code> \u21d2 Request traces from the state history plugin</li> <li><code>\"disable_reading\": false</code> \u21d2 Completely disable block reading, for lagged queue processing</li> <li><code>\"disable_indexing\": false</code> \u21d2 Disable indexing</li> <li><code>\"process_deltas\": true</code> \u21d2 Read table deltas</li> <li><code>\"max_inline\": 20</code> \u21d2 Max inline actions depth to index</li> </ul>"},{"location":"providers/setup/chain/#7-features","title":"7. Features","text":"<ul> <li><code>\"index_deltas\": true</code> \u21d2 Index common table deltas (see delta on definitions/mappings)</li> <li><code>\"index_transfer_memo\": true</code> \u21d2 Index transfers memo</li> <li><code>\"index_all_deltas\": true</code> \u21d2 Index all table deltas</li> </ul>"},{"location":"providers/setup/chain/#71-streaming","title":"7.1 Streaming","text":"<p>Enable live streaming</p> <ul> <li><code>\"enable\": false</code></li> <li><code>\"traces\": false</code></li> <li><code>\"deltas\": false</code></li> </ul>"},{"location":"providers/setup/chain/#72-tables","title":"7.2 Tables","text":"<p>Tables to fetch</p> <ul> <li><code>\"proposals\": true</code></li> <li><code>\"accounts\": true</code></li> <li><code>\"voters\": true</code></li> <li><code>\"userres\": false</code></li> <li><code>\"delband\": false</code></li> </ul>"},{"location":"providers/setup/chain/#8-prefetch","title":"8. Prefetch","text":"<ul> <li><code>\"read\": 50</code> \u21d2 Stage 1 prefetch size</li> <li><code>\"block\": 100</code> \u21d2 Stage 2 prefetch size</li> <li><code>\"index\": 500</code> \u21d2 Stage 3 prefetch size</li> </ul>"},{"location":"providers/setup/chain/#example","title":"Example","text":"<p>!!! tip For multiple chains, you should have one config file for each chain.</p> <p>Let's suppose that we gonna start Indexing the EOS Mainnet with:</p> <ul> <li>Locally exposed API</li> <li>2 Readers</li> <li>2 Deserializer Queues</li> <li>Live Streaming Enabled with Traces</li> <li>ABI scan already done</li> </ul> <p>The first step is to make a copy of the config file and rename it: <code>chains/example.config.json</code> to <code>chains/eos.config.json</code>. The next step is to edit the file as the following:</p> <pre><code>{\n\"api\": {\n\"chain_name\": \"eos\",\n\"server_addr\": \"127.0.0.1\",\n\"server_port\": 7000,\n\"server_name\": \"127.0.0.1:7000\",\n\"provider_name\": \"Example Provider\",\n\"provider_url\": \"https://example.com\",\n\"chain_logo_url\": \"\",\n\"enable_caching\": true,\n\"cache_life\": 1,\n\"limits\": {\n\"get_actions\": 1000,\n\"get_voters\": 100,\n\"get_links\": 1000,\n\"get_deltas\": 1000\n},\n\"access_log\": false,\n\"enable_explorer\": false\n},\n\"settings\": {\n\"preview\": false,\n\"chain\": \"eos\",\n\"eosio_alias\": \"eosio\",\n\"parser\": \"1.8\",\n\"auto_stop\": 300,\n\"index_version\": \"v1\",\n\"debug\": false,\n\"rate_monitoring\": true,\n\"bp_logs\": false,\n\"bp_monitoring\": false,\n\"ipc_debug_rate\": 60000,\n\"allow_custom_abi\": false\n},\n\"blacklists\": {\n\"actions\": [],\n\"deltas\": []\n},\n\"whitelists\": {\n\"actions\": [],\n\"deltas\": []\n},\n\"scaling\": {\n\"batch_size\": 10000,\n\"queue_limit\": 50000,\n\"readers\": 2,\n\"ds_queues\": 2,\n\"ds_threads\": 1,\n\"ds_pool_size\": 1,\n\"indexing_queues\": 1,\n\"ad_idx_queues\": 1,\n\"max_autoscale\": 4,\n\"auto_scale_trigger\": 20000\n},\n\"indexer\": {\n\"start_on\": 1,\n\"stop_on\": 0,\n\"rewrite\": false,\n\"purge_queues\": true,\n\"live_reader\": false,\n\"live_only_mode\": false,\n\"abi_scan_mode\": false,\n\"fetch_block\": true,\n\"fetch_traces\": true,\n\"disable_reading\": false,\n\"disable_indexing\": false,\n\"process_deltas\": true,\n\"max_inline\": 20\n},\n\"features\": {\n\"streaming\": {\n\"enable\": true,\n\"traces\": true,\n\"deltas\": false\n},\n\"tables\": {\n\"proposals\": true,\n\"accounts\": true,\n\"voters\": true,\n\"userres\": false,\n\"delband\": false\n},\n\"index_deltas\": true,\n\"index_transfer_memo\": true,\n\"index_all_deltas\": true\n},\n\"prefetch\": {\n\"read\": 50,\n\"block\": 100,\n\"index\": 500\n}\n}\n</code></pre> <p> Hyperion Configuration</p>"},{"location":"providers/setup/connections/","title":"Connections Reference","text":"<p> Hyperion Configuration</p>"},{"location":"providers/setup/connections/#connections-configuration-reference","title":"Connections Configuration Reference","text":""},{"location":"providers/setup/connections/#rabbitmq-parameters","title":"RabbitMQ parameters","text":"<ul> <li><code>\"host\":\"127.0.0.1:5672\"</code></li> <li><code>\"api\":\"127.0.0.1:15672\"</code></li> <li><code>\"user\":\"my_user\"</code></li> <li><code>\"pass\":\"my_password\"</code></li> <li><code>\"vhost\":\"hyperion\"</code></li> </ul>"},{"location":"providers/setup/connections/#elasticsearch-parameters","title":"Elasticsearch parameters","text":"<ul> <li><code>\"protocol\": \"http\"</code> \u21d2 Protocol used to connect to Elasticsearch (default: http).</li> <li><code>\"host\":\"127.0.0.1:9200\"</code></li> <li><code>\"ingest_nodes\": [ \"127.0.0.1:9200\"]</code></li> <li><code>\"user\":\"\"</code> \u21d2 User defined on elasticsearch configuration.</li> <li><code>\"pass\":\"\"</code> \u21d2 Password defined on elasticsearch configuration.</li> </ul>"},{"location":"providers/setup/connections/#redis-parameters","title":"Redis parameters","text":"<ul> <li><code>\"host\":\"127.0.0.1\"</code></li> <li><code>\"port\":\"6379\"</code></li> </ul>"},{"location":"providers/setup/connections/#chain-parameters","title":"Chain Parameters","text":"<ul> <li><code>\"name\":\"EOS Mainnet\"</code></li> <li><code>\"chain_id\":\"aca376f206b8fc25a6ed44dbdc66547c36c6c33e3a119ffbeaef943642f0e906\"</code></li> <li><code>\"http\":\"http://127.0.0.1:8888\"</code></li> <li><code>\"ship\":\"ws://127.0.0.1:8080\"</code></li> <li><code>\"WS_ROUTER_HOST\": \"127.0.0.1\"</code> \u21d2 Endpoint used by the Streaming API to connect to the Indexer. This is important when   Indexer and API aren't on the same machine/instance.</li> <li><code>\"WS_ROUTER_PORT\":7001</code> \u21d2 Port used by the Streaming API to connect to the Indexer.</li> </ul>"},{"location":"providers/setup/connections/#example","title":"Example","text":"<p>In this example we have a connection.json file with:</p> <ul> <li>Local RabbitMQ<ul> <li>user: admin</li> <li>pass: 123456</li> </ul> </li> <li>Local Elasticsearch<ul> <li>no user</li> <li>no password</li> </ul> </li> <li>Local Reddis</li> <li>State History connections: <ul> <li>Remote EOS Mainnet</li> <li>Remote sample chain</li> </ul> </li> </ul> <pre><code>{\n\"amqp\": {\n\"host\": \"127.0.0.1:5672\",\n\"api\": \"127.0.0.1:15672\",\n\"user\": \"admin\",\n\"pass\": \"123456\",\n\"vhost\": \"hyperion\"\n},\n\"elasticsearch\": {\n\"protocol\": \"http\",\n\"host\": \"127.0.0.1:9200\",\n\"ingest_nodes\": [\n\"127.0.0.1:9200\"\n],\n\"user\": \"\",\n\"pass\": \"\"\n},\n\"redis\": {\n\"host\": \"127.0.0.1\",\n\"port\": \"6379\"\n},\n\"chains\": {\n\"eos\": {\n\"name\": \"EOS Mainnet\",\n\"chain_id\": \"aca376f206b8fc25a6ed44dbdc66547c36c6c33e3a119ffbeaef943642f0e906\",\n\"http\": \"http://127.0.0.1:8888\",\n\"ship\": \"ws://127.0.0.1:8080\",\n\"WS_ROUTER_PORT\": 7001\n},\n\"sample\": {\n\"name\": \"Sample Mainnet\",\n\"chain_id\": \"9473887b3cd1a897ce03ae5b6a865651747e2e152090f99c1d19d4adf73238fas\",\n\"http\": \"https://sample.io\",\n\"ship\": \"ws://192.168.0.1:8080\",\n\"WS_ROUTER_HOST\": \"127.0.0.1\",\n\"WS_ROUTER_PORT\": 8034\n}\n}\n}\n</code></pre> <p> Hyperion Configuration</p>"},{"location":"providers/setup/hyperion_configuration/","title":"Hyperion Configuration","text":"<p>We've developed a tool to automate the configuration of Hyperion. It initializes the connections with all the dependencies and creates the configuration for each chain you are running.</p> <p>Warning</p> <p>Make sure you are in the installation directory: <pre><code>cd ~/hyperion\n</code></pre></p> <p>Tip</p> <p>Run <code>./hyp-config --help</code> for more details.</p>"},{"location":"providers/setup/hyperion_configuration/#initialize-connections","title":"Initialize connections","text":"<p>First, let's initialize our configuration. Just run:</p> <pre><code>./hyp-config connections init\n</code></pre> <p>Note</p> <p>This command will also check the connection to Elasticsearch, Rabbitmq and Redis. Make sure everything is up and running.</p> <p>You can use <code>./hyp-config connections test</code> to test connectivity at any point and <code>./hyp-config connections reset</code> to back up and remove the current configuration.</p> <p>The initialization command will create a <code>connections.json</code> file that follows the template described here.</p>"},{"location":"providers/setup/hyperion_configuration/#add-new-chain","title":"Add new chain","text":"<p>Now you can proceed and add a new chain to your configuration. Run the following command:</p> <pre><code>./hyp-config new chain eos --http \"http://127.0.0.1:8888\" --ship \"ws://127.0.0.1:8080\"\n</code></pre> <p>This command will create a <code>chains/eos.config.json</code> file that follows the template described here and also configure the state history section of the <code>connections.json</code> file for this chain.</p>"},{"location":"providers/setup/hyperion_configuration/#check-your-chain-configuration","title":"Check your chain configuration","text":"<p>Finally, check your configuration running:</p> <pre><code>./hyp-config list chains\n</code></pre> <p>You should see an output similar to:</p> <p></p>"},{"location":"providers/setup/hyperion_configuration/#running-hyperion","title":"Running Hyperion","text":"<p>We provide scripts to simplify the process of starting and stopping your Hyperion Indexer or API instance.</p>"},{"location":"providers/setup/hyperion_configuration/#starting","title":"Starting","text":"<p>To run the indexer, execute <code>./run.sh [chain name]-indexer</code></p> <p>To run the api, execute <code>./run.sh [chain name]-api</code></p> <p>Examples</p> <p>Starting indexer for \"eos\" chain: <pre><code>./run.sh eos-indexer\n</code></pre> Starting API for \"test\" chain: <pre><code>./run.sh test-api\n</code></pre></p> <p>Note</p> <p>You need to pass the name of the chain you previously created followed by indexer or api to indicate the instance you want to start.</p>"},{"location":"providers/setup/hyperion_configuration/#stopping","title":"Stopping","text":"<p>Use the stop.sh script to stop an instance as follows:</p> <p>Examples</p> <p>Stop API for EOS mainnet: <pre><code>./stop.sh eos-api\n</code></pre> Stop indexer for WAX mainnet: <pre><code>./stop.sh wax-indexer\n</code></pre></p> <p>Note</p> <p>You need to pass the name of the chain you previously created followed by indexer or api to indicate the instance you want to stop.</p> <p>Attention</p> <p>The stop script won't stop Hyperion Indexer immediately, it will first flush the queues. Be aware that this operation could take some time.</p>"},{"location":"providers/setup/hyperion_configuration/#indexer","title":"Indexer","text":"<p>The Hyperion Indexer is configured to perform an abi scan <code>(\"abi_scan_mode\": true)</code> as default. So, on your first run, you'll probably see something like this:</p> <p></p> <p>This an example of an ABI SCAN on the WAX chain.</p> <p>Where:</p> <ul> <li>W (Workers): Number of workers.</li> <li>R (Read): Blocks read from state history and pushing into the blocks queue.</li> <li>C (Consumed): Blocks consumed from blocks queue.</li> <li>A (Actions): Actions being read out of processed blocks.</li> <li>D (Deserialized): Deserializations of the actions.</li> <li>I (Indexed): Indexing of all of the docs.</li> </ul>"},{"location":"providers/setup/hyperion_configuration/#api","title":"API","text":"<p>After running the api, you should see a log like this:</p> <p></p> <p>Now, it's time to play around making some queries. </p> <p>Tip</p> <p>we are using <code>jq</code> to format the json output for better readability</p> <p>if you don't have it installed use</p> <pre><code>sudo apt install jq\n</code></pre> <p>First, let's test the health check endpoint</p> <pre><code>curl -Ss \"http://127.0.0.1:7000/v2/health\" | jq\n</code></pre> View example <p></p> <p>Then we can ask for the last action on chain:</p> <pre><code>curl -Ss \"http://127.0.0.1:7000/v2/history/get_actions?limit=1\" | jq\n</code></pre> View example <p></p> <p>We can do the same for deltas:</p> <pre><code>curl -Ss \"http://127.0.0.1:7000/v2/history/get_deltas?limit=1\" | jq\n</code></pre> View example <p></p> <p>You can check the Swagger UI at: <code>http://127.0.0.1:7000/v2/docs</code> for more information on all the available endpoints</p>"},{"location":"providers/setup/hyperion_configuration/#enabling-streaming","title":"Enabling Streaming","text":"<p>Once your indexer is finished and it's only reading live blocks, you can enable the streaming api if needed. To do so, enable all options under <code>features.streaming</code> in your chain config file</p> <pre><code>\"features\": {\n    \"streaming\": {\n      \"enable\": true,\n      \"traces\": true,\n      \"deltas\": true\n    }\n</code></pre> <p>By default, the stream api will be available on the port 1234, this can be configured by the <code>api.stream_port</code> property in the chain config file.</p> <p>Once you're done configuring, just restart both the indexer and api.</p> <p>A quick test using <code>curl 127.0.0.1:1234/stream/</code> should result in the output <code>{\"code\":0,\"message\":\"Transport unknown\"}</code> meaning the port is ready for websocket connections. Alternatively, you can check the api logs after restart for a <code>Websocket manager loaded!</code> message</p> <p>NGINX</p> <p>if you are using <code>NGINX</code> as your reverse proxy, use the following block to properly forward your <code>/stream</code> path to the correct port</p> <pre><code>location /stream/ {\n    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n    proxy_set_header Host $host;\n    proxy_pass http://127.0.0.1:1234/stream/;\n    proxy_http_version 1.1;\n    proxy_set_header Upgrade $http_upgrade;\n    proxy_set_header Connection \"upgrade\";\n}\n</code></pre> <p>Finally, clients using the Hyperion Stream Client will be able to connect.</p>"},{"location":"providers/setup/hyperion_configuration/#plugins-set-up","title":"Plugins Set Up","text":"<p>Plugins are optional. Follow the documentation on the required plugin page.</p> <p>Official Plugins:</p> <ul> <li>Hyperion Explorer</li> </ul> <p>Experimental Feature</p> <p>Running 3rd-party plugins could be dangerous, please make sure you review the published code before installing</p>"}]}