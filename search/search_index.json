{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hyperion History API \u00b6 Scalable Full History API Solution for Antelope (former EOSIO) based blockchains. Made with \u2665 by Rio Blocks / EOS Rio Official documentation \u00b6 How to use \u00b6 For Providers For Developers Official plugins: \u00b6 Hyperion Lightweight Explorer 1. Overview \u00b6 Hyperion is a full history solution for indexing, storing and retrieving Antelope blockchain's historical data. Antelope protocol is highly scalable reaching up to tens of thousands of transactions per second demanding high performance indexing and optimized storage and querying solutions. Hyperion is developed to tackle those challenges providing open source software to be operated by block producers, infrastructure providers and dApp developers. Focused on delivering faster search times, lower bandwidth overhead and easier usability for UI/UX developers, Hyperion implements an improved data structure. Actions are stored in a flattened format, transaction ids are added to all inline actions, allowing to group by transaction without storing a full transaction index. Besides that if the inline action data is identical to the parent, it is considered a notification and thus removed from the database. No full block or transaction data is stored, all information can be reconstructed from actions and deltas, only a block header index is stored. 2. Architecture \u00b6 The following components are required in order to have a fully functional Hyperion API deployment. For small use cases, it is absolutely fine to run all components on a single machine. For larger chains and production environments, we recommend setting them up into different servers under a high-speed local network. 2.1 Elasticsearch Cluster \u00b6 The ES cluster is responsible for storing all indexed data. Direct access to the Hyperion API and Indexer must be provided. We recommend nodes in the cluster to have at least 32 GB of RAM and 8 cpu cores. SSD/NVME drives are recommended for maximum indexing throughput, although HDDs can be used for cold storage nodes. For production environments, a multi-node cluster is highly recommended. 2.2 Hyperion Indexer \u00b6 The Indexer is a Node.js based app that process data from the state history plugin and allows it to be indexed. The PM2 process manager is used to launch and operate the indexer. The configuration flexibility is very extensive, so system recommendations will depend on the use case and data load. It will require access to at least one ES node, RabbitMQ and the state history node. 2.3 Hyperion API \u00b6 Parallelizable API server that provides the V2 and V1 (legacy history plugin) endpoints. It is launched by PM2 and can also operate in cluster mode. It requires direct access to at least one ES node for the queries and all other services for full healthcheck 2.4 RabbitMQ \u00b6 Used as messaging queue and data transport between the indexer stages and for real-time data streaming 2.5 Redis \u00b6 Used for transient data storage across processes and for the preemptive transaction caching used on the v2/history/get_transaction and v2/history/check_transaction endpoints 2.6 Leap State History \u00b6 Leap / Nodeos plugin used to collect action traces and state deltas. Provides data via websocket to the indexer 2.7 Hyperion Stream Client (optional) \u00b6 Web and Node.js client for real-time streaming on enabled hyperion providers. Documentation 2.8 Hyperion Plugins (optional) \u00b6 Hyperion includes a flexible plugin architecture to allow further customization. Plugins are managed by the hpm (hyperion plugin manager) command line tool.","title":"Home"},{"location":"#hyperion-history-api","text":"Scalable Full History API Solution for Antelope (former EOSIO) based blockchains. Made with \u2665 by Rio Blocks / EOS Rio","title":"Hyperion History API"},{"location":"#official-documentation","text":"","title":"Official documentation"},{"location":"#how-to-use","text":"For Providers For Developers","title":"How to use"},{"location":"#official-plugins","text":"Hyperion Lightweight Explorer","title":"Official plugins:"},{"location":"#1-overview","text":"Hyperion is a full history solution for indexing, storing and retrieving Antelope blockchain's historical data. Antelope protocol is highly scalable reaching up to tens of thousands of transactions per second demanding high performance indexing and optimized storage and querying solutions. Hyperion is developed to tackle those challenges providing open source software to be operated by block producers, infrastructure providers and dApp developers. Focused on delivering faster search times, lower bandwidth overhead and easier usability for UI/UX developers, Hyperion implements an improved data structure. Actions are stored in a flattened format, transaction ids are added to all inline actions, allowing to group by transaction without storing a full transaction index. Besides that if the inline action data is identical to the parent, it is considered a notification and thus removed from the database. No full block or transaction data is stored, all information can be reconstructed from actions and deltas, only a block header index is stored.","title":"1. Overview"},{"location":"#2-architecture","text":"The following components are required in order to have a fully functional Hyperion API deployment. For small use cases, it is absolutely fine to run all components on a single machine. For larger chains and production environments, we recommend setting them up into different servers under a high-speed local network.","title":"2. Architecture"},{"location":"#21-elasticsearch-cluster","text":"The ES cluster is responsible for storing all indexed data. Direct access to the Hyperion API and Indexer must be provided. We recommend nodes in the cluster to have at least 32 GB of RAM and 8 cpu cores. SSD/NVME drives are recommended for maximum indexing throughput, although HDDs can be used for cold storage nodes. For production environments, a multi-node cluster is highly recommended.","title":"2.1 Elasticsearch Cluster"},{"location":"#22-hyperion-indexer","text":"The Indexer is a Node.js based app that process data from the state history plugin and allows it to be indexed. The PM2 process manager is used to launch and operate the indexer. The configuration flexibility is very extensive, so system recommendations will depend on the use case and data load. It will require access to at least one ES node, RabbitMQ and the state history node.","title":"2.2 Hyperion Indexer"},{"location":"#23-hyperion-api","text":"Parallelizable API server that provides the V2 and V1 (legacy history plugin) endpoints. It is launched by PM2 and can also operate in cluster mode. It requires direct access to at least one ES node for the queries and all other services for full healthcheck","title":"2.3 Hyperion API"},{"location":"#24-rabbitmq","text":"Used as messaging queue and data transport between the indexer stages and for real-time data streaming","title":"2.4 RabbitMQ"},{"location":"#25-redis","text":"Used for transient data storage across processes and for the preemptive transaction caching used on the v2/history/get_transaction and v2/history/check_transaction endpoints","title":"2.5 Redis"},{"location":"#26-leap-state-history","text":"Leap / Nodeos plugin used to collect action traces and state deltas. Provides data via websocket to the indexer","title":"2.6 Leap State History"},{"location":"#27-hyperion-stream-client-optional","text":"Web and Node.js client for real-time streaming on enabled hyperion providers. Documentation","title":"2.7 Hyperion Stream Client (optional)"},{"location":"#28-hyperion-plugins-optional","text":"Hyperion includes a flexible plugin architecture to allow further customization. Plugins are managed by the hpm (hyperion plugin manager) command line tool.","title":"2.8 Hyperion Plugins (optional)"},{"location":"chain/","text":"Detailed Description of Chain Configuration \u00b6 This section is a quick guide of the config.json file. Here you are going to find a brief explanation of each parameter with its default value and an example of real usage. 1. API Configuration \u00b6 \"chain_name\": \"EXAMPLE Chain\" \"server_addr\": \"127.0.0.1\" \"server_port\": 7000 \"server_name\": \"127.0.0.1:7000\" \"provider_name\": \"Example Provider\" \"provider_url\": \"https://example.com\" \"chain_logo_url\": \"\" \"enable_caching\": true \u21d2 Set API cache \"cache_life\": 1 \u21d2 Define the cache life \"limits\" \u21d2 Set API response limits \"get_actions\": 1000 \"get_voters\": 100 \"get_links\": 1000 \"get_deltas\": 1000 \"access_log\": false \u21d2 Enable log API access. \"enable_explorer\": false 2. Settings \u00b6 \"preview\": false \u21d2 Preview mode - prints worker map and exit \"chain\": \"eos\" \u21d2 Chain name (The same used on ecosystem.config.js) \"eosio_alias\": \"eosio\" \"parser\": \"1.8\" \u21d2 Version of the parser to be used \"auto_stop\": 300 \u21d2 Automatically stop Indexer after X seconds if no more blocks are being processed (0=disable) \"index_version\": \"v1\" \u21d2 Set the index version \"debug\": false \u21d2 Enable debug mode \"rate_monitoring\": true \u21d2 Print data rates for each phase every 5s \"bp_logs\": false \u21d2 Enable logs \"bp_monitoring\": false \u21d2 Allow monitoring and logging of missed blocks and rounds \"ipc_debug_rate\": 60000 \u21d2 interval between IPC messaging rate debug messages, setting it to false or 0 will disable the logging \"allow_custom_abi\": false \u21d2 allow using custom ABIs from the custom-abi/ folder, they must match the pattern - - .json. Those ABIs will overwrite on-chain data for the given range. 3. Blacklists \u00b6 Blacklist for actions and deltas \"actions\": [] \"deltas\": [] 4. Whitelists \u00b6 Whitelist for actions and deltas actions\": [] \"deltas\": [] 5. Scaling options \u00b6 \"batch_size\": 10000 \u21d2 Parallel reader batch size in blocks \"queue_limit\": 50000 \u21d2 Queue size limit on rabbitmq \"readers\": 1 \u21d2 Number of readers \"ds_queues\": 1 \u21d2 Number of deserializer queues \"ds_threads\": 1 \u21d2 Number of deserializer threads \"ds_pool_size\": 1 \u21d2 Deserializer pool size \"indexing_queues\": 1 \u21d2 Number of indexing queues \"ad_idx_queues\": 1 \u21d2 Multiplier for action indexing queues \"max_autoscale\": 4 \u21d2 Max number of readers to autoscale \"auto_scale_trigger\": 20000 \u21d2 Number of itens on queue to trigger autoscale \"routing_mode\": \"heatmap\" \u21d2 We recommend using the heatmap routing algorithm since it uses less memory. If you see performance issues on large chains with very unbalanced action distribution (majority of the action on a single contract) you can change it to round_robin to improve throughput. Keep in mind that the round_robin method will use more RAM as it has larger ds_pool_size values 6. Indexer configuration \u00b6 \"start_on\": 0 \u21d2 Start indexing on block (0=disable) \"stop_on\": 0 \u21d2 Stop indexing on block (0=disable) \"rewrite\": false \u21d2 Force rewrite the target replay range \"purge_queues\": true \u21d2 Clear rabbitmq queues before starting the indexer \"live_reader\": false \u21d2 Enable live reader \"live_only_mode\": false \u21d2 Only reads realtime data serially \"abi_scan_mode\": true \u21d2 Enable abi scan mode ( Indicated on the first run ) \"fetch_block\": true \u21d2 Request full blocks from the state history plugin \"fetch_traces\": true \u21d2 Request traces from the state history plugin \"disable_reading\": false \u21d2 Completely disable block reading, for lagged queue processing \"disable_indexing\": false \u21d2 Disable indexing \"process_deltas\": true \u21d2 Read table deltas \"max_inline\": 20 \u21d2 Max inline actions depth to index 7. Features \u00b6 \"index_deltas\": true \u21d2 Index common table deltas (see delta on definitions/mappings) \"index_transfer_memo\": true \u21d2 Index transfers memo \"index_all_deltas\": true \u21d2 Index all table deltas 7.1 Streaming \u00b6 Enable live streaming \"enable\": false \"traces\": false \"deltas\": false 7.2 Tables \u00b6 Tables to fetch \"proposals\": true \"accounts\": true \"voters\": true \"userres\": false \"delband\": false 8. Prefetch \u00b6 \"read\": 50 \u21d2 Stage 1 prefetch size \"block\": 100 \u21d2 Stage 2 prefetch size \"index\": 500 \u21d2 Stage 3 prefetch size Example \u00b6 !!! tip For multiple chains, you should have one config file for each chain. Let's suppose that we gonna start Indexing the EOS Mainnet with: Locally exposed API 2 Readers 2 Deserializer Queues Live Streaming Enabled with Traces ABI scan already done The first step is to make a copy of the config file and rename it: chains/example.config.json to chains/eos.config.json . The next step is to edit the file as the following: { \"api\" : { \"chain_name\" : \"eos\" , \"server_addr\" : \"127.0.0.1\" , \"server_port\" : 7000 , \"server_name\" : \"127.0.0.1:7000\" , \"provider_name\" : \"Example Provider\" , \"provider_url\" : \"https://example.com\" , \"chain_logo_url\" : \"\" , \"enable_caching\" : true , \"cache_life\" : 1 , \"limits\" : { \"get_actions\" : 1000 , \"get_voters\" : 100 , \"get_links\" : 1000 , \"get_deltas\" : 1000 }, \"access_log\" : false , \"enable_explorer\" : false }, \"settings\" : { \"preview\" : false , \"chain\" : \"eos\" , \"eosio_alias\" : \"eosio\" , \"parser\" : \"1.8\" , \"auto_stop\" : 300 , \"index_version\" : \"v1\" , \"debug\" : false , \"rate_monitoring\" : true , \"bp_logs\" : false , \"bp_monitoring\" : false , \"ipc_debug_rate\" : 60000 , \"allow_custom_abi\" : false }, \"blacklists\" : { \"actions\" : [], \"deltas\" : [] }, \"whitelists\" : { \"actions\" : [], \"deltas\" : [] }, \"scaling\" : { \"batch_size\" : 10000 , \"queue_limit\" : 50000 , \"readers\" : 2 , \"ds_queues\" : 2 , \"ds_threads\" : 1 , \"ds_pool_size\" : 1 , \"indexing_queues\" : 1 , \"ad_idx_queues\" : 1 , \"max_autoscale\" : 4 , \"auto_scale_trigger\" : 20000 }, \"indexer\" : { \"start_on\" : 1 , \"stop_on\" : 0 , \"rewrite\" : false , \"purge_queues\" : true , \"live_reader\" : false , \"live_only_mode\" : false , \"abi_scan_mode\" : false , \"fetch_block\" : true , \"fetch_traces\" : true , \"disable_reading\" : false , \"disable_indexing\" : false , \"process_deltas\" : true , \"max_inline\" : 20 }, \"features\" : { \"streaming\" : { \"enable\" : true , \"traces\" : true , \"deltas\" : false }, \"tables\" : { \"proposals\" : true , \"accounts\" : true , \"voters\" : true , \"userres\" : false , \"delband\" : false }, \"index_deltas\" : true , \"index_transfer_memo\" : true , \"index_all_deltas\" : true }, \"prefetch\" : { \"read\" : 50 , \"block\" : 100 , \"index\" : 500 } }","title":"Chain Configuration"},{"location":"chain/#detailed-description-of-chain-configuration","text":"This section is a quick guide of the config.json file. Here you are going to find a brief explanation of each parameter with its default value and an example of real usage.","title":"Detailed Description of Chain Configuration"},{"location":"chain/#1-api-configuration","text":"\"chain_name\": \"EXAMPLE Chain\" \"server_addr\": \"127.0.0.1\" \"server_port\": 7000 \"server_name\": \"127.0.0.1:7000\" \"provider_name\": \"Example Provider\" \"provider_url\": \"https://example.com\" \"chain_logo_url\": \"\" \"enable_caching\": true \u21d2 Set API cache \"cache_life\": 1 \u21d2 Define the cache life \"limits\" \u21d2 Set API response limits \"get_actions\": 1000 \"get_voters\": 100 \"get_links\": 1000 \"get_deltas\": 1000 \"access_log\": false \u21d2 Enable log API access. \"enable_explorer\": false","title":"1. API Configuration"},{"location":"chain/#2-settings","text":"\"preview\": false \u21d2 Preview mode - prints worker map and exit \"chain\": \"eos\" \u21d2 Chain name (The same used on ecosystem.config.js) \"eosio_alias\": \"eosio\" \"parser\": \"1.8\" \u21d2 Version of the parser to be used \"auto_stop\": 300 \u21d2 Automatically stop Indexer after X seconds if no more blocks are being processed (0=disable) \"index_version\": \"v1\" \u21d2 Set the index version \"debug\": false \u21d2 Enable debug mode \"rate_monitoring\": true \u21d2 Print data rates for each phase every 5s \"bp_logs\": false \u21d2 Enable logs \"bp_monitoring\": false \u21d2 Allow monitoring and logging of missed blocks and rounds \"ipc_debug_rate\": 60000 \u21d2 interval between IPC messaging rate debug messages, setting it to false or 0 will disable the logging \"allow_custom_abi\": false \u21d2 allow using custom ABIs from the custom-abi/ folder, they must match the pattern - - .json. Those ABIs will overwrite on-chain data for the given range.","title":"2. Settings"},{"location":"chain/#3-blacklists","text":"Blacklist for actions and deltas \"actions\": [] \"deltas\": []","title":"3. Blacklists"},{"location":"chain/#4-whitelists","text":"Whitelist for actions and deltas actions\": [] \"deltas\": []","title":"4. Whitelists"},{"location":"chain/#5-scaling-options","text":"\"batch_size\": 10000 \u21d2 Parallel reader batch size in blocks \"queue_limit\": 50000 \u21d2 Queue size limit on rabbitmq \"readers\": 1 \u21d2 Number of readers \"ds_queues\": 1 \u21d2 Number of deserializer queues \"ds_threads\": 1 \u21d2 Number of deserializer threads \"ds_pool_size\": 1 \u21d2 Deserializer pool size \"indexing_queues\": 1 \u21d2 Number of indexing queues \"ad_idx_queues\": 1 \u21d2 Multiplier for action indexing queues \"max_autoscale\": 4 \u21d2 Max number of readers to autoscale \"auto_scale_trigger\": 20000 \u21d2 Number of itens on queue to trigger autoscale \"routing_mode\": \"heatmap\" \u21d2 We recommend using the heatmap routing algorithm since it uses less memory. If you see performance issues on large chains with very unbalanced action distribution (majority of the action on a single contract) you can change it to round_robin to improve throughput. Keep in mind that the round_robin method will use more RAM as it has larger ds_pool_size values","title":"5. Scaling options"},{"location":"chain/#6-indexer-configuration","text":"\"start_on\": 0 \u21d2 Start indexing on block (0=disable) \"stop_on\": 0 \u21d2 Stop indexing on block (0=disable) \"rewrite\": false \u21d2 Force rewrite the target replay range \"purge_queues\": true \u21d2 Clear rabbitmq queues before starting the indexer \"live_reader\": false \u21d2 Enable live reader \"live_only_mode\": false \u21d2 Only reads realtime data serially \"abi_scan_mode\": true \u21d2 Enable abi scan mode ( Indicated on the first run ) \"fetch_block\": true \u21d2 Request full blocks from the state history plugin \"fetch_traces\": true \u21d2 Request traces from the state history plugin \"disable_reading\": false \u21d2 Completely disable block reading, for lagged queue processing \"disable_indexing\": false \u21d2 Disable indexing \"process_deltas\": true \u21d2 Read table deltas \"max_inline\": 20 \u21d2 Max inline actions depth to index","title":"6. Indexer configuration"},{"location":"chain/#7-features","text":"\"index_deltas\": true \u21d2 Index common table deltas (see delta on definitions/mappings) \"index_transfer_memo\": true \u21d2 Index transfers memo \"index_all_deltas\": true \u21d2 Index all table deltas","title":"7. Features"},{"location":"chain/#71-streaming","text":"Enable live streaming \"enable\": false \"traces\": false \"deltas\": false","title":"7.1 Streaming"},{"location":"chain/#72-tables","text":"Tables to fetch \"proposals\": true \"accounts\": true \"voters\": true \"userres\": false \"delband\": false","title":"7.2 Tables"},{"location":"chain/#8-prefetch","text":"\"read\": 50 \u21d2 Stage 1 prefetch size \"block\": 100 \u21d2 Stage 2 prefetch size \"index\": 500 \u21d2 Stage 3 prefetch size","title":"8. Prefetch"},{"location":"chain/#example","text":"!!! tip For multiple chains, you should have one config file for each chain. Let's suppose that we gonna start Indexing the EOS Mainnet with: Locally exposed API 2 Readers 2 Deserializer Queues Live Streaming Enabled with Traces ABI scan already done The first step is to make a copy of the config file and rename it: chains/example.config.json to chains/eos.config.json . The next step is to edit the file as the following: { \"api\" : { \"chain_name\" : \"eos\" , \"server_addr\" : \"127.0.0.1\" , \"server_port\" : 7000 , \"server_name\" : \"127.0.0.1:7000\" , \"provider_name\" : \"Example Provider\" , \"provider_url\" : \"https://example.com\" , \"chain_logo_url\" : \"\" , \"enable_caching\" : true , \"cache_life\" : 1 , \"limits\" : { \"get_actions\" : 1000 , \"get_voters\" : 100 , \"get_links\" : 1000 , \"get_deltas\" : 1000 }, \"access_log\" : false , \"enable_explorer\" : false }, \"settings\" : { \"preview\" : false , \"chain\" : \"eos\" , \"eosio_alias\" : \"eosio\" , \"parser\" : \"1.8\" , \"auto_stop\" : 300 , \"index_version\" : \"v1\" , \"debug\" : false , \"rate_monitoring\" : true , \"bp_logs\" : false , \"bp_monitoring\" : false , \"ipc_debug_rate\" : 60000 , \"allow_custom_abi\" : false }, \"blacklists\" : { \"actions\" : [], \"deltas\" : [] }, \"whitelists\" : { \"actions\" : [], \"deltas\" : [] }, \"scaling\" : { \"batch_size\" : 10000 , \"queue_limit\" : 50000 , \"readers\" : 2 , \"ds_queues\" : 2 , \"ds_threads\" : 1 , \"ds_pool_size\" : 1 , \"indexing_queues\" : 1 , \"ad_idx_queues\" : 1 , \"max_autoscale\" : 4 , \"auto_scale_trigger\" : 20000 }, \"indexer\" : { \"start_on\" : 1 , \"stop_on\" : 0 , \"rewrite\" : false , \"purge_queues\" : true , \"live_reader\" : false , \"live_only_mode\" : false , \"abi_scan_mode\" : false , \"fetch_block\" : true , \"fetch_traces\" : true , \"disable_reading\" : false , \"disable_indexing\" : false , \"process_deltas\" : true , \"max_inline\" : 20 }, \"features\" : { \"streaming\" : { \"enable\" : true , \"traces\" : true , \"deltas\" : false }, \"tables\" : { \"proposals\" : true , \"accounts\" : true , \"voters\" : true , \"userres\" : false , \"delband\" : false }, \"index_deltas\" : true , \"index_transfer_memo\" : true , \"index_all_deltas\" : true }, \"prefetch\" : { \"read\" : 50 , \"block\" : 100 , \"index\" : 500 } }","title":"Example"},{"location":"connections/","text":"Detailed Description of Connections Configuration \u00b6 RabbitMQ parameters \u00b6 \"host\":\"127.0.0.1:5672\" \"api\":\"127.0.0.1:15672\" \"user\":\"my_user\" \"pass\":\"my_password\" \"vhost\":\"hyperion\" Elasticsearch parameters \u00b6 \"protocol\": \"http\" \u21d2 Protocol used to connect to Elasticsearch (default: http). \"host\":\"127.0.0.1:9200\" \"ingest_nodes\": [ \"127.0.0.1:9200\"] \"user\":\"\" \u21d2 User defined on elasticsearch configuration. \"pass\":\"\" \u21d2 Password defined on elasticsearch configuration. Redis parameters \u00b6 \"host\":\"127.0.0.1\" \"port\":\"6379\" Chain Parameters \u00b6 \"name\":\"EOS Mainnet\" \"chain_id\":\"aca376f206b8fc25a6ed44dbdc66547c36c6c33e3a119ffbeaef943642f0e906\" \"http\":\"http://127.0.0.1:8888\" \"ship\":\"ws://127.0.0.1:8080\" \"WS_ROUTER_HOST\": \"127.0.0.1\" \u21d2 Endpoint used by the Streaming API to connect to the Indexer. This is important when Indexer and API aren't on the same machine/instance. \"WS_ROUTER_PORT\":7001 \u21d2 Port used by the Streaming API to connect to the Indexer. Example \u00b6 In this example we have a connection.json file with: Local RabbitMQ user: admin pass: 123456 Local Elasticsearch no user no password Local Reddis Remote EOS Mainnet state history Remote WAX state history The first step is to make a copy of the config file and rename it: example-connections.json to connections.json . The next step is to edit the file as the follows: { \"amqp\" : { \"host\" : \"127.0.0.1:5672\" , \"api\" : \"127.0.0.1:15672\" , \"user\" : \"admin\" , \"pass\" : \"123456\" , \"vhost\" : \"hyperion\" }, \"elasticsearch\" : { \"host\" : \"127.0.0.1:9200\" , \"ingest_nodes\" : [ \"127.0.0.1:9200\" ], \"user\" : \"\" , \"pass\" : \"\" }, \"redis\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"6379\" }, \"chains\" : { \"eos\" : { \"name\" : \"EOS Mainnet\" , \"chain_id\" : \"aca376f206b8fc25a6ed44dbdc66547c36c6c33e3a119ffbeaef943642f0e906\" , \"http\" : \"http://127.0.0.1:8888\" , \"ship\" : \"ws://127.0.0.1:8080\" , \"WS_ROUTER_PORT\" : 7001 }, \"sample\" : { \"name\" : \"Sample Mainnet\" , \"chain_id\" : \"9473887b3cd1a897ce03ae5b6a865651747e2e152090f99c1d19d4adf73238fas\" , \"http\" : \"https://sample.io\" , \"ship\" : \"ws://192.168.0.1:8080\" , \"WS_ROUTER_HOST\" : \"127.0.0.1\" , \"WS_ROUTER_PORT\" : 8034 } } }","title":"Connections Configuration"},{"location":"connections/#detailed-description-of-connections-configuration","text":"","title":"Detailed Description of Connections Configuration"},{"location":"connections/#rabbitmq-parameters","text":"\"host\":\"127.0.0.1:5672\" \"api\":\"127.0.0.1:15672\" \"user\":\"my_user\" \"pass\":\"my_password\" \"vhost\":\"hyperion\"","title":"RabbitMQ parameters"},{"location":"connections/#elasticsearch-parameters","text":"\"protocol\": \"http\" \u21d2 Protocol used to connect to Elasticsearch (default: http). \"host\":\"127.0.0.1:9200\" \"ingest_nodes\": [ \"127.0.0.1:9200\"] \"user\":\"\" \u21d2 User defined on elasticsearch configuration. \"pass\":\"\" \u21d2 Password defined on elasticsearch configuration.","title":"Elasticsearch parameters"},{"location":"connections/#redis-parameters","text":"\"host\":\"127.0.0.1\" \"port\":\"6379\"","title":"Redis parameters"},{"location":"connections/#chain-parameters","text":"\"name\":\"EOS Mainnet\" \"chain_id\":\"aca376f206b8fc25a6ed44dbdc66547c36c6c33e3a119ffbeaef943642f0e906\" \"http\":\"http://127.0.0.1:8888\" \"ship\":\"ws://127.0.0.1:8080\" \"WS_ROUTER_HOST\": \"127.0.0.1\" \u21d2 Endpoint used by the Streaming API to connect to the Indexer. This is important when Indexer and API aren't on the same machine/instance. \"WS_ROUTER_PORT\":7001 \u21d2 Port used by the Streaming API to connect to the Indexer.","title":"Chain Parameters"},{"location":"connections/#example","text":"In this example we have a connection.json file with: Local RabbitMQ user: admin pass: 123456 Local Elasticsearch no user no password Local Reddis Remote EOS Mainnet state history Remote WAX state history The first step is to make a copy of the config file and rename it: example-connections.json to connections.json . The next step is to edit the file as the follows: { \"amqp\" : { \"host\" : \"127.0.0.1:5672\" , \"api\" : \"127.0.0.1:15672\" , \"user\" : \"admin\" , \"pass\" : \"123456\" , \"vhost\" : \"hyperion\" }, \"elasticsearch\" : { \"host\" : \"127.0.0.1:9200\" , \"ingest_nodes\" : [ \"127.0.0.1:9200\" ], \"user\" : \"\" , \"pass\" : \"\" }, \"redis\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"6379\" }, \"chains\" : { \"eos\" : { \"name\" : \"EOS Mainnet\" , \"chain_id\" : \"aca376f206b8fc25a6ed44dbdc66547c36c6c33e3a119ffbeaef943642f0e906\" , \"http\" : \"http://127.0.0.1:8888\" , \"ship\" : \"ws://127.0.0.1:8080\" , \"WS_ROUTER_PORT\" : 7001 }, \"sample\" : { \"name\" : \"Sample Mainnet\" , \"chain_id\" : \"9473887b3cd1a897ce03ae5b6a865651747e2e152090f99c1d19d4adf73238fas\" , \"http\" : \"https://sample.io\" , \"ship\" : \"ws://192.168.0.1:8080\" , \"WS_ROUTER_HOST\" : \"127.0.0.1\" , \"WS_ROUTER_PORT\" : 8034 } } }","title":"Example"},{"location":"docker/","text":"Hyperion Docker \u00b6 Attention This guide is not to date. Hyperion Docker is a multi-container Docker application intended to get Hyperion up and running as fast as possible. It will index data from a development chain where you can set your contracts, push some actions and see what happens when querying the Hyperion API. Warning Using Hyperion Docker is not recommended for production environments, only for testing, debugging and local networks. Recommend OS: Ubuntu 18.04 1. Dependencies \u00b6 docker and docker-compose Warning You should configure your system to run Docker as a non-root user. 2. RUN \u00b6 Inside the docker folder you will find everything you need to run Hyperion Docker. First of all, you need to generate the Hyperion configuration files. To do that, from the docker folder, run generate-config.sh located inside the scripts folder. You will have to pass an identifier and a name to the chain you will run. Example: ./scripts/generate-config.sh --chain eos --chain-name \"EOS Testnet\" Feel free to change the configuration files in hyperion/config folder the way it suits you. For more details, please refer to the Hyperion Setup Section . Now you have three options to run it: Option 1: docker-compose up \u00b6 This is the simplest way to run Hyperion. Just run docker-compose up -d and after some time all necessary docker containers will be running. You can start using it as you like. Warning We recommend using the Script to run Hyperion Docker as the order in which containers are started is important. To check logs run: docker-compose logs -f And to bring all containers down run: docker-compose down Option 2: Script \u00b6 We created a script to start every container in a specific order to avoid problems like connection errors. From the docker folder run the start.sh script located inside the scripts folder. Example: ./scripts/start.sh --chain eos With this script, you also have the option to start the chain from a snapshot. Example ./scripts/start.sh --chain eos --snapshot snapshot-file.bin Don't forget to move the snapshot file to the eosio/data/snapshots folder and make sure to change the chain_id on connections.json file. You can also use the stop.sh script to stop all or a specific service and also to bring all the containers down. Check the script usage for more information. Option 3: Manual \u00b6 If you have some experience with Docker Compose you can probably explore Hyperion Docker a bit more. We recommend starting services in the following order: redis, rabbitmq, elasticsearch, kibana, eosio-node, hyperion-indexer and hyperion-api . Wait until each of them is listening for connections before you start the next. Feel free to change the docker-compose.yml as you like. Bellow, you can find a simple example of how to control hyperion-indexer service: docker-compose up --no-start docker-compose start hyperion-indexer docker-compose stop hyperion-indexer docker-compose down It's also possible to start the chain form a snapshot passing a variable named SNAPSHOT to docker-compose up . 3. Usage \u00b6 After running Hyperion Docker, you should have a development chain producing blocks on a Docker container (eosio-node) as well as Hyperion Indexer and API on the other two Docker containers (hyperion-indexer and hyperion-api). If for some reason you decide to start it fresh again, make sure to clean all generated data. To do that just run clean-up.sh script inside scripts folder. EOSIO-NODE \u00b6 The port 8888 of this container is exposed so you can use it to interact with the chain. Example cleos -u http://127.0.0.1:8888 get info Hyperion API \u00b6 Perform queries on the endpoint at http://127.0.0.1:7000/ . The complete API reference can be found at API section: v2 Example curl http://127.0.0.1:7000/v2/history/get_actions Kibana \u00b6 Access http://127.0.0.1:5601/ RabbitMQ \u00b6 Access http://127.0.0.1:15672/ 4. Troubleshooting \u00b6 If you're having problems accessing Kibana or using Elasticsearch API, you could disable the xpack security on the docker-compose.yml setting it to false: xpack.security.enabled=false 5. Next steps \u00b6 Feel free to change configurations as you like. All configurations files are located in hyperion/config or eosio/config . For more details, please refer to the Hyperion Setup Section .","title":"Docker"},{"location":"docker/#hyperion-docker","text":"Attention This guide is not to date. Hyperion Docker is a multi-container Docker application intended to get Hyperion up and running as fast as possible. It will index data from a development chain where you can set your contracts, push some actions and see what happens when querying the Hyperion API. Warning Using Hyperion Docker is not recommended for production environments, only for testing, debugging and local networks. Recommend OS: Ubuntu 18.04","title":"Hyperion Docker"},{"location":"docker/#1-dependencies","text":"docker and docker-compose Warning You should configure your system to run Docker as a non-root user.","title":"1. Dependencies"},{"location":"docker/#2-run","text":"Inside the docker folder you will find everything you need to run Hyperion Docker. First of all, you need to generate the Hyperion configuration files. To do that, from the docker folder, run generate-config.sh located inside the scripts folder. You will have to pass an identifier and a name to the chain you will run. Example: ./scripts/generate-config.sh --chain eos --chain-name \"EOS Testnet\" Feel free to change the configuration files in hyperion/config folder the way it suits you. For more details, please refer to the Hyperion Setup Section . Now you have three options to run it:","title":"2. RUN"},{"location":"docker/#option-1-docker-compose-up","text":"This is the simplest way to run Hyperion. Just run docker-compose up -d and after some time all necessary docker containers will be running. You can start using it as you like. Warning We recommend using the Script to run Hyperion Docker as the order in which containers are started is important. To check logs run: docker-compose logs -f And to bring all containers down run: docker-compose down","title":"Option 1: docker-compose up"},{"location":"docker/#option-2-script","text":"We created a script to start every container in a specific order to avoid problems like connection errors. From the docker folder run the start.sh script located inside the scripts folder. Example: ./scripts/start.sh --chain eos With this script, you also have the option to start the chain from a snapshot. Example ./scripts/start.sh --chain eos --snapshot snapshot-file.bin Don't forget to move the snapshot file to the eosio/data/snapshots folder and make sure to change the chain_id on connections.json file. You can also use the stop.sh script to stop all or a specific service and also to bring all the containers down. Check the script usage for more information.","title":"Option 2: Script"},{"location":"docker/#option-3-manual","text":"If you have some experience with Docker Compose you can probably explore Hyperion Docker a bit more. We recommend starting services in the following order: redis, rabbitmq, elasticsearch, kibana, eosio-node, hyperion-indexer and hyperion-api . Wait until each of them is listening for connections before you start the next. Feel free to change the docker-compose.yml as you like. Bellow, you can find a simple example of how to control hyperion-indexer service: docker-compose up --no-start docker-compose start hyperion-indexer docker-compose stop hyperion-indexer docker-compose down It's also possible to start the chain form a snapshot passing a variable named SNAPSHOT to docker-compose up .","title":"Option 3: Manual"},{"location":"docker/#3-usage","text":"After running Hyperion Docker, you should have a development chain producing blocks on a Docker container (eosio-node) as well as Hyperion Indexer and API on the other two Docker containers (hyperion-indexer and hyperion-api). If for some reason you decide to start it fresh again, make sure to clean all generated data. To do that just run clean-up.sh script inside scripts folder.","title":"3. Usage"},{"location":"docker/#eosio-node","text":"The port 8888 of this container is exposed so you can use it to interact with the chain. Example cleos -u http://127.0.0.1:8888 get info","title":"EOSIO-NODE"},{"location":"docker/#hyperion-api","text":"Perform queries on the endpoint at http://127.0.0.1:7000/ . The complete API reference can be found at API section: v2 Example curl http://127.0.0.1:7000/v2/history/get_actions","title":"Hyperion API"},{"location":"docker/#kibana","text":"Access http://127.0.0.1:5601/","title":"Kibana"},{"location":"docker/#rabbitmq","text":"Access http://127.0.0.1:15672/","title":"RabbitMQ"},{"location":"docker/#4-troubleshooting","text":"If you're having problems accessing Kibana or using Elasticsearch API, you could disable the xpack security on the docker-compose.yml setting it to false: xpack.security.enabled=false","title":"4. Troubleshooting"},{"location":"docker/#5-next-steps","text":"Feel free to change configurations as you like. All configurations files are located in hyperion/config or eosio/config . For more details, please refer to the Hyperion Setup Section .","title":"5. Next steps"},{"location":"ecosystem/","text":"Detailed description of the ecosystem.config.js \u00b6 const { addApiServer , addIndexer } = require ( \"./definitions/ecosystem_settings\" ); module . exports = { apps : [ addIndexer ( 'eos' ), // Index chain name addApiServer ( 'eos' , 1 ) // API chain name, API threads number ] }; Multiple chains configuration \u00b6 To setup multiple Indexers and/or APIs, just add then inside the apps square brackets: const { addApiServer , addIndexer } = require ( \"./definitions/ecosystem_settings\" ); module . exports = { apps : [ addIndexer ( 'eos' ), addIndexer ( 'wax' ), addIndexer ( 'bos' ), addApiServer ( 'eos' , 1 ), addApiServer ( 'bos' , 1 ) ] };","title":"Ecosystem"},{"location":"ecosystem/#detailed-description-of-the-ecosystemconfigjs","text":"const { addApiServer , addIndexer } = require ( \"./definitions/ecosystem_settings\" ); module . exports = { apps : [ addIndexer ( 'eos' ), // Index chain name addApiServer ( 'eos' , 1 ) // API chain name, API threads number ] };","title":"Detailed description of the ecosystem.config.js"},{"location":"ecosystem/#multiple-chains-configuration","text":"To setup multiple Indexers and/or APIs, just add then inside the apps square brackets: const { addApiServer , addIndexer } = require ( \"./definitions/ecosystem_settings\" ); module . exports = { apps : [ addIndexer ( 'eos' ), addIndexer ( 'wax' ), addIndexer ( 'bos' ), addApiServer ( 'eos' , 1 ), addApiServer ( 'bos' , 1 ) ] };","title":"Multiple chains configuration"},{"location":"endpoint/","text":"Hyperion Open History Endpoint List \u00b6 Tip Check the endpoint status at https://bloks.io/hyperion 1. Mainnets: \u00b6 EOS \u00b6 url docs explorer https://api.eossweden.org/v2 docs BOS \u00b6 url docs explorer https://api.bossweden.org/v2 docs https://bos.eosusa.news/v2 docs WAX \u00b6 url docs explorer https://wax.eosrio.io/v2 docs explorer https://api.waxsweden.org/v2 docs https://wax.eosusa.news/v2 docs explorer https://wax.eosphere.io/v2 docs https://wax.pink.gg/v2 docs https://api-wax.maltablock.org/v2 docs https://wax.blokcrafters.io/v2 docs https://hyperion.wax.eosdetroit.io/v2 docs https://wax.cryptolions.io/v2 docs explorer TELOS \u00b6 url docs explorer https://mainnet.telos.net/v2 docs explorer https://telos.eosrio.io/v2 docs explorer https://mainnet.telosusa.io/v2 docs explorer https://telos.eosphere.io/v2 docs https://api-telos-21zephyr.maltablock.org/v2 docs https://telos.caleos.io/v2 docs explorer https://hyperion.telos.eosdetroit.io/v2 docs DAOBet \u00b6 url docs explorer https://daobet.eossweden.org/v2 docs https://daobet.eosusa.news/v2 docs INSTAR \u00b6 url docs explorer https://instar.eosusa.news/v2 docs explorer UOS \u00b6 url docs explorer https://uos.eosusa.news/v2 docs explorer Proton \u00b6 url docs explorer https://proton.eosusa.news/v2 docs explorer https://proton.cryptolions.io/v2 docs explorer WORBLI \u00b6 url docs explorer https://api.worblisweden.org/v2 docs MEETONE \u00b6 url docs explorer https://api.meetsweden.org/v2 docs Aikon/ORE \u00b6 url docs explorer https://ore.eosusa.news/v2 docs explorer Coffe \u00b6 url docs explorer https://hyperion.coffe.io/v2 docs https://coffe.eosusa.news/v2 docs explorer Remme \u00b6 url docs explorer https://rem.eon.llc/v2 docs Covax \u00b6 url docs explorer https://covax.eosdsp.com/v2 docs explorer FIO \u00b6 url docs explorer https://fio.cryptolions.io/v2 docs explorer 2. Testnets: \u00b6 Kylin EOS Testnet \u00b6 url docs explorer https://kylin.eossweden.org/v2 docs https://kylin.eosusa.news/v2 docs Jungle 2 EOS Testnet \u00b6 url docs explorer https://jungle2.cryptolions.io/v2 docs explorer https://jungle.eossweden.org/v2 docs Jungle 3 EOS Testnet \u00b6 url docs explorer https://jungle3.eosrio.io/v2 docs explorer https://jungle3.eosusa.news/v2 docs explorer https://jungle3history.cryptolions.io/v2 docs explore https://jungle3.eossweden.org/v2 docs BOS Testnet \u00b6 url docs explorer https://tst.bossweden.org/v2 docs Telos Testnet \u00b6 url docs explorer https://testnet.telos.net/v2 docs explorer https://testnet.telosusa.io/v2 docs explorer https://testnet.telos.caleos.io/v2 docs explorer WAX Testnet \u00b6 url docs explorer https://testnet.wax.pink.gg/v2 docs https://wax-test.blokcrafters.io/v2 docs DAOBet Testnet \u00b6 url docs explorer https://daobet-test.eossweden.org/v2 docs https://test.daobet.eosusa.news/v2 docs Proton Testnet \u00b6 url docs explorer https://test.proton.eosusa.news/v2 docs explorer https://testnet.protonchain.com/v2 docs explorer","title":"Endpoint List"},{"location":"endpoint/#hyperion-open-history-endpoint-list","text":"Tip Check the endpoint status at https://bloks.io/hyperion","title":"Hyperion Open History Endpoint List"},{"location":"endpoint/#1-mainnets","text":"","title":"1. Mainnets:"},{"location":"endpoint/#eos","text":"url docs explorer https://api.eossweden.org/v2 docs","title":"EOS"},{"location":"endpoint/#bos","text":"url docs explorer https://api.bossweden.org/v2 docs https://bos.eosusa.news/v2 docs","title":"BOS"},{"location":"endpoint/#wax","text":"url docs explorer https://wax.eosrio.io/v2 docs explorer https://api.waxsweden.org/v2 docs https://wax.eosusa.news/v2 docs explorer https://wax.eosphere.io/v2 docs https://wax.pink.gg/v2 docs https://api-wax.maltablock.org/v2 docs https://wax.blokcrafters.io/v2 docs https://hyperion.wax.eosdetroit.io/v2 docs https://wax.cryptolions.io/v2 docs explorer","title":"WAX"},{"location":"endpoint/#telos","text":"url docs explorer https://mainnet.telos.net/v2 docs explorer https://telos.eosrio.io/v2 docs explorer https://mainnet.telosusa.io/v2 docs explorer https://telos.eosphere.io/v2 docs https://api-telos-21zephyr.maltablock.org/v2 docs https://telos.caleos.io/v2 docs explorer https://hyperion.telos.eosdetroit.io/v2 docs","title":"TELOS"},{"location":"endpoint/#daobet","text":"url docs explorer https://daobet.eossweden.org/v2 docs https://daobet.eosusa.news/v2 docs","title":"DAOBet"},{"location":"endpoint/#instar","text":"url docs explorer https://instar.eosusa.news/v2 docs explorer","title":"INSTAR"},{"location":"endpoint/#uos","text":"url docs explorer https://uos.eosusa.news/v2 docs explorer","title":"UOS"},{"location":"endpoint/#proton","text":"url docs explorer https://proton.eosusa.news/v2 docs explorer https://proton.cryptolions.io/v2 docs explorer","title":"Proton"},{"location":"endpoint/#worbli","text":"url docs explorer https://api.worblisweden.org/v2 docs","title":"WORBLI"},{"location":"endpoint/#meetone","text":"url docs explorer https://api.meetsweden.org/v2 docs","title":"MEETONE"},{"location":"endpoint/#aikonore","text":"url docs explorer https://ore.eosusa.news/v2 docs explorer","title":"Aikon/ORE"},{"location":"endpoint/#coffe","text":"url docs explorer https://hyperion.coffe.io/v2 docs https://coffe.eosusa.news/v2 docs explorer","title":"Coffe"},{"location":"endpoint/#remme","text":"url docs explorer https://rem.eon.llc/v2 docs","title":"Remme"},{"location":"endpoint/#covax","text":"url docs explorer https://covax.eosdsp.com/v2 docs explorer","title":"Covax"},{"location":"endpoint/#fio","text":"url docs explorer https://fio.cryptolions.io/v2 docs explorer","title":"FIO"},{"location":"endpoint/#2-testnets","text":"","title":"2. Testnets:"},{"location":"endpoint/#kylin-eos-testnet","text":"url docs explorer https://kylin.eossweden.org/v2 docs https://kylin.eosusa.news/v2 docs","title":"Kylin EOS Testnet"},{"location":"endpoint/#jungle-2-eos-testnet","text":"url docs explorer https://jungle2.cryptolions.io/v2 docs explorer https://jungle.eossweden.org/v2 docs","title":"Jungle 2 EOS Testnet"},{"location":"endpoint/#jungle-3-eos-testnet","text":"url docs explorer https://jungle3.eosrio.io/v2 docs explorer https://jungle3.eosusa.news/v2 docs explorer https://jungle3history.cryptolions.io/v2 docs explore https://jungle3.eossweden.org/v2 docs","title":"Jungle 3 EOS Testnet"},{"location":"endpoint/#bos-testnet","text":"url docs explorer https://tst.bossweden.org/v2 docs","title":"BOS Testnet"},{"location":"endpoint/#telos-testnet","text":"url docs explorer https://testnet.telos.net/v2 docs explorer https://testnet.telosusa.io/v2 docs explorer https://testnet.telos.caleos.io/v2 docs explorer","title":"Telos Testnet"},{"location":"endpoint/#wax-testnet","text":"url docs explorer https://testnet.wax.pink.gg/v2 docs https://wax-test.blokcrafters.io/v2 docs","title":"WAX Testnet"},{"location":"endpoint/#daobet-testnet","text":"url docs explorer https://daobet-test.eossweden.org/v2 docs https://test.daobet.eosusa.news/v2 docs","title":"DAOBet Testnet"},{"location":"endpoint/#proton-testnet","text":"url docs explorer https://test.proton.eosusa.news/v2 docs explorer https://testnet.protonchain.com/v2 docs explorer","title":"Proton Testnet"},{"location":"faq/","text":"","title":"Faq"},{"location":"howtouse/","text":"How to use \u00b6 1. JavaScript client using https \u00b6 Fetching data from Hyperion using JavaScript is quite simple. To do that we're going to use https library to make the requests. So, the first step is to include the https lib: const https = require ( 'https' ); In this example, we will fetch the WAX chain for the transaction id: 14b36232e919307090c7e9a7a2b17915f40bf0ce72734647c9f8c145c110dda0. This is the query: \"https://wax.hyperion.eosrio.io/v2/history/get_transaction?id=14b36232e919307090c7e9a7a2b17915f40bf0ce72734647c9f8c145c110dda0\" Now, let's create a Promise function that will receive the tx id as parameter and save it into a variable called getTransaction : let getTransaction = function ( args ) { let url = \"https://wax.hyperion.eosrio.io/v2/history/get_transaction?id=\" + args ; return new Promise ( function ( resolve ) { https . get ( url , ( resp ) => { let data = '' ; // A chunk of data has been recieved. resp . on ( 'data' , ( chunk ) => { data += chunk ; }); // The whole response has been received. Print out the result. resp . on ( 'end' , () => { resolve ( JSON . parse ( data )); }); }); }) }; And finally, let's call the function passing the tx id as parameter: ( async () =>{ let id = \"14b36232e919307090c7e9a7a2b17915f40bf0ce72734647c9f8c145c110dda0\" getTransaction ( id ). then ( data => { console . log ( data ); }); })(); Request response: { tr x_id : ' 14 b 36232e919307090 c 7e9 a 7 a 2 b 17915 f 40 b f 0 ce 72734647 c 9 f 8 c 145 c 110 dda 0 ' , lib : 49925737 , ac t io ns : [ { ac t io n _ordi nal : 1 , crea t or_ac t io n _ordi nal : 0 , ac t : [ Objec t ], co nte x t _ free : false , elapsed : ' 0 ' , accou nt _ram_del tas : [ Array ], '@ t imes ta mp' : ' 2020-04-08 T 23 : 02 : 02.500 ' , block_ nu m : 49924003 , producer : 'ze n blockswax' , tr x_id : ' 14 b 36232e919307090 c 7e9 a 7 a 2 b 17915 f 40 b f 0 ce 72734647 c 9 f 8 c 145 c 110 dda 0 ' , global_seque n ce : 260804226 , cpu_usage_us : 1173 , net _usage_words : 36 , i nl i ne _cou nt : 3 , i nl i ne _ f il tere d : false , receip ts : [ Array ], code_seque n ce : 7 , abi_seque n ce : 7 , n o t i f ied : [ Array ], t imes ta mp : ' 2020-04-08 T 23 : 02 : 02.500 ' }, { ac t io n _ordi nal : 2 , crea t or_ac t io n _ordi nal : 1 , ac t : [ Objec t ], co nte x t _ free : false , elapsed : ' 0 ' , accou nt _ram_del tas : [ Array ], '@ t imes ta mp' : ' 2020-04-08 T 23 : 02 : 02.500 ' , block_ nu m : 49924003 , producer : 'ze n blockswax' , tr x_id : ' 14 b 36232e919307090 c 7e9 a 7 a 2 b 17915 f 40 b f 0 ce 72734647 c 9 f 8 c 145 c 110 dda 0 ' , global_seque n ce : 260804227 , receip ts : [ Array ], code_seque n ce : 6 , abi_seque n ce : 6 , n o t i f ied : [ Array ], t imes ta mp : ' 2020-04-08 T 23 : 02 : 02.500 ' } ], query_ t ime_ms : 45.26 } 2. Third party library \u00b6 There is a third party Javascript library made by EOS Cafe. Refer to their github for further information: https://github.com/eoscafe/hyperion-api Note This is a third party library and is not maintained by EOS Rio","title":"Getting Started"},{"location":"howtouse/#how-to-use","text":"","title":"How to use"},{"location":"howtouse/#1-javascript-client-using-https","text":"Fetching data from Hyperion using JavaScript is quite simple. To do that we're going to use https library to make the requests. So, the first step is to include the https lib: const https = require ( 'https' ); In this example, we will fetch the WAX chain for the transaction id: 14b36232e919307090c7e9a7a2b17915f40bf0ce72734647c9f8c145c110dda0. This is the query: \"https://wax.hyperion.eosrio.io/v2/history/get_transaction?id=14b36232e919307090c7e9a7a2b17915f40bf0ce72734647c9f8c145c110dda0\" Now, let's create a Promise function that will receive the tx id as parameter and save it into a variable called getTransaction : let getTransaction = function ( args ) { let url = \"https://wax.hyperion.eosrio.io/v2/history/get_transaction?id=\" + args ; return new Promise ( function ( resolve ) { https . get ( url , ( resp ) => { let data = '' ; // A chunk of data has been recieved. resp . on ( 'data' , ( chunk ) => { data += chunk ; }); // The whole response has been received. Print out the result. resp . on ( 'end' , () => { resolve ( JSON . parse ( data )); }); }); }) }; And finally, let's call the function passing the tx id as parameter: ( async () =>{ let id = \"14b36232e919307090c7e9a7a2b17915f40bf0ce72734647c9f8c145c110dda0\" getTransaction ( id ). then ( data => { console . log ( data ); }); })(); Request response: { tr x_id : ' 14 b 36232e919307090 c 7e9 a 7 a 2 b 17915 f 40 b f 0 ce 72734647 c 9 f 8 c 145 c 110 dda 0 ' , lib : 49925737 , ac t io ns : [ { ac t io n _ordi nal : 1 , crea t or_ac t io n _ordi nal : 0 , ac t : [ Objec t ], co nte x t _ free : false , elapsed : ' 0 ' , accou nt _ram_del tas : [ Array ], '@ t imes ta mp' : ' 2020-04-08 T 23 : 02 : 02.500 ' , block_ nu m : 49924003 , producer : 'ze n blockswax' , tr x_id : ' 14 b 36232e919307090 c 7e9 a 7 a 2 b 17915 f 40 b f 0 ce 72734647 c 9 f 8 c 145 c 110 dda 0 ' , global_seque n ce : 260804226 , cpu_usage_us : 1173 , net _usage_words : 36 , i nl i ne _cou nt : 3 , i nl i ne _ f il tere d : false , receip ts : [ Array ], code_seque n ce : 7 , abi_seque n ce : 7 , n o t i f ied : [ Array ], t imes ta mp : ' 2020-04-08 T 23 : 02 : 02.500 ' }, { ac t io n _ordi nal : 2 , crea t or_ac t io n _ordi nal : 1 , ac t : [ Objec t ], co nte x t _ free : false , elapsed : ' 0 ' , accou nt _ram_del tas : [ Array ], '@ t imes ta mp' : ' 2020-04-08 T 23 : 02 : 02.500 ' , block_ nu m : 49924003 , producer : 'ze n blockswax' , tr x_id : ' 14 b 36232e919307090 c 7e9 a 7 a 2 b 17915 f 40 b f 0 ce 72734647 c 9 f 8 c 145 c 110 dda 0 ' , global_seque n ce : 260804227 , receip ts : [ Array ], code_seque n ce : 6 , abi_seque n ce : 6 , n o t i f ied : [ Array ], t imes ta mp : ' 2020-04-08 T 23 : 02 : 02.500 ' } ], query_ t ime_ms : 45.26 }","title":"1. JavaScript client using https"},{"location":"howtouse/#2-third-party-library","text":"There is a third party Javascript library made by EOS Cafe. Refer to their github for further information: https://github.com/eoscafe/hyperion-api Note This is a third party library and is not maintained by EOS Rio","title":"2. Third party library"},{"location":"hyperion_configuration/","text":"Hyperion Set Up \u00b6 Set Up \u00b6 We've developed a tool to automate the configuration of Hyperion. It basically initializes the connections with all the dependencies and creates the configuration for each chain you are running. Run ./hyp-config --help for more details. Initialize connections \u00b6 First, let's initialize our configuration. Just run: ./hyp-config init connections Note This command will also check the connection to Elasticsearch, Rabbitmq and Redis. Make sure everything is up and running. Add new chain \u00b6 Now you can proceed and add a new chain to your configuration. Run the following command: ./hyp-config new chain eos --http \"http://127.0.0.1:8888\" --ship \"ws://127.0.0.1:8080\" Check you configuration \u00b6 Finally, check your configuration running: /hyp-config list chains Plugins Set Up \u00b6 Attention Under construction Running Hyperion \u00b6 We provide scripts to simplify the process of starting and stopping your Hyperion Indexer or API instance. Starting \u00b6 To start, just use the run.sh script. Here are some examples: Examples Starting indexer for EOS mainnet: ./run.sh eos-indexer Starting API for test chain: ./run.sh test-api Note You need to pass the name of the chain you previously created followed by indexer or api to indicate the instance you want to start. Stopping \u00b6 Use the stop.sh script to stop an instance as follows: Examples Stop API for EOS mainnet: ./stop.sh eos-api Stop indexer for WAX mainnet: ./stop.sh wax-indexer Note You need to pass the name of the chain you previously created followed by indexer or api to indicate the instance you want to stop. Attention The stop script won't stop Hyperion Indexer immediately, it will first flush the queues. Be aware that this operation could take some time. Indexer \u00b6 The Hyperion Indexer is configured to perform an abi scan (\"abi_scan_mode\": true) as default. So, on your first run, you'll probably see something like this: This an example of an ABI SCAN on the WAX chain. Where: W (Workers): Number of workers. R (Read): Blocks read from state history and pushing into the blocks queue. C (Consumed): Blocks consumed from blocks queue. A (Actions): Actions being read out of processed blocks. D (Deserialized): Deserializations of the actions. I (Indexed): Indexing of all of the docs. API \u00b6 After running the api, you should see a log like this: Now, it's time to play around making some queries. :fontawesome-regular-laugh-beam:","title":"Configuration"},{"location":"hyperion_configuration/#hyperion-set-up","text":"","title":"Hyperion Set Up"},{"location":"hyperion_configuration/#set-up","text":"We've developed a tool to automate the configuration of Hyperion. It basically initializes the connections with all the dependencies and creates the configuration for each chain you are running. Run ./hyp-config --help for more details.","title":"Set Up"},{"location":"hyperion_configuration/#initialize-connections","text":"First, let's initialize our configuration. Just run: ./hyp-config init connections Note This command will also check the connection to Elasticsearch, Rabbitmq and Redis. Make sure everything is up and running.","title":"Initialize connections"},{"location":"hyperion_configuration/#add-new-chain","text":"Now you can proceed and add a new chain to your configuration. Run the following command: ./hyp-config new chain eos --http \"http://127.0.0.1:8888\" --ship \"ws://127.0.0.1:8080\"","title":"Add new chain"},{"location":"hyperion_configuration/#check-you-configuration","text":"Finally, check your configuration running: /hyp-config list chains","title":"Check you configuration"},{"location":"hyperion_configuration/#plugins-set-up","text":"Attention Under construction","title":"Plugins Set Up"},{"location":"hyperion_configuration/#running-hyperion","text":"We provide scripts to simplify the process of starting and stopping your Hyperion Indexer or API instance.","title":"Running Hyperion"},{"location":"hyperion_configuration/#starting","text":"To start, just use the run.sh script. Here are some examples: Examples Starting indexer for EOS mainnet: ./run.sh eos-indexer Starting API for test chain: ./run.sh test-api Note You need to pass the name of the chain you previously created followed by indexer or api to indicate the instance you want to start.","title":"Starting"},{"location":"hyperion_configuration/#stopping","text":"Use the stop.sh script to stop an instance as follows: Examples Stop API for EOS mainnet: ./stop.sh eos-api Stop indexer for WAX mainnet: ./stop.sh wax-indexer Note You need to pass the name of the chain you previously created followed by indexer or api to indicate the instance you want to stop. Attention The stop script won't stop Hyperion Indexer immediately, it will first flush the queues. Be aware that this operation could take some time.","title":"Stopping"},{"location":"hyperion_configuration/#indexer","text":"The Hyperion Indexer is configured to perform an abi scan (\"abi_scan_mode\": true) as default. So, on your first run, you'll probably see something like this: This an example of an ABI SCAN on the WAX chain. Where: W (Workers): Number of workers. R (Read): Blocks read from state history and pushing into the blocks queue. C (Consumed): Blocks consumed from blocks queue. A (Actions): Actions being read out of processed blocks. D (Deserialized): Deserializations of the actions. I (Indexed): Indexing of all of the docs.","title":"Indexer"},{"location":"hyperion_configuration/#api","text":"After running the api, you should see a log like this: Now, it's time to play around making some queries. :fontawesome-regular-laugh-beam:","title":"API"},{"location":"installation_script/","text":"Installation Script \u00b6 Attention The installation script and this guide are not to date. Tip The usage of this script is highly recommended for fresh installations. If you already have dependencies installed, update them manually before running the script. Note For Windows installation using Multipass, refer to this guide We provide an automated shell script that installs all dependencies and then configure Hyperion. The first step is to clone the Hyperion repository: git clone https://github.com/eosrio/hyperion-history-api.git cd hyperion-history-api Then, you just need to run the script: ./install_env.sh The script will ask you for some information: Enter rabbitmq user [hyperion]: Enter the desired rabbitmq user and hit enter. If you leave it blank, the default user hyperion will be set. Then, do the same for the rabbitmq password: Enter rabbitmq password [123456]: And finally, it will ask if you want to create the npm global folder: Do you want to create a directory for npm global installations [Y/n] : This is recommended. If you choose n , global npm packages will require root permissions. Now, the script will do the work, this can take a while. Get a cup of coffee and relax. :fontawesome-regular-laugh-wink: Info The installation script may ask you for the admin password. This is needed to install the dependencies, please, provide it. If everything runs smoothly, the script will automatically generate elasticsearch passwords and save them on the elastic_pass.txt file. To login into Kibana, use the elastic user credentials. Now it's time to install hyperion ! :fontawesome-solid-glass-cheers:","title":"Installation Script"},{"location":"installation_script/#installation-script","text":"Attention The installation script and this guide are not to date. Tip The usage of this script is highly recommended for fresh installations. If you already have dependencies installed, update them manually before running the script. Note For Windows installation using Multipass, refer to this guide We provide an automated shell script that installs all dependencies and then configure Hyperion. The first step is to clone the Hyperion repository: git clone https://github.com/eosrio/hyperion-history-api.git cd hyperion-history-api Then, you just need to run the script: ./install_env.sh The script will ask you for some information: Enter rabbitmq user [hyperion]: Enter the desired rabbitmq user and hit enter. If you leave it blank, the default user hyperion will be set. Then, do the same for the rabbitmq password: Enter rabbitmq password [123456]: And finally, it will ask if you want to create the npm global folder: Do you want to create a directory for npm global installations [Y/n] : This is recommended. If you choose n , global npm packages will require root permissions. Now, the script will do the work, this can take a while. Get a cup of coffee and relax. :fontawesome-regular-laugh-wink: Info The installation script may ask you for the admin password. This is needed to install the dependencies, please, provide it. If everything runs smoothly, the script will automatically generate elasticsearch passwords and save them on the elastic_pass.txt file. To login into Kibana, use the elastic user credentials. Now it's time to install hyperion ! :fontawesome-solid-glass-cheers:","title":"Installation Script"},{"location":"kibana/","text":"The purpose here is to guide you through some basic steps using and configuring the Kibana. For more detailed information, please, refer to the official documentation . Running Kibana with systemd \u00b6 To configure Kibana to start automatically when the system boots up, run the following commands: sudo /bin/systemctl daemon-reload sudo /bin/systemctl enable kibana.service Kibana can be started and stopped as follows: sudo systemctl start kibana.service sudo systemctl stop kibana.service These commands provide no feedback as to whether Kibana was started successfully or not. Log information can be accessed via journalctl -u kibana.service. Opening Kibana \u00b6 Open http://localhost:5601 and check if you can access Kibana. If Kibana asks for credentials, the default user and password is: user: elastic password: changeme If you installed Kibana through the install_infra.sh script, check your elastic_pass.txt file. If you can't access, check your credentials on your config file. Creating index pattern \u00b6 To create a new index pattern, go to Management Click on Index Patterns at the left menu Click on Create index pattern Enter desired index pattern and click on > Next step Tip Index Pattern List: eos-abi-* eos-action-* eos-block-* eos-logs-* eos-delta-* Where eos is the name of the chain. Select a time filter, if there are any, and click on Create index pattern","title":"Kibana"},{"location":"kibana/#running-kibana-with-systemd","text":"To configure Kibana to start automatically when the system boots up, run the following commands: sudo /bin/systemctl daemon-reload sudo /bin/systemctl enable kibana.service Kibana can be started and stopped as follows: sudo systemctl start kibana.service sudo systemctl stop kibana.service These commands provide no feedback as to whether Kibana was started successfully or not. Log information can be accessed via journalctl -u kibana.service.","title":"Running Kibana with systemd"},{"location":"kibana/#opening-kibana","text":"Open http://localhost:5601 and check if you can access Kibana. If Kibana asks for credentials, the default user and password is: user: elastic password: changeme If you installed Kibana through the install_infra.sh script, check your elastic_pass.txt file. If you can't access, check your credentials on your config file.","title":"Opening Kibana"},{"location":"kibana/#creating-index-pattern","text":"To create a new index pattern, go to Management Click on Index Patterns at the left menu Click on Create index pattern Enter desired index pattern and click on > Next step Tip Index Pattern List: eos-abi-* eos-action-* eos-block-* eos-logs-* eos-delta-* Where eos is the name of the chain. Select a time filter, if there are any, and click on Create index pattern","title":"Creating index pattern"},{"location":"manual_installation/","text":"Manual Installation \u00b6 This section describes how to manually install Hyperion and its environment. If you want more control of your installation, this is the way to go. Attention Recommended OS: Ubuntu 20.04 Dependencies \u00b6 Below you can find the list of all Hyperion's dependencies: Elasticsearch 7.16.X Kibana 7.16.X RabbitMQ Redis Node.js v16 PM2 EOSIO 2.1 On the next steps you will install and configure each one of them. Note The Hyperion Indexer requires Node.js and pm2 to be on the same machine. All other dependencies (Elasticsearch, RabbitMQ, Redis and EOSIO) can be installed on different machines, preferably on a high speed and low latency network. Keep in mind that indexing speed will vary greatly depending on this configuration. Elasticsearch \u00b6 Follow the detailed installation instructions on the official Elasticsearch documentation and return to this guide before running it. Info Elasticsearch is not started automatically after installation. We recommend running it with systemd . Note It is very important to know the Elasticsearch directory layout and to understand how the configuration works. Configuration \u00b6 1. Elasticsearch configuration \u00b6 Edit the following lines on /etc/elasticsearch/elasticsearch.yml : cluster.name: CLUSTER_NAME bootstrap.memory_lock: true The memory lock option will prevent any Elasticsearch heap memory from being swapped out. Warning Setting bootstrap.memory_lock: true will make Elasticsearch try to use all the RAM configured for JVM on startup ( check next step). This can cause the application to crash if you allocate more RAM than available. Note A different approach is to disable swapping on your system. Testing After starting Elasticsearch, you can see whether this setting was applied successfully by checking the value of mlockall in the output from this request: curl -X GET \"localhost:9200/_nodes?filter_path=**.mlockall&pretty\" 2. Heap size configuration \u00b6 For a optimized heap size, check how much RAM can be allocated by the JVM on your system. Run the following command: java -Xms16g -Xmx16g -XX:+UseCompressedOops -XX:+PrintFlagsFinal Oops | grep Oops Check if UseCompressedOops is true on the results and change -Xms and -Xmx to the desired value. Note Elasticsearch includes a bundled version of OpenJDK from the JDK maintainers. You can find it on /usr/share/elasticsearch/jdk . After that, change the heap size by editting the following lines on /etc/elasticsearch/jvm.options : -Xms16g -Xmx16g Note Xms and Xmx must have the same value. Warning Avoid allocating more than 31GB when setting your heap size, even if you have enough RAM. 3. Allow memory lock \u00b6 Override systemd configuration by running sudo systemctl edit elasticsearch and add the following lines: [Service] LimitMEMLOCK=infinity Run the following command to reload units: sudo systemctl daemon-reload 4. Start Elasticsearch \u00b6 Start Elasticsearch and check the logs: sudo systemctl start elasticsearch.service sudo less /var/log/elasticsearch/CLUSTE_NAME.log Enable it to run at startup: sudo systemctl enable elasticsearch.service And finally, test the REST API: curl -X GET \"localhost:9200/?pretty\" Note Don't forget to check if memory lock worked. The expected result should be something like this: { \"name\" : \"ip-172-31-5-121\" , \"cluster_name\" : \"CLUSTER_NAME\" , \"cluster_uuid\" : \"FFl8DNcOQV-dVk3p1JDNMA\" , \"version\" : { \"number\" : \"7.14.1\" , \"build_flavor\" : \"default\" , \"build_type\" : \"deb\" , \"build_hash\" : \"606a173\" , \"build_date\" : \"2021-08-26T00:43:15.323135Z\" , \"build_snapshot\" : false , \"lucene_version\" : \"8.9.0\" , \"minimum_wire_compatibility_version\" : \"6.8.0\" , \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" }, \"tagline\" : \"You Know, for Search\" } 5. Set up minimal security \u00b6 The Elasticsearch security features are disabled by default. To avoid security problems, we recommend enabling the security pack. To do that, add the following line to the end of the /etc/elasticsearch/elasticsearch.yml file: xpack.security.enabled: true Restart Elasticsearch and set the passwords for the cluster: sudo systemctl restart elasticsearch.service sudo /usr/share/elasticsearch/bin/elasticsearch-setup-passwords auto Keep track of these passwords, we\u2019ll need them again soon. Note You can alternatively use the interactive parameter to manually define your passwords. Attention The minimal security scenario is not sufficient for production mode clusters. Check the documentation for more information. Kibana \u00b6 Follow the detailed installation instructions on the official Kibana documentation . Return to this documentation before running it. Info Kibana is not started automatically after installation. We recomend running it with systemd . Note Like on Elasticsearch, it is very important to know the Kibana directory layout and to understand how the configuration works. Configuration \u00b6 1. Elasticsearch security \u00b6 If you have enabled the security pack on Elasticsearch, you need to set up the password on Kibana. Edit the folowing lines on the /etc/kibana/kibana.yml file: elasticsearch.username: \"kibana_system\" elasticsearch.password: \"password\" 2. Start Kibana \u00b6 Start Kibana and check the logs: sudo systemctl start kibana.service sudo less /var/log/kibana/kibana.log Enable it to run at startup: sudo systemctl enable kibana.service RabbitMQ \u00b6 Follow the detailed installation instructions on the official RabbitMQ documentation . RabbitMQ should automatically start after installation. Check the documentation for more details on how to manage its service. Configuration \u00b6 1. Enable the WebUI \u00b6 sudo rabbitmq-plugins enable rabbitmq_management 2. Add vhost \u00b6 sudo rabbitmqctl add_vhost hyperion 3. Create a user and password \u00b6 sudo rabbitmqctl add_user USER PASSWORD 4. Set the user as administrator \u00b6 sudo rabbitmqctl set_user_tags USER administrator 5. Set the user permissions to the vhost \u00b6 sudo rabbitmqctl set_permissions -p hyperion USER \".*\" \".*\" \".*\" 6. Check access to the WebUI \u00b6 Try to access RabbitMQ WebUI at http://localhost:15672 . Redis \u00b6 sudo apt install redis-server Redis will also start automatically after installation. Configuration \u00b6 1. Update Redis supervision method \u00b6 Change the supervised configuration from supervised no to supervised systemd on /etc/redis/redis.conf . Note By default, Redis binds to the localhost address. You need to edit bind in the config file if you want to listen to other network. 2. Restart Redis \u00b6 sudo systemctl restart redis.service NodeJS \u00b6 curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash - sudo apt-get install -y nodejs Attention Make sure to configure npm not to use sudo when installing global packages. PM2 \u00b6 npm install pm2@latest -g Configuration \u00b6 1. Configure for system startup \u00b6 pm2 startup EOSIO \u00b6 wget https://github.com/eosio/eos/releases/download/v2.1.0/eosio_2.1.0-1-ubuntu-20.04_amd64.deb sudo apt install ./eosio_2.1.0-1-ubuntu-20.04_amd64.deb Configuration \u00b6 Add the following configuration to the config.ini file: state-history-dir = \"state-history\" trace-history = true chain-state-history = true state-history-endpoint = 127.0.0.1:8080 plugin = eosio::chain_api_plugin plugin = eosio::state_history_plugin Hyperion \u00b6 If everything runs smoothly, it's time to install Hyperion! :fontawesome-solid-glass-cheers: To do that, simply run the following commands: git clone https://github.com/eosrio/hyperion-history-api.git cd hyperion-history-api npm install","title":"Manual Installation"},{"location":"manual_installation/#manual-installation","text":"This section describes how to manually install Hyperion and its environment. If you want more control of your installation, this is the way to go. Attention Recommended OS: Ubuntu 20.04","title":"Manual Installation"},{"location":"manual_installation/#dependencies","text":"Below you can find the list of all Hyperion's dependencies: Elasticsearch 7.16.X Kibana 7.16.X RabbitMQ Redis Node.js v16 PM2 EOSIO 2.1 On the next steps you will install and configure each one of them. Note The Hyperion Indexer requires Node.js and pm2 to be on the same machine. All other dependencies (Elasticsearch, RabbitMQ, Redis and EOSIO) can be installed on different machines, preferably on a high speed and low latency network. Keep in mind that indexing speed will vary greatly depending on this configuration.","title":"Dependencies"},{"location":"manual_installation/#elasticsearch","text":"Follow the detailed installation instructions on the official Elasticsearch documentation and return to this guide before running it. Info Elasticsearch is not started automatically after installation. We recommend running it with systemd . Note It is very important to know the Elasticsearch directory layout and to understand how the configuration works.","title":"Elasticsearch"},{"location":"manual_installation/#configuration","text":"","title":"Configuration"},{"location":"manual_installation/#1-elasticsearch-configuration","text":"Edit the following lines on /etc/elasticsearch/elasticsearch.yml : cluster.name: CLUSTER_NAME bootstrap.memory_lock: true The memory lock option will prevent any Elasticsearch heap memory from being swapped out. Warning Setting bootstrap.memory_lock: true will make Elasticsearch try to use all the RAM configured for JVM on startup ( check next step). This can cause the application to crash if you allocate more RAM than available. Note A different approach is to disable swapping on your system. Testing After starting Elasticsearch, you can see whether this setting was applied successfully by checking the value of mlockall in the output from this request: curl -X GET \"localhost:9200/_nodes?filter_path=**.mlockall&pretty\"","title":"1. Elasticsearch configuration"},{"location":"manual_installation/#2-heap-size-configuration","text":"For a optimized heap size, check how much RAM can be allocated by the JVM on your system. Run the following command: java -Xms16g -Xmx16g -XX:+UseCompressedOops -XX:+PrintFlagsFinal Oops | grep Oops Check if UseCompressedOops is true on the results and change -Xms and -Xmx to the desired value. Note Elasticsearch includes a bundled version of OpenJDK from the JDK maintainers. You can find it on /usr/share/elasticsearch/jdk . After that, change the heap size by editting the following lines on /etc/elasticsearch/jvm.options : -Xms16g -Xmx16g Note Xms and Xmx must have the same value. Warning Avoid allocating more than 31GB when setting your heap size, even if you have enough RAM.","title":"2. Heap size configuration"},{"location":"manual_installation/#3-allow-memory-lock","text":"Override systemd configuration by running sudo systemctl edit elasticsearch and add the following lines: [Service] LimitMEMLOCK=infinity Run the following command to reload units: sudo systemctl daemon-reload","title":"3. Allow memory lock"},{"location":"manual_installation/#4-start-elasticsearch","text":"Start Elasticsearch and check the logs: sudo systemctl start elasticsearch.service sudo less /var/log/elasticsearch/CLUSTE_NAME.log Enable it to run at startup: sudo systemctl enable elasticsearch.service And finally, test the REST API: curl -X GET \"localhost:9200/?pretty\" Note Don't forget to check if memory lock worked. The expected result should be something like this: { \"name\" : \"ip-172-31-5-121\" , \"cluster_name\" : \"CLUSTER_NAME\" , \"cluster_uuid\" : \"FFl8DNcOQV-dVk3p1JDNMA\" , \"version\" : { \"number\" : \"7.14.1\" , \"build_flavor\" : \"default\" , \"build_type\" : \"deb\" , \"build_hash\" : \"606a173\" , \"build_date\" : \"2021-08-26T00:43:15.323135Z\" , \"build_snapshot\" : false , \"lucene_version\" : \"8.9.0\" , \"minimum_wire_compatibility_version\" : \"6.8.0\" , \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" }, \"tagline\" : \"You Know, for Search\" }","title":"4. Start Elasticsearch"},{"location":"manual_installation/#5-set-up-minimal-security","text":"The Elasticsearch security features are disabled by default. To avoid security problems, we recommend enabling the security pack. To do that, add the following line to the end of the /etc/elasticsearch/elasticsearch.yml file: xpack.security.enabled: true Restart Elasticsearch and set the passwords for the cluster: sudo systemctl restart elasticsearch.service sudo /usr/share/elasticsearch/bin/elasticsearch-setup-passwords auto Keep track of these passwords, we\u2019ll need them again soon. Note You can alternatively use the interactive parameter to manually define your passwords. Attention The minimal security scenario is not sufficient for production mode clusters. Check the documentation for more information.","title":"5. Set up minimal security"},{"location":"manual_installation/#kibana","text":"Follow the detailed installation instructions on the official Kibana documentation . Return to this documentation before running it. Info Kibana is not started automatically after installation. We recomend running it with systemd . Note Like on Elasticsearch, it is very important to know the Kibana directory layout and to understand how the configuration works.","title":"Kibana"},{"location":"manual_installation/#configuration_1","text":"","title":"Configuration"},{"location":"manual_installation/#1-elasticsearch-security","text":"If you have enabled the security pack on Elasticsearch, you need to set up the password on Kibana. Edit the folowing lines on the /etc/kibana/kibana.yml file: elasticsearch.username: \"kibana_system\" elasticsearch.password: \"password\"","title":"1. Elasticsearch security"},{"location":"manual_installation/#2-start-kibana","text":"Start Kibana and check the logs: sudo systemctl start kibana.service sudo less /var/log/kibana/kibana.log Enable it to run at startup: sudo systemctl enable kibana.service","title":"2. Start Kibana"},{"location":"manual_installation/#rabbitmq","text":"Follow the detailed installation instructions on the official RabbitMQ documentation . RabbitMQ should automatically start after installation. Check the documentation for more details on how to manage its service.","title":"RabbitMQ"},{"location":"manual_installation/#configuration_2","text":"","title":"Configuration"},{"location":"manual_installation/#1-enable-the-webui","text":"sudo rabbitmq-plugins enable rabbitmq_management","title":"1. Enable the WebUI"},{"location":"manual_installation/#2-add-vhost","text":"sudo rabbitmqctl add_vhost hyperion","title":"2. Add vhost"},{"location":"manual_installation/#3-create-a-user-and-password","text":"sudo rabbitmqctl add_user USER PASSWORD","title":"3. Create a user and password"},{"location":"manual_installation/#4-set-the-user-as-administrator","text":"sudo rabbitmqctl set_user_tags USER administrator","title":"4. Set the user as administrator"},{"location":"manual_installation/#5-set-the-user-permissions-to-the-vhost","text":"sudo rabbitmqctl set_permissions -p hyperion USER \".*\" \".*\" \".*\"","title":"5. Set the user permissions to the vhost"},{"location":"manual_installation/#6-check-access-to-the-webui","text":"Try to access RabbitMQ WebUI at http://localhost:15672 .","title":"6. Check access to the WebUI"},{"location":"manual_installation/#redis","text":"sudo apt install redis-server Redis will also start automatically after installation.","title":"Redis"},{"location":"manual_installation/#configuration_3","text":"","title":"Configuration"},{"location":"manual_installation/#1-update-redis-supervision-method","text":"Change the supervised configuration from supervised no to supervised systemd on /etc/redis/redis.conf . Note By default, Redis binds to the localhost address. You need to edit bind in the config file if you want to listen to other network.","title":"1. Update Redis supervision method"},{"location":"manual_installation/#2-restart-redis","text":"sudo systemctl restart redis.service","title":"2. Restart Redis"},{"location":"manual_installation/#nodejs","text":"curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash - sudo apt-get install -y nodejs Attention Make sure to configure npm not to use sudo when installing global packages.","title":"NodeJS"},{"location":"manual_installation/#pm2","text":"npm install pm2@latest -g","title":"PM2"},{"location":"manual_installation/#configuration_4","text":"","title":"Configuration"},{"location":"manual_installation/#1-configure-for-system-startup","text":"pm2 startup","title":"1. Configure for system startup"},{"location":"manual_installation/#eosio","text":"wget https://github.com/eosio/eos/releases/download/v2.1.0/eosio_2.1.0-1-ubuntu-20.04_amd64.deb sudo apt install ./eosio_2.1.0-1-ubuntu-20.04_amd64.deb","title":"EOSIO"},{"location":"manual_installation/#configuration_5","text":"Add the following configuration to the config.ini file: state-history-dir = \"state-history\" trace-history = true chain-state-history = true state-history-endpoint = 127.0.0.1:8080 plugin = eosio::chain_api_plugin plugin = eosio::state_history_plugin","title":"Configuration"},{"location":"manual_installation/#hyperion","text":"If everything runs smoothly, it's time to install Hyperion! :fontawesome-solid-glass-cheers: To do that, simply run the following commands: git clone https://github.com/eosrio/hyperion-history-api.git cd hyperion-history-api npm install","title":"Hyperion"},{"location":"rabbit/","text":"RabbitMQ \u00b6 How to Delete all the queues from RabbitMQ? \u00b6 1. Using Policies - Management Console \u00b6 Go to Management Console Click on Admin tab Policies tab (on the right side) Add Policy Fill Fields Virtual Host: Select (Default is /hyperion) Name: Expire All Policies(Delete Later) Pattern: .* Apply to: Queues Definition: expires with value 1 (change type from String to Number) Click on Add / update policy Checkout Queues tab again, all queues must be deleted. Warning You must remove this policy after this operation. 2. Using command line \u00b6 First, list your queues: rabbitmqadmin list queues name Then from the list, you'll need to manually delete them one by one: rabbitmqadmin delete queue name = 'queuename' Because of the output format, doesn't appear you can grep the response from list queues . Alternatively, if you're just looking for a way to clear everything, use: rabbitmqctl stop_app rabbitmqctl reset rabbitmqctl start_app Warning Be sure you really want to do this! This gonna reset all your rabbitMQ configuration, and return it to the default state.","title":"RabbitMQ"},{"location":"rabbit/#rabbitmq","text":"","title":"RabbitMQ"},{"location":"rabbit/#how-to-delete-all-the-queues-from-rabbitmq","text":"","title":"How to Delete all the queues from RabbitMQ?"},{"location":"rabbit/#1-using-policies-management-console","text":"Go to Management Console Click on Admin tab Policies tab (on the right side) Add Policy Fill Fields Virtual Host: Select (Default is /hyperion) Name: Expire All Policies(Delete Later) Pattern: .* Apply to: Queues Definition: expires with value 1 (change type from String to Number) Click on Add / update policy Checkout Queues tab again, all queues must be deleted. Warning You must remove this policy after this operation.","title":"1. Using Policies - Management Console"},{"location":"rabbit/#2-using-command-line","text":"First, list your queues: rabbitmqadmin list queues name Then from the list, you'll need to manually delete them one by one: rabbitmqadmin delete queue name = 'queuename' Because of the output format, doesn't appear you can grep the response from list queues . Alternatively, if you're just looking for a way to clear everything, use: rabbitmqctl stop_app rabbitmqctl reset rabbitmqctl start_app Warning Be sure you really want to do this! This gonna reset all your rabbitMQ configuration, and return it to the default state.","title":"2. Using command line"},{"location":"stream_client/","text":"Hyperion Stream Client \u00b6 Streaming Client for Hyperion History API (v3+) \u00b6 Usage \u00b6 Supported Environments \u00b6 Node.js v16 and up ES Module CommonJS Browsers ES Module - Angular, React and other frameworks UMD Quick Start \u00b6 Installing via npm package \u00b6 npm install @eosrio/hyperion-stream-client --save Importing the client \u00b6 ESM (Node and Browser): import { HyperionStreamClient } from \"@eosrio/hyperion-stream-client\" ; CommonJs (Node): const { HyperionStreamClient } = require ( '@eosrio/hyperion-stream-client' ); Browser library (served from public Hyperion APIs) \u00b6 Without installing via npm, you can also load the webpack bundle directly: < script src = \"https://<ENDPOINT>/stream-client.js\" ></ script > Where <ENDPOINT> is the Hyperion API (e.g. https://eos.hyperion.eosrio.io ) For other usages, the bundle is also available at dist/hyperion-stream-client.js 1. Connection \u00b6 Set up the endpoint that you want to fetch data from: const client = new HyperionStreamClient ({ endpoint : 'https://example.com' , debug : true , libStream : false }); https://example.com is the host, from where https://example.com/v2/history/... is served. set libStream to true if you want to enable a stream of only irreversible data . Don't forget to attach the handler using the method: setAsyncLibDataHandler(handler: AsyncHandlerFunction) set debug to true to print debugging messages 2. Making requests \u00b6 to ensure the client is connected, requests should be made only after calling the client.connect() method, refer to examples below; 2.1 Action Stream - client.streamActions \u00b6 client.streamActions(request: StreamActionsRequest): void contract - contract account action - action name account - notified account name start_from - start reading on block or on a specific date. 0=disabled means it will read starting from HEAD block. read_until - stop reading on block (0=disable) or on a specific date (0=disabled) filters - actions filter (more details below) Notes Block number can be either positive or negative - E.g.: 700 (start from block 700) In case of negative block number, it will be subtracted from the HEAD - E.g.: -150 (since 150 blocks ago) Date format (ISO 8601) - e.g. 2020-01-01T00:00:00.000Z import { HyperionStreamClient , StreamClientEvents } from \"@eosrio/hyperion-stream-client\" ; const client = new HyperionStreamClient ({ endpoint : \"https://sidechain.node.tibs.app\" , debug : true , libStream : false }); client . on ( StreamClientEvents . LIBUPDATE , ( data : EventData ) => { console . log ( data ); }); client . on ( 'connect' , () => { console . log ( 'connected!' ); }); client . setAsyncDataHandler ( async ( data ) => { console . log ( data ); // process incoming data, replace with your code // await processSomethingHere(); }) await client . connect (); client . streamActions ({ contract : 'eosio' , action : 'voteproducer' , account : '' , start_from : '2020-03-15T00:00:00.000Z' , read_until : 0 , filters : [], }); 2.1.1 Act Data Filters \u00b6 You can set up filters to refine your stream. Filters should use fields following the Hyperion Action Data Structure, such as: act.data.producers (on eosio::voteproducer) @transfer.to (here the @ prefix is required since transfers have special mappings) Please refer to the mapping definitions to know which data fields are available For example, to filter the stream for every transfer made to the eosio.ramfee account: client . streamActions ({ contract : 'eosio.token' , action : 'transfer' , account : 'eosio' , start_from : 0 , read_until : 0 , filters : [ { field : '@transfer.to' , value : 'eosio.ramfee' } ], }); To refine even more your stream, you could add more filters. Remember that adding more filters will result in AND operations. For OR operations setup another request. 2.2 Delta Stream (contract rows) - client.streamDeltas \u00b6 client.streamDeltas(request: StreamDeltasRequest): void code - contract account table - table name scope - table scope payer - ram payer start_from - start reading on block or on a specific date. 0=disabled means it will read starting from HEAD block. read_until - stop reading on block (0=disable) or on a specific date (0=disabled) Example: Referring to the same pattern as the action stream example above, one could also include a delta stream request client . streamDeltas ({ code : 'eosio.token' , table : '*' , scope : '' , payer : '' , start_from : 0 , read_until : 0 , }); Note: Delta filters are planned to be implemented soon. 3. Handling Data \u00b6 Incoming data handler is defined via the client.setAsyncDataHandler(async (data)=> void) method if you set libStream to true another stream of only irreversible data will be available. Don't forget to attach the handler using the method: setAsyncLibDataHandler(handler: AsyncHandlerFunction) data object is structured as follows: type - action | delta mode - live | history content - Hyperion Data Structure ( see action index and delta index templates) client . setAsyncDataHandler ( async ( data ) => { console . log ( data ); // process incoming data, replace with your code // await processSomethingHere(); }) // irreversible data stream only for when libStream: true on client connection setup client . setAsyncLibDataHandler ( async ( data ) => { console . log ( data ); // process incoming data, replace with your code // await processSomethingHere(); }) Useful information about load-balancing multiple Socket.IO servers: https://socket.io/docs/v4/using-multiple-nodes/#NginX-configuration","title":"Stream Client"},{"location":"stream_client/#hyperion-stream-client","text":"","title":"Hyperion Stream Client"},{"location":"stream_client/#streaming-client-for-hyperion-history-api-v3","text":"","title":"Streaming Client for Hyperion History API (v3+)"},{"location":"stream_client/#usage","text":"","title":"Usage"},{"location":"stream_client/#supported-environments","text":"Node.js v16 and up ES Module CommonJS Browsers ES Module - Angular, React and other frameworks UMD","title":"Supported Environments"},{"location":"stream_client/#quick-start","text":"","title":"Quick Start"},{"location":"stream_client/#installing-via-npm-package","text":"npm install @eosrio/hyperion-stream-client --save","title":"Installing via npm package"},{"location":"stream_client/#importing-the-client","text":"ESM (Node and Browser): import { HyperionStreamClient } from \"@eosrio/hyperion-stream-client\" ; CommonJs (Node): const { HyperionStreamClient } = require ( '@eosrio/hyperion-stream-client' );","title":"Importing the client"},{"location":"stream_client/#browser-library-served-from-public-hyperion-apis","text":"Without installing via npm, you can also load the webpack bundle directly: < script src = \"https://<ENDPOINT>/stream-client.js\" ></ script > Where <ENDPOINT> is the Hyperion API (e.g. https://eos.hyperion.eosrio.io ) For other usages, the bundle is also available at dist/hyperion-stream-client.js","title":"Browser library (served from public Hyperion APIs)"},{"location":"stream_client/#1-connection","text":"Set up the endpoint that you want to fetch data from: const client = new HyperionStreamClient ({ endpoint : 'https://example.com' , debug : true , libStream : false }); https://example.com is the host, from where https://example.com/v2/history/... is served. set libStream to true if you want to enable a stream of only irreversible data . Don't forget to attach the handler using the method: setAsyncLibDataHandler(handler: AsyncHandlerFunction) set debug to true to print debugging messages","title":"1. Connection"},{"location":"stream_client/#2-making-requests","text":"to ensure the client is connected, requests should be made only after calling the client.connect() method, refer to examples below;","title":"2. Making requests"},{"location":"stream_client/#21-action-stream-clientstreamactions","text":"client.streamActions(request: StreamActionsRequest): void contract - contract account action - action name account - notified account name start_from - start reading on block or on a specific date. 0=disabled means it will read starting from HEAD block. read_until - stop reading on block (0=disable) or on a specific date (0=disabled) filters - actions filter (more details below) Notes Block number can be either positive or negative - E.g.: 700 (start from block 700) In case of negative block number, it will be subtracted from the HEAD - E.g.: -150 (since 150 blocks ago) Date format (ISO 8601) - e.g. 2020-01-01T00:00:00.000Z import { HyperionStreamClient , StreamClientEvents } from \"@eosrio/hyperion-stream-client\" ; const client = new HyperionStreamClient ({ endpoint : \"https://sidechain.node.tibs.app\" , debug : true , libStream : false }); client . on ( StreamClientEvents . LIBUPDATE , ( data : EventData ) => { console . log ( data ); }); client . on ( 'connect' , () => { console . log ( 'connected!' ); }); client . setAsyncDataHandler ( async ( data ) => { console . log ( data ); // process incoming data, replace with your code // await processSomethingHere(); }) await client . connect (); client . streamActions ({ contract : 'eosio' , action : 'voteproducer' , account : '' , start_from : '2020-03-15T00:00:00.000Z' , read_until : 0 , filters : [], });","title":"2.1 Action Stream - client.streamActions"},{"location":"stream_client/#211-act-data-filters","text":"You can set up filters to refine your stream. Filters should use fields following the Hyperion Action Data Structure, such as: act.data.producers (on eosio::voteproducer) @transfer.to (here the @ prefix is required since transfers have special mappings) Please refer to the mapping definitions to know which data fields are available For example, to filter the stream for every transfer made to the eosio.ramfee account: client . streamActions ({ contract : 'eosio.token' , action : 'transfer' , account : 'eosio' , start_from : 0 , read_until : 0 , filters : [ { field : '@transfer.to' , value : 'eosio.ramfee' } ], }); To refine even more your stream, you could add more filters. Remember that adding more filters will result in AND operations. For OR operations setup another request.","title":"2.1.1 Act Data Filters"},{"location":"stream_client/#22-delta-stream-contract-rows-clientstreamdeltas","text":"client.streamDeltas(request: StreamDeltasRequest): void code - contract account table - table name scope - table scope payer - ram payer start_from - start reading on block or on a specific date. 0=disabled means it will read starting from HEAD block. read_until - stop reading on block (0=disable) or on a specific date (0=disabled) Example: Referring to the same pattern as the action stream example above, one could also include a delta stream request client . streamDeltas ({ code : 'eosio.token' , table : '*' , scope : '' , payer : '' , start_from : 0 , read_until : 0 , }); Note: Delta filters are planned to be implemented soon.","title":"2.2 Delta Stream (contract rows) - client.streamDeltas"},{"location":"stream_client/#3-handling-data","text":"Incoming data handler is defined via the client.setAsyncDataHandler(async (data)=> void) method if you set libStream to true another stream of only irreversible data will be available. Don't forget to attach the handler using the method: setAsyncLibDataHandler(handler: AsyncHandlerFunction) data object is structured as follows: type - action | delta mode - live | history content - Hyperion Data Structure ( see action index and delta index templates) client . setAsyncDataHandler ( async ( data ) => { console . log ( data ); // process incoming data, replace with your code // await processSomethingHere(); }) // irreversible data stream only for when libStream: true on client connection setup client . setAsyncLibDataHandler ( async ( data ) => { console . log ( data ); // process incoming data, replace with your code // await processSomethingHere(); }) Useful information about load-balancing multiple Socket.IO servers: https://socket.io/docs/v4/using-multiple-nodes/#NginX-configuration","title":"3. Handling Data"},{"location":"v1/","text":"API Reference: v1 \u00b6 /v1/history/get_actions \u00b6 POST \u00b6 Summary \u00b6 get actions Description \u00b6 legacy get actions query Request Body \u00b6 { \"account_name\": \"string\", \"pos\": 0, \"offset\": 0, \"filter\": \"string\", \"sort\": \"desc\", \"after\": \"2020-01-17T19:51:03.618Z\", \"before\": \"2020-01-17T19:51:03.618Z\", \"parent\": 0 } Schema \u00b6 variable type description account_name string minLength: 1 maxLength: 12 notified account pos integer action position (pagination) offset integer limit of [n] actions per page filter string minLength: 3 code:name filter sort string sort direction Enum: [ desc, asc, 1, -1 ] after string($date-time) filter after specified date (ISO8601) before string($date-time) filter before specified date (ISO8601) parent integer minimum: 0 filter by parent global sequence Responses \u00b6 Code Description 200 Example \u00b6 curl -X POST \"https://eos.hyperion.eosrio.io/v1/history/get_actions\" /v1/history/get_controlled_accounts \u00b6 POST \u00b6 Summary \u00b6 get controlled accounts by controlling accounts Description \u00b6 get controlled accounts by controlling accounts Request Body Required \u00b6 { \"controlling_account\": \"string\" } Schema \u00b6 variable type description controlling_account string controlling account Responses \u00b6 Code Description 200 Example \u00b6 curl -X POST \"https://eos.hyperion.eosrio.io/v1/history/get_controlled_accounts\" -d '{\"controlling_account\":\"eosio\"}' /v1/history/get_key_accounts \u00b6 POST \u00b6 Summary \u00b6 get accounts by public key Description \u00b6 get accounts by public key Request Body Required \u00b6 { \"public_key\": \"string\" } Schema \u00b6 variable type description public_key public key public key Responses \u00b6 Code Description 200 Default Response Example \u00b6 curl -X POST \"https://eos.hyperion.eosrio.io/v1/history/get_key_accounts\" -d '{\"public_key\":\"EOS8fDDZm7ommT5XBf9MPYkRioXX6GeCUeSNkTpimdwKon5bNAVm7\"}' Code Description 200 /v1/history/get_transaction \u00b6 POST \u00b6 Summary \u00b6 get transaction by id Description \u00b6 get all actions belonging to the same transaction Request Body Required \u00b6 { \"id\": \"string\" } Schema \u00b6 variable type description id string transaction id Responses \u00b6 Code Description 200 Default Response Example \u00b6 curl -X POST \"https://eos.hyperion.eosrio.io/v1/history/get_transaction\" /v1/chain/get_block \u00b6 POST \u00b6 Summary \u00b6 Returns an object containing various details about a specific block on the blockchain. Description \u00b6 Returns an object containing various details about a specific block on the blockchain. Request Body \u00b6 { \"block_num_or_id\": \"string\" } Schema \u00b6 block_num_or_id* -> string -> Provide a block number or a block id Responses \u00b6 Code Description 200 Default Response Example \u00b6 curl -X POST \"https://eos.hyperion.eosrio.io/v1/chain/get_block\" -d '{\"block_num_or_id\": \"1000\"}' /v1/trace_api/get_block \u00b6 POST \u00b6 Summary \u00b6 Get block traces. Description \u00b6 Get block traces. Request Body \u00b6 { \"block_num\": 0, \"block_id\": \"string\" } Schema \u00b6 block_num -> integer -> block number block_id -> string -> block id Responses \u00b6 Code Description 200 Default Response Examples \u00b6 Get block 10000: curl -X POST --url https://wax.hyperion.eosrio.io/v1/trace_api/get_block --data '{\"block_num\": 10000}' --header 'content-type: application/json' Get latest block: curl -X POST --url https://wax.hyperion.eosrio.io/v1/trace_api/get_block --data '{\"block_num\": -1}' --header 'content-type: application/json' By block_id: curl -X POST --url https://wax.hyperion.eosrio.io/v1/trace_api/get_block --data '{\"block_id\": \"0307e42d3b2328805606929438411aa78dffb1756b2a0bcb2a9f7ce8a7035990\"}' --header 'content-type: application/json'","title":"v1 compatible"},{"location":"v1/#api-reference-v1","text":"","title":"API Reference: v1"},{"location":"v1/#v1historyget_actions","text":"","title":"/v1/history/get_actions"},{"location":"v1/#post","text":"","title":"POST"},{"location":"v1/#summary","text":"get actions","title":"Summary"},{"location":"v1/#description","text":"legacy get actions query","title":"Description"},{"location":"v1/#request-body","text":"{ \"account_name\": \"string\", \"pos\": 0, \"offset\": 0, \"filter\": \"string\", \"sort\": \"desc\", \"after\": \"2020-01-17T19:51:03.618Z\", \"before\": \"2020-01-17T19:51:03.618Z\", \"parent\": 0 }","title":"Request Body"},{"location":"v1/#schema","text":"variable type description account_name string minLength: 1 maxLength: 12 notified account pos integer action position (pagination) offset integer limit of [n] actions per page filter string minLength: 3 code:name filter sort string sort direction Enum: [ desc, asc, 1, -1 ] after string($date-time) filter after specified date (ISO8601) before string($date-time) filter before specified date (ISO8601) parent integer minimum: 0 filter by parent global sequence","title":"Schema"},{"location":"v1/#responses","text":"Code Description 200","title":"Responses"},{"location":"v1/#example","text":"curl -X POST \"https://eos.hyperion.eosrio.io/v1/history/get_actions\"","title":"Example"},{"location":"v1/#v1historyget_controlled_accounts","text":"","title":"/v1/history/get_controlled_accounts"},{"location":"v1/#post_1","text":"","title":"POST"},{"location":"v1/#summary_1","text":"get controlled accounts by controlling accounts","title":"Summary"},{"location":"v1/#description_1","text":"get controlled accounts by controlling accounts","title":"Description"},{"location":"v1/#request-body-required","text":"{ \"controlling_account\": \"string\" }","title":"Request Body Required"},{"location":"v1/#schema_1","text":"variable type description controlling_account string controlling account","title":"Schema"},{"location":"v1/#responses_1","text":"Code Description 200","title":"Responses"},{"location":"v1/#example_1","text":"curl -X POST \"https://eos.hyperion.eosrio.io/v1/history/get_controlled_accounts\" -d '{\"controlling_account\":\"eosio\"}'","title":"Example"},{"location":"v1/#v1historyget_key_accounts","text":"","title":"/v1/history/get_key_accounts"},{"location":"v1/#post_2","text":"","title":"POST"},{"location":"v1/#summary_2","text":"get accounts by public key","title":"Summary"},{"location":"v1/#description_2","text":"get accounts by public key","title":"Description"},{"location":"v1/#request-body-required_1","text":"{ \"public_key\": \"string\" }","title":"Request Body Required"},{"location":"v1/#schema_2","text":"variable type description public_key public key public key","title":"Schema"},{"location":"v1/#responses_2","text":"Code Description 200 Default Response","title":"Responses"},{"location":"v1/#example_2","text":"curl -X POST \"https://eos.hyperion.eosrio.io/v1/history/get_key_accounts\" -d '{\"public_key\":\"EOS8fDDZm7ommT5XBf9MPYkRioXX6GeCUeSNkTpimdwKon5bNAVm7\"}' Code Description 200","title":"Example"},{"location":"v1/#v1historyget_transaction","text":"","title":"/v1/history/get_transaction"},{"location":"v1/#post_3","text":"","title":"POST"},{"location":"v1/#summary_3","text":"get transaction by id","title":"Summary"},{"location":"v1/#description_3","text":"get all actions belonging to the same transaction","title":"Description"},{"location":"v1/#request-body-required_2","text":"{ \"id\": \"string\" }","title":"Request Body Required"},{"location":"v1/#schema_3","text":"variable type description id string transaction id","title":"Schema"},{"location":"v1/#responses_3","text":"Code Description 200 Default Response","title":"Responses"},{"location":"v1/#example_3","text":"curl -X POST \"https://eos.hyperion.eosrio.io/v1/history/get_transaction\"","title":"Example"},{"location":"v1/#v1chainget_block","text":"","title":"/v1/chain/get_block"},{"location":"v1/#post_4","text":"","title":"POST"},{"location":"v1/#summary_4","text":"Returns an object containing various details about a specific block on the blockchain.","title":"Summary"},{"location":"v1/#description_4","text":"Returns an object containing various details about a specific block on the blockchain.","title":"Description"},{"location":"v1/#request-body_1","text":"{ \"block_num_or_id\": \"string\" }","title":"Request Body"},{"location":"v1/#schema_4","text":"block_num_or_id* -> string -> Provide a block number or a block id","title":"Schema"},{"location":"v1/#responses_4","text":"Code Description 200 Default Response","title":"Responses"},{"location":"v1/#example_4","text":"curl -X POST \"https://eos.hyperion.eosrio.io/v1/chain/get_block\" -d '{\"block_num_or_id\": \"1000\"}'","title":"Example"},{"location":"v1/#v1trace_apiget_block","text":"","title":"/v1/trace_api/get_block"},{"location":"v1/#post_5","text":"","title":"POST"},{"location":"v1/#summary_5","text":"Get block traces.","title":"Summary"},{"location":"v1/#description_5","text":"Get block traces.","title":"Description"},{"location":"v1/#request-body_2","text":"{ \"block_num\": 0, \"block_id\": \"string\" }","title":"Request Body"},{"location":"v1/#schema_5","text":"block_num -> integer -> block number block_id -> string -> block id","title":"Schema"},{"location":"v1/#responses_5","text":"Code Description 200 Default Response","title":"Responses"},{"location":"v1/#examples","text":"Get block 10000: curl -X POST --url https://wax.hyperion.eosrio.io/v1/trace_api/get_block --data '{\"block_num\": 10000}' --header 'content-type: application/json' Get latest block: curl -X POST --url https://wax.hyperion.eosrio.io/v1/trace_api/get_block --data '{\"block_num\": -1}' --header 'content-type: application/json' By block_id: curl -X POST --url https://wax.hyperion.eosrio.io/v1/trace_api/get_block --data '{\"block_id\": \"0307e42d3b2328805606929438411aa78dffb1756b2a0bcb2a9f7ce8a7035990\"}' --header 'content-type: application/json'","title":"Examples"},{"location":"v2/","text":"API Reference: v2 \u00b6 Tip For more details and live testing, check out our WAX swagger /v2/history/get_abi_snapshot \u00b6 GET \u00b6 Summary \u00b6 Fetch contract ABI Description \u00b6 Fetch contract ABI at a specific block if specified. Parameters \u00b6 Name Located in Description Required Schema contract query Contract account Yes string block query Target block No integer fetch query Fetch the ABI No boolean Responses \u00b6 Code Description 200 Default Response Examples \u00b6 /v2/history/get_abi_snapshot?contract=eosio.token&fetch=true /v2/history/get_actions \u00b6 GET \u00b6 Summary \u00b6 Get actions Description \u00b6 Get past actions based on notified account. Parameters \u00b6 Name Located in Description Required Schema account query Notified account No string filter query Filter by code:name No string track query Total results to track (count) No string skip query Skip [n] actions (pagination) No integer limit query Limit of [n] actions per page No integer sort query Sort direction ('desc', 'asc', '1' or '-1') No string after query Filter actions after specified date (ISO8601) No string before query Filter actions before specified date (ISO8601) No string simple query Simplified output mode No boolean noBinary query exclude large binary data No boolean checkLib query perform reversibility check No boolean Note Besides the parameters listed above, this endpoint also accepts generic parameters based on indexed fields (e.g. act.authorization.actor=eosio or act.name=delegatebw ). If included they will be combined with an AND operator. Tip You can check action mappings on index-templates.ts file for the list of all possible parameters. Responses \u00b6 Code Description 200 Default Response Examples \u00b6 /v2/history/get_actions?account=eosio&act.name=delegatebw noBinary - Will suppress large hex payloads to reduce the transmission latency /v2/history/get_actions?noBinary=true checkLib - Performs lib test. The default is false, meaning the query will be faster since it won't wait for the get_info to happen. If you wish to see the lib value on the results you must request explicitly. /v2/history/get_actions?checkLib=true The simple mode will add the irreversible field ( true/false ) on each action /v2/history/get_actions?checkLib=true&simple=true /v2/history/get_created_accounts \u00b6 GET \u00b6 Summary \u00b6 Get created accounts Description \u00b6 Get all accounts created by one creator. Parameters \u00b6 Name Located in Description Required Schema account query Account creator Yes string Responses \u00b6 Code Description 200 Default Response Examples \u00b6 /v2/history/get_created_accounts?account=eosio /v2/history/get_creator \u00b6 GET \u00b6 Summary \u00b6 Get account creator Description \u00b6 Get the creator of a specific account. Parameters \u00b6 Name Located in Description Required Schema account query Created account Yes string Responses \u00b6 Code Description 200 Default Response Examples \u00b6 /v2/history/get_creator?account=eosriobrazil /v2/history/get_deltas \u00b6 GET \u00b6 Summary \u00b6 Get state deltas Description \u00b6 Get state deltas of any table. Parameters \u00b6 Name Located in Description Required Schema code query Contract account No string scope query Table scope No string table query Table name No string payer query Payer account No string Note Besides the parameters listed above, this endpoint also accepts generic parameters based on indexed fields (e.g. block_num=10000000 ). If included they will be combined with an AND operator. Tip You can check delta mappings on index-templates.ts file for the list of all possible parameters. Responses \u00b6 Code Description 200 Default Response Examples \u00b6 Get all deltas from eosio.token contract on a specific block /v2/history/get_deltas?code=eosio.token&block_num=100000 Get all deltas from the table accounts of the eosio.token contract /v2/history/get_deltas?code=eosio.token&table=accounts /v2/history/get_transacted_accounts \u00b6 GET \u00b6 Summary \u00b6 Get account interactions based on transfers Description \u00b6 Get all accounts that interacted with the source account provided. Interactions can be from or to (direction) source account. Parameters \u00b6 Name Located in Description Required Schema account query Source account Yes string direction query Interaction direction ('in', 'out' or 'both') Yes string symbol query Token symbol No string contract query Token contract No string min query Transfer minimum value No number max query Transfer maximum value No number limit query Limit of [n] interactions No number after query Filter actions after specified date (ISO8601) or block number No string before query Filter actions before specified date (ISO8601) or block number No string Responses \u00b6 Code Description 200 Default Response Examples \u00b6 /v2/history/get_transacted_accounts?account=eosriobrazil&direction=out /v2/history/get_transaction \u00b6 GET \u00b6 Summary \u00b6 Get transaction by id Description \u00b6 Get a transaction with all its actions. Parameters \u00b6 Name Located in Description Required Schema id query Transaction ID Yes string Responses \u00b6 Code Description 200 Default Response Examples \u00b6 v2/history/get_transaction?id=eec44c2ab2c2330e88fdacd3cd4c63838adb60da679ff12f299a4341fd036658 /v2/history/get_transfers \u00b6 GET \u00b6 Summary \u00b6 Get token transfers Description \u00b6 Get token transfers from contracts using the eosio.token standard. Parameters \u00b6 Name Located in Description Required Schema from query Source account No string to query Destination account No string symbol query Token symbol No string contract query Token contract No string skip query Skip [n] actions (pagination) No integer limit query Limit of [n] actions per page No integer after query Filter actions after specified date (ISO8601) No string before query Filter actions before specified date (ISO8601) No string Responses \u00b6 Code Description 200 Default Response Examples \u00b6 Get all transfers from eosriobrazil account /v2/history/get_transfers?from=eosriobrazil Get all transfers from eosriobrazil account to eosio.ramfee account /v2/history/get_transfers?from=eosriobrazil&to=eosio.ramfee\" Get all transfers from eosriobrazil account to eosio.ramfee account after November 2019 /v2/history/get_transfers?from=eosriobrazil&to=eosio.ramfee&after=2019-11-01T00:00:00.000Z /v2/state/get_account \u00b6 GET \u00b6 Summary \u00b6 Get account summary Description \u00b6 Get all data related to an account. Parameters \u00b6 Name Located in Description Required Schema account query Account name No string Responses \u00b6 Code Description 200 Default Response Examples \u00b6 /v2/state/get_account?account=eosio /v2/state/get_key_accounts \u00b6 GET \u00b6 Summary \u00b6 Get accounts by public key Description \u00b6 Get all accounts with specified public key. Parameters \u00b6 Name Located in Description Required Schema public_key query Public key Yes string Responses \u00b6 Code Description 200 Default Response Examples \u00b6 /v2/state/get_key_accounts?public_key=EOS8fDDZm7ommT5XBf9MPYkRioXX6GeCUeSNkTpimdwKon5bNAVm7\" /v2/state/get_key_accounts \u00b6 POST \u00b6 Summary \u00b6 Get accounts by public key Description \u00b6 Get all accounts with specified public key. Parameters \u00b6 Name Located in Description Required Schema public_key body Public key Yes string Responses \u00b6 Code Description 200 Default Response Examples \u00b6 /v2/state/get_key_accounts - body: {\"public_key\":\"EOS8fDDZm7ommT5XBf9MPYkRioXX6GeCUeSNkTpimdwKon5bNAVm7\"} /v2/state/get_links \u00b6 GET \u00b6 Summary \u00b6 Get permission links Description \u00b6 Get permission links from an account. Parameters \u00b6 Name Located in Description Required Schema account name (query) Account name Yes string code (query) Contract name No string action (query) Method name No string permission (query) Permission name No string Responses \u00b6 Code Description 200 Default Response Examples \u00b6 /v2/state/get_links?account=eosriobrazil /v2/state/get_proposals \u00b6 GET \u00b6 Summary \u00b6 Get proposals Description \u00b6 Get all available proposals. Parameters \u00b6 Name Located in Description Required Schema proposer query Filter by proposer No string proposal query Filter by proposal name No string account query Filter by either requested or provided account No string requested query Filter by requested account No string provided query Filter by provided account No string executed query Filter by execution status No boolean track query Total results to track (count) No string skip query Skip [n] actions (pagination) No integer limit query Limit of [n] actions per page No integer Responses \u00b6 Code Description 200 Default Response Examples \u00b6 /v2/state/get_proposals?proposer=eosriobrazil /v2/state/get_tokens \u00b6 GET \u00b6 Summary \u00b6 Get tokens from account Description \u00b6 Retrieve all tokens holded by an account. Parameters \u00b6 Name Located in Description Required Schema account query Account name Yes string Responses \u00b6 Code Description 200 Default Response Examples \u00b6 /v2/state/get_tokens?account=eosriobrazil /v2/state/get_voters \u00b6 GET \u00b6 Summary \u00b6 Get voters Description \u00b6 Get all voting accounts. Parameters \u00b6 Name Located in Description Required Schema producer query Filter by voted producer (comma separated) No string skip query Skip [n] actions (pagination) No integer limit query Limit of [n] actions per page No integer Responses \u00b6 Code Description 200 Default Response Examples \u00b6 Get all eosriobrazil voters /v2/state/get_voters?producer=eosriobrazil Get only the first 3 responses /v2/state/get_voters?producer=eosriobrazil&limit=3 /v2/health \u00b6 GET \u00b6 Summary: \u00b6 API Service Health Report Responses \u00b6 Code Description 200 Default Response","title":"v2"},{"location":"v2/#api-reference-v2","text":"Tip For more details and live testing, check out our WAX swagger","title":"API Reference: v2"},{"location":"v2/#v2historyget_abi_snapshot","text":"","title":"/v2/history/get_abi_snapshot"},{"location":"v2/#get","text":"","title":"GET"},{"location":"v2/#summary","text":"Fetch contract ABI","title":"Summary"},{"location":"v2/#description","text":"Fetch contract ABI at a specific block if specified.","title":"Description"},{"location":"v2/#parameters","text":"Name Located in Description Required Schema contract query Contract account Yes string block query Target block No integer fetch query Fetch the ABI No boolean","title":"Parameters"},{"location":"v2/#responses","text":"Code Description 200 Default Response","title":"Responses"},{"location":"v2/#examples","text":"/v2/history/get_abi_snapshot?contract=eosio.token&fetch=true","title":"Examples"},{"location":"v2/#v2historyget_actions","text":"","title":"/v2/history/get_actions"},{"location":"v2/#get_1","text":"","title":"GET"},{"location":"v2/#summary_1","text":"Get actions","title":"Summary"},{"location":"v2/#description_1","text":"Get past actions based on notified account.","title":"Description"},{"location":"v2/#parameters_1","text":"Name Located in Description Required Schema account query Notified account No string filter query Filter by code:name No string track query Total results to track (count) No string skip query Skip [n] actions (pagination) No integer limit query Limit of [n] actions per page No integer sort query Sort direction ('desc', 'asc', '1' or '-1') No string after query Filter actions after specified date (ISO8601) No string before query Filter actions before specified date (ISO8601) No string simple query Simplified output mode No boolean noBinary query exclude large binary data No boolean checkLib query perform reversibility check No boolean Note Besides the parameters listed above, this endpoint also accepts generic parameters based on indexed fields (e.g. act.authorization.actor=eosio or act.name=delegatebw ). If included they will be combined with an AND operator. Tip You can check action mappings on index-templates.ts file for the list of all possible parameters.","title":"Parameters"},{"location":"v2/#responses_1","text":"Code Description 200 Default Response","title":"Responses"},{"location":"v2/#examples_1","text":"/v2/history/get_actions?account=eosio&act.name=delegatebw noBinary - Will suppress large hex payloads to reduce the transmission latency /v2/history/get_actions?noBinary=true checkLib - Performs lib test. The default is false, meaning the query will be faster since it won't wait for the get_info to happen. If you wish to see the lib value on the results you must request explicitly. /v2/history/get_actions?checkLib=true The simple mode will add the irreversible field ( true/false ) on each action /v2/history/get_actions?checkLib=true&simple=true","title":"Examples"},{"location":"v2/#v2historyget_created_accounts","text":"","title":"/v2/history/get_created_accounts"},{"location":"v2/#get_2","text":"","title":"GET"},{"location":"v2/#summary_2","text":"Get created accounts","title":"Summary"},{"location":"v2/#description_2","text":"Get all accounts created by one creator.","title":"Description"},{"location":"v2/#parameters_2","text":"Name Located in Description Required Schema account query Account creator Yes string","title":"Parameters"},{"location":"v2/#responses_2","text":"Code Description 200 Default Response","title":"Responses"},{"location":"v2/#examples_2","text":"/v2/history/get_created_accounts?account=eosio","title":"Examples"},{"location":"v2/#v2historyget_creator","text":"","title":"/v2/history/get_creator"},{"location":"v2/#get_3","text":"","title":"GET"},{"location":"v2/#summary_3","text":"Get account creator","title":"Summary"},{"location":"v2/#description_3","text":"Get the creator of a specific account.","title":"Description"},{"location":"v2/#parameters_3","text":"Name Located in Description Required Schema account query Created account Yes string","title":"Parameters"},{"location":"v2/#responses_3","text":"Code Description 200 Default Response","title":"Responses"},{"location":"v2/#examples_3","text":"/v2/history/get_creator?account=eosriobrazil","title":"Examples"},{"location":"v2/#v2historyget_deltas","text":"","title":"/v2/history/get_deltas"},{"location":"v2/#get_4","text":"","title":"GET"},{"location":"v2/#summary_4","text":"Get state deltas","title":"Summary"},{"location":"v2/#description_4","text":"Get state deltas of any table.","title":"Description"},{"location":"v2/#parameters_4","text":"Name Located in Description Required Schema code query Contract account No string scope query Table scope No string table query Table name No string payer query Payer account No string Note Besides the parameters listed above, this endpoint also accepts generic parameters based on indexed fields (e.g. block_num=10000000 ). If included they will be combined with an AND operator. Tip You can check delta mappings on index-templates.ts file for the list of all possible parameters.","title":"Parameters"},{"location":"v2/#responses_4","text":"Code Description 200 Default Response","title":"Responses"},{"location":"v2/#examples_4","text":"Get all deltas from eosio.token contract on a specific block /v2/history/get_deltas?code=eosio.token&block_num=100000 Get all deltas from the table accounts of the eosio.token contract /v2/history/get_deltas?code=eosio.token&table=accounts","title":"Examples"},{"location":"v2/#v2historyget_transacted_accounts","text":"","title":"/v2/history/get_transacted_accounts"},{"location":"v2/#get_5","text":"","title":"GET"},{"location":"v2/#summary_5","text":"Get account interactions based on transfers","title":"Summary"},{"location":"v2/#description_5","text":"Get all accounts that interacted with the source account provided. Interactions can be from or to (direction) source account.","title":"Description"},{"location":"v2/#parameters_5","text":"Name Located in Description Required Schema account query Source account Yes string direction query Interaction direction ('in', 'out' or 'both') Yes string symbol query Token symbol No string contract query Token contract No string min query Transfer minimum value No number max query Transfer maximum value No number limit query Limit of [n] interactions No number after query Filter actions after specified date (ISO8601) or block number No string before query Filter actions before specified date (ISO8601) or block number No string","title":"Parameters"},{"location":"v2/#responses_5","text":"Code Description 200 Default Response","title":"Responses"},{"location":"v2/#examples_5","text":"/v2/history/get_transacted_accounts?account=eosriobrazil&direction=out","title":"Examples"},{"location":"v2/#v2historyget_transaction","text":"","title":"/v2/history/get_transaction"},{"location":"v2/#get_6","text":"","title":"GET"},{"location":"v2/#summary_6","text":"Get transaction by id","title":"Summary"},{"location":"v2/#description_6","text":"Get a transaction with all its actions.","title":"Description"},{"location":"v2/#parameters_6","text":"Name Located in Description Required Schema id query Transaction ID Yes string","title":"Parameters"},{"location":"v2/#responses_6","text":"Code Description 200 Default Response","title":"Responses"},{"location":"v2/#examples_6","text":"v2/history/get_transaction?id=eec44c2ab2c2330e88fdacd3cd4c63838adb60da679ff12f299a4341fd036658","title":"Examples"},{"location":"v2/#v2historyget_transfers","text":"","title":"/v2/history/get_transfers"},{"location":"v2/#get_7","text":"","title":"GET"},{"location":"v2/#summary_7","text":"Get token transfers","title":"Summary"},{"location":"v2/#description_7","text":"Get token transfers from contracts using the eosio.token standard.","title":"Description"},{"location":"v2/#parameters_7","text":"Name Located in Description Required Schema from query Source account No string to query Destination account No string symbol query Token symbol No string contract query Token contract No string skip query Skip [n] actions (pagination) No integer limit query Limit of [n] actions per page No integer after query Filter actions after specified date (ISO8601) No string before query Filter actions before specified date (ISO8601) No string","title":"Parameters"},{"location":"v2/#responses_7","text":"Code Description 200 Default Response","title":"Responses"},{"location":"v2/#examples_7","text":"Get all transfers from eosriobrazil account /v2/history/get_transfers?from=eosriobrazil Get all transfers from eosriobrazil account to eosio.ramfee account /v2/history/get_transfers?from=eosriobrazil&to=eosio.ramfee\" Get all transfers from eosriobrazil account to eosio.ramfee account after November 2019 /v2/history/get_transfers?from=eosriobrazil&to=eosio.ramfee&after=2019-11-01T00:00:00.000Z","title":"Examples"},{"location":"v2/#v2stateget_account","text":"","title":"/v2/state/get_account"},{"location":"v2/#get_8","text":"","title":"GET"},{"location":"v2/#summary_8","text":"Get account summary","title":"Summary"},{"location":"v2/#description_8","text":"Get all data related to an account.","title":"Description"},{"location":"v2/#parameters_8","text":"Name Located in Description Required Schema account query Account name No string","title":"Parameters"},{"location":"v2/#responses_8","text":"Code Description 200 Default Response","title":"Responses"},{"location":"v2/#examples_8","text":"/v2/state/get_account?account=eosio","title":"Examples"},{"location":"v2/#v2stateget_key_accounts","text":"","title":"/v2/state/get_key_accounts"},{"location":"v2/#get_9","text":"","title":"GET"},{"location":"v2/#summary_9","text":"Get accounts by public key","title":"Summary"},{"location":"v2/#description_9","text":"Get all accounts with specified public key.","title":"Description"},{"location":"v2/#parameters_9","text":"Name Located in Description Required Schema public_key query Public key Yes string","title":"Parameters"},{"location":"v2/#responses_9","text":"Code Description 200 Default Response","title":"Responses"},{"location":"v2/#examples_9","text":"/v2/state/get_key_accounts?public_key=EOS8fDDZm7ommT5XBf9MPYkRioXX6GeCUeSNkTpimdwKon5bNAVm7\"","title":"Examples"},{"location":"v2/#v2stateget_key_accounts_1","text":"","title":"/v2/state/get_key_accounts"},{"location":"v2/#post","text":"","title":"POST"},{"location":"v2/#summary_10","text":"Get accounts by public key","title":"Summary"},{"location":"v2/#description_10","text":"Get all accounts with specified public key.","title":"Description"},{"location":"v2/#parameters_10","text":"Name Located in Description Required Schema public_key body Public key Yes string","title":"Parameters"},{"location":"v2/#responses_10","text":"Code Description 200 Default Response","title":"Responses"},{"location":"v2/#examples_10","text":"/v2/state/get_key_accounts - body: {\"public_key\":\"EOS8fDDZm7ommT5XBf9MPYkRioXX6GeCUeSNkTpimdwKon5bNAVm7\"}","title":"Examples"},{"location":"v2/#v2stateget_links","text":"","title":"/v2/state/get_links"},{"location":"v2/#get_10","text":"","title":"GET"},{"location":"v2/#summary_11","text":"Get permission links","title":"Summary"},{"location":"v2/#description_11","text":"Get permission links from an account.","title":"Description"},{"location":"v2/#parameters_11","text":"Name Located in Description Required Schema account name (query) Account name Yes string code (query) Contract name No string action (query) Method name No string permission (query) Permission name No string","title":"Parameters"},{"location":"v2/#responses_11","text":"Code Description 200 Default Response","title":"Responses"},{"location":"v2/#examples_11","text":"/v2/state/get_links?account=eosriobrazil","title":"Examples"},{"location":"v2/#v2stateget_proposals","text":"","title":"/v2/state/get_proposals"},{"location":"v2/#get_11","text":"","title":"GET"},{"location":"v2/#summary_12","text":"Get proposals","title":"Summary"},{"location":"v2/#description_12","text":"Get all available proposals.","title":"Description"},{"location":"v2/#parameters_12","text":"Name Located in Description Required Schema proposer query Filter by proposer No string proposal query Filter by proposal name No string account query Filter by either requested or provided account No string requested query Filter by requested account No string provided query Filter by provided account No string executed query Filter by execution status No boolean track query Total results to track (count) No string skip query Skip [n] actions (pagination) No integer limit query Limit of [n] actions per page No integer","title":"Parameters"},{"location":"v2/#responses_12","text":"Code Description 200 Default Response","title":"Responses"},{"location":"v2/#examples_12","text":"/v2/state/get_proposals?proposer=eosriobrazil","title":"Examples"},{"location":"v2/#v2stateget_tokens","text":"","title":"/v2/state/get_tokens"},{"location":"v2/#get_12","text":"","title":"GET"},{"location":"v2/#summary_13","text":"Get tokens from account","title":"Summary"},{"location":"v2/#description_13","text":"Retrieve all tokens holded by an account.","title":"Description"},{"location":"v2/#parameters_13","text":"Name Located in Description Required Schema account query Account name Yes string","title":"Parameters"},{"location":"v2/#responses_13","text":"Code Description 200 Default Response","title":"Responses"},{"location":"v2/#examples_13","text":"/v2/state/get_tokens?account=eosriobrazil","title":"Examples"},{"location":"v2/#v2stateget_voters","text":"","title":"/v2/state/get_voters"},{"location":"v2/#get_13","text":"","title":"GET"},{"location":"v2/#summary_14","text":"Get voters","title":"Summary"},{"location":"v2/#description_14","text":"Get all voting accounts.","title":"Description"},{"location":"v2/#parameters_14","text":"Name Located in Description Required Schema producer query Filter by voted producer (comma separated) No string skip query Skip [n] actions (pagination) No integer limit query Limit of [n] actions per page No integer","title":"Parameters"},{"location":"v2/#responses_14","text":"Code Description 200 Default Response","title":"Responses"},{"location":"v2/#examples_14","text":"Get all eosriobrazil voters /v2/state/get_voters?producer=eosriobrazil Get only the first 3 responses /v2/state/get_voters?producer=eosriobrazil&limit=3","title":"Examples"},{"location":"v2/#v2health","text":"","title":"/v2/health"},{"location":"v2/#get_14","text":"","title":"GET"},{"location":"v2/#summary_15","text":"API Service Health Report","title":"Summary:"},{"location":"v2/#responses_15","text":"Code Description 200 Default Response","title":"Responses"},{"location":"windows/","text":"Windows \u00b6 Installation instructions for Multipass on Windows 10 with Hyper-V \u00b6 1. Create VM: \u00b6 multipass launch -m 16G -c 4 -d 16G -n hyperion *adjust -m/-d/-c according to your needs 2. Stop the VM: \u00b6 multipass stop hyperion 3. Disable Dynamic Memory \u00b6 Open the Hyper-V Manager right click the \"hyperion\" VM go on \"Settings...\" -> \"Memory\" -> Uncheck \"Enable Dynamic Memory\" 4. Start the VM: \u00b6 multipass start hyperion 5. Proceed with the automated install: \u00b6 git clone https://github.com/eosrio/hyperion-history-api.git cd hyperion-history-api ./install_env.sh 6. Check if elasticsearch memlock was successful: \u00b6 systemctl status elasticsearch.service Note \"Dynamic Memory\" can be enabled after installation if desired, just stop the vm, update the settings and restart.","title":"Windows"},{"location":"windows/#windows","text":"","title":"Windows"},{"location":"windows/#installation-instructions-for-multipass-on-windows-10-with-hyper-v","text":"","title":"Installation instructions for Multipass on Windows 10 with Hyper-V"},{"location":"windows/#1-create-vm","text":"multipass launch -m 16G -c 4 -d 16G -n hyperion *adjust -m/-d/-c according to your needs","title":"1. Create VM:"},{"location":"windows/#2-stop-the-vm","text":"multipass stop hyperion","title":"2. Stop the VM:"},{"location":"windows/#3-disable-dynamic-memory","text":"Open the Hyper-V Manager right click the \"hyperion\" VM go on \"Settings...\" -> \"Memory\" -> Uncheck \"Enable Dynamic Memory\"","title":"3. Disable Dynamic Memory"},{"location":"windows/#4-start-the-vm","text":"multipass start hyperion","title":"4. Start the VM:"},{"location":"windows/#5-proceed-with-the-automated-install","text":"git clone https://github.com/eosrio/hyperion-history-api.git cd hyperion-history-api ./install_env.sh","title":"5. Proceed with the automated install:"},{"location":"windows/#6-check-if-elasticsearch-memlock-was-successful","text":"systemctl status elasticsearch.service Note \"Dynamic Memory\" can be enabled after installation if desired, just stop the vm, update the settings and restart.","title":"6. Check if elasticsearch memlock was successful:"}]}